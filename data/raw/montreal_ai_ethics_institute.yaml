- author: Renjie Butalid
  content: Content not found
  date: Unknown Date
  id: d5ce4738444ee37085087e7256331198
  publication: Montreal AI Ethics Institute
  tags: &id001
  - academic
  - institute
  - applied-ethics
  title: The AI Ethics Brief
  url: https://montrealethics.ai/the-ai-ethics-brief-163-navigating-uncertainty-ais-expanding-influence-on-society-governance-and-power/
- author: Unknown Author
  content: "Mapping the Responsible AI Profession, A Field in Formation (techUK)\n\
    April 28, 2025 \n\n\U0001F52C Report Summary by \u270D\uFE0F Tess Buckley\nTess\
    \ is Programme Manager, Digital Ethics and AI Safety at techUK and holds a Masters\
    \ in Philosophy and Artificial Intelligence from Northeastern University London.\n\
    [Original Paper by techUK]\n\n\n1. What Happened / Overview\nOn April 8, 2025,\
    \ techUK, the tech trade association of the UK, released our paper titled Mapping\
    \ the Responsible AI Profession, A Field in Formation. Our digital ethics working\
    \ group, whose ~40 members range from data specialists to chief ethics officers\
    \ at one of our 1,100 membership companies, brought this piece to life.\_\nThe\
    \ paper examined Responsible AI practitioners, highlighting their emergence as\
    \ essential human infrastructure to operationalise ethical principles and regulatory\
    \ requirements. These professionals ensure AI systems are developed and deployed\
    \ ethically, safely, and fairly across the UK economy.\_\nOur conversational,\
    \ mixed-methods approach was chosen intentionally to capture both the formal frameworks\
    \ emerging in the field and the practical, day-to-day experiences of practitioners\
    \ implementing ethical principles. We were focused on drawing our research from\
    \ direct engagement with practitioners to ensure that the paper was both by and\
    \ for them.\n2. Why It Matters\_\nThe rapid mainstreaming of AI has created a\
    \ fundamental shift in how organisations approach AI governance and ethics. What\
    \ was once primarily a theoretical discourse or auxiliary function has evolved\
    \ into an urgent operational imperative, often seen on the board agenda, with\
    \ organisations scrambling to establish robust frameworks for responsible AI (RAI)\
    \ implementation. At the heart of this transformation lies a pressing question:\
    \ who, precisely, is responsible for responsible AI?\_\nThe UK government\u2019\
    s current aim is to foster increased AI adoption and diffusion across the economy.\
    \ Key to achieving this will be cultivating greater trust and confidence in AI\
    \ systems and credibility in the professionals who safeguard them. This is why\
    \ the role of the RAI practioners is crucial and the need to support the development\
    \ of this profession is vital. However, we currently lack clear pathways for individuals\
    \ to enter the responsible AI profession, creating uncertainty for hiring managers\
    \ and impeding the development of a robust assurance ecosystem and supportive\
    \ skills programmes as recommended in the recently published AI Opportunities\
    \ Action Plan.\_\nOur paper reveals that this professional field is at a critical\
    \ juncture\u2014shifting from an emergent discipline into an essential organisational\
    \ function yet still defining its formal structure and boundaries.\_\nWe see three\
    \ critical gaps currently undermining the effectiveness of responsible AI practitioners\
    \ and threatening the UK\u2019s AI leadership ambitions:\_\n\nThe absence of clear\
    \ role definitions and organisational placement\_\nThe lack of structured career\
    \ pathways\_\nUnderdeveloped standardised skills and training frameworks\_\n\n\
    Just as privacy experts became indispensable during the internet\u2019s expansion,\
    \ responsible AI ethics practitioners are now becoming essential for our AI future.\n\
    3. Between the Lines\_\nThe career pathways leading to RAI practice (e.g. chief\
    \ ethics officers, heads of AI ethics and responsible AI leads within organisations)\
    \ are remarkably diverse, reflecting the field\u2019s multidisciplinary nature.\_\
    \nCurrent RAI practitioners come from varied backgrounds including philosophy,\
    \ compliance, computer science, law, the social sciences and business management.\
    \ This diversity brings rich perspectives to RAI implementation and should be\
    \ viewed as a strength. However, some have compared the current state of RAI practice\
    \ to privacy practice 20 years ago, when defined career paths had not yet emerged.\
    \ As the profession matures, more standardised educational and career pathways\
    \ will develop, even though maintaining diversity in professional backgrounds\
    \ will remain valuable.\_\nThe business implications of addressing these professional\
    \ development challenges are substantial. Without structured professionalisation,\
    \ organisations may face inconsistent implementation of ethical AI principles,\
    \ the erosion of stakeholder trust and potential regulatory complications that\
    \ could hinder innovation and competitive advantage. The absence of professional\
    \ standards leaves companies vulnerable to reputational damage and creates barriers\
    \ to international collaboration and commerce in AI systems.\_\nThis need for\
    \ practical wisdom has fostered vibrant communities of practice, both online and\
    \ offline, where current RAI practitioners actively share insights, resources\
    \ and support. These range from informal peer networks (such as All Tech is Human,\
    \ Montreal AI Ethics Institute, and Responsible AI UK) to established associations\
    \ like the International Association of International Privacy Professionals, the\
    \ Association of AI Ethicists and the International Association of Safe and Ethical\
    \ AI, creating a collaborative ecosystem where practitioners at all levels can\
    \ learn from others\u2019 experiences and challenges. Alongside leading consortiums\
    \ and global communities such as AI Verify Foundation\u2019s Project Moonshot,\
    \ Partnership on AI, and The AIQI Consortium.\_These communities function as \u2018\
    professional incubators\u2019 creating a collaborative ecosystem where practitioners\
    \ at all levels can learn from others\u2019 experiences and challenges (Page 27).\
    \ In addition to communities of practice, we provided a mapping of current educational\
    \ opportunities in the form of both postgraduate and short online courses that\
    \ are working to provide suitable training for a RAI talent pipeline (Pages 23-27).\n\
    4. Moving forward\nTo address these critical gaps and strengthen the responsible\
    \ AI profession, we recommend targeted interventions across three key stakeholder\
    \ groups (Pages 36-37).\n1. Priority Actions for Organisations: \nEstablish RAI\
    \ roles with clear mandates and sufficient authority to influence AI development\
    \ proactively. Invest equally in technical capabilities and governance skills\
    \ when developing AI talent. Ensure that RAI practitioners have direct reporting\
    \ lines to senior leadership. The following priorities represent the most urgent\
    \ actions needed to strengthen this crucial professional community.\_\n2. Priority\
    \ Actions for Professional Bodies: \nDevelop flexible certification frameworks\
    \ that recognise multiple pathways to expertise. Centre current practitioners\
    \ in professionalisation discussions to build upon existing best practices. Create\
    \ accessible professional development opportunities that maintain diversity while\
    \ establishing standards. Define clear boundaries between the ethical, auditorial\
    \ and compliance functions of RAI practice. Ensure that emerging certification\
    \ frameworks accommodate a wide range of entry routes and validate both formal\
    \ and experiential learning, especially in ethics, social impact, and interdisciplinary\
    \ practice.\_\n3. Priority Actions for Policymakers: \nRecognise RAI practitioners\
    \ as essential human infrastructure for effective AI governance, adoption across\
    \ the economy and development of the assurance ecosystem. Support industry collaboration\
    \ through networks like techUK to address common challenges. Invest in educational\
    \ pathways and talent pipelines that develop both technical and ethical competencies.\
    \ Monitor the profession\u2019s evolution to identify areas requiring additional\
    \ support.\n\nWant quick summaries of the latest research & reporting in AI ethics\
    \ delivered to your inbox? Subscribe to the AI Ethics Brief. We publish bi-weekly."
  date: '2025-04-28T21:59:40+00:00'
  id: f460ad47f3c612c264ef698d239d7e08
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: Mapping the Responsible AI Profession, A Field in Formation (techUK)
  url: https://montrealethics.ai/mapping-the-responsible-ai-profession-a-field-in-formation-techuk/
- author: Unknown Author
  content: "AI Policy Corner: Frontier AI Safety Commitments, AI Seoul Summit 2024\n\
    April 28, 2025 \n\n\u270D\uFE0F By Alexander Wilhelm.\nAlexander is a PhD Student\
    \ in Political Science and a Graduate Affiliate at the Governance and Responsible\
    \ AI Lab (GRAIL), Purdue University. \n\n\n\U0001F4CC Editor\u2019s Note: This\
    \ article is part of our AI Policy Corner series, a collaboration between the\
    \ Montreal AI Ethics Institute (MAIEI) and the Governance and Responsible AI Lab\
    \ (GRAIL) at Purdue University. The series provides concise insights into critical\
    \ AI policy developments from the local to international levels, helping our readers\
    \ stay informed about the evolving landscape of AI governance.\n\nDiscussions\
    \ between governments, civil society, and companies on the \u2018safe\u2019 development\
    \ of AI have advanced through collaborations such as the AI Safety Summit 2023\
    \ held in the UK and the AI Seoul Summit 2024. Led by the United Kingdom and the\
    \ Republic of South Korea, the Seoul Summit resulted in a framework of commitments,\
    \ known as the Frontier AI Safety Commitments, which 20 organizations, including\
    \ Anthropic, Microsoft, NVIDIA, and OpenAI, have agreed to. These commitments\
    \ required signatories to publish \u201Ca safety framework focused on severe risks\u201D\
    \ at the AI Summit in France in February 2025  (See The AI Ethics Brief #158 for\
    \ more on the Paris AI Action Summit). However, rhetoric at the Paris Summit emphasized\
    \ the benefits of AI rather than its potential harms and risks, raising questions\
    \ about the future of the three goals outlined in the Frontier AI Safety Commitments.\n\
    Three outcomes of the Frontier AI Safety Commitments\nOutcome 1: Organisations\
    \ effectively identify, assess and manage risks when developing and deploying\
    \ their frontier AI models and systems.\n\nSignatories to the Commitments agree\
    \ to identify risks relevant to their frontier models, including risks detected\
    \ by external entities and governments. Frontier models are defined within the\
    \ Commitments as \u201Chighly capable general-purpose AI models or systems that\
    \ can perform a wide variety of tasks and match or exceed the capabilities present\
    \ in the most advanced models.\u201D Multiple stakeholders are expected to collaboratively\
    \ identify unacceptable levels of risk within frontier models, with justifications\
    \ for the boundaries once they are set. Risk mitigation should then be planned\
    \ to maintain the acceptable levels, with a commitment not to develop models that\
    \ fail to meet these standards.\_\n\nOutcome 2: Organisations are accountable\
    \ for safely developing and deploying their frontier AI models and systems.\n\n\
    Groups that voluntarily pledge to join the Frontier AI Safety Commitments must\
    \ update their policies on an ongoing basis, extending the viability of the agreement\
    \ as these technologies evolve.\n\nOutcome 3: Organisations\u2019 approaches to\
    \ frontier AI safety are appropriately transparent to external actors, including\
    \ governments.\n\nSignatories are expected to provide transparency to the public\
    \ except when \u201Cdoing so would increase risk or divulge sensitive commercial\
    \ information to a degree disproportionate to the societal benefit.\u201D The\
    \ caveat, however, is that more details should be provided to \u201Ctrusted actors,\u201D\
    \ such as a home government. Finally, external actors should be engaged in the\
    \ assessment of risk, the organization\u2019s internal plans to safely develop\
    \ frontier AI models, and their follow through in implementing these plans.\n\n\
    Recent Developments in Frontier AI Governance\nThe Frontier AI Safety Commitments\
    \ provide a framework to mitigate risks to safety, security, and transparency,\
    \ while discussing governance strategies such as disclosure, evaluation, and performance\
    \ requirements. While not all AI development organizations have signed the Frontier\
    \ AI Safety Commitments, consensus on frontier AI standards is developing, as\
    \ the Frontier AI Safety Commitments are reflected in China\u2019s AI Safety Commitments.\_\
    \nNonetheless, some experts remain concerned about the voluntary nature of these\
    \ commitments. The Paris AI Summit\u2019s focus on the promise and opportunity\
    \ of AI instead of the risks latent in frontier models led to disappointment for\
    \ some civil society groups. The voluntary commitments remain for the 20 signatories\
    \ to the Frontier AI Safety Commitments, but the future of such standards is an\
    \ open question as the focus of AI Summits shifts.\nFurther Reading\n\nThe AI\
    \ Seoul Summit 2024\nTech Giants Pledge AI Safety Commitments \u2014 Including\
    \ a \u2018Kill Switch\u2019 if They Can\u2019t Mitigate Risks\nThe Bletchley Park\
    \ Process Could be a Building Block for Global Cooperation on AI Safety\n\n\n\
    Want quick summaries of the latest research & reporting in AI ethics delivered\
    \ to your inbox? Subscribe to the AI Ethics Brief. We publish bi-weekly."
  date: '2025-04-28T21:13:36+00:00'
  id: 8fc79a8b36c7c491e0db716572ff13fe
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: 'AI Policy Corner: Frontier AI Safety Commitments, AI Seoul Summit 2024'
  url: https://montrealethics.ai/ai-policy-corner-frontier-ai-safety-commitments-ai-seoul-summit-2024/
- author: Renjie Butalid
  content: Content not found
  date: Unknown Date
  id: 182cb47a483841faed0a70deda19548a
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: The AI Ethics Brief
  url: https://montrealethics.ai/the-ai-ethics-brief-162-beyond-the-prompt/
- author: Unknown Author
  content: "AI Policy Corner: The Colorado State Deepfakes Act\nApril 14, 2025 \n\n\
    \u270D\uFE0F By Ogadinma Enwereazu.\nOgadinma is a PhD Student in Political Science\
    \ and a Graduate Affiliate at the Governance and Responsible AI Lab (GRAIL), Purdue\
    \ University. \n\n\n\U0001F4CC Editor\u2019s Note: This article is part of our\
    \ AI Policy Corner series, a collaboration between the Montreal AI Ethics Institute\
    \ (MAIEI) and the Governance and Responsible AI Lab (GRAIL) at Purdue University.\
    \ The series provides concise insights into critical AI policy developments from\
    \ the local to international levels, helping our readers stay informed about the\
    \ evolving landscape of AI governance.\n\nIn 2024, the state of Colorado enacted\
    \ the Candidate Election Deepfake Disclosures Act which primarily aims to address\
    \ the increasing concern around AI-generated deepfakes in political campaigns.\_\
    \_\nWhat are Deepfakes?\n\u201CDeepfakes\u201D refer to synthetic or manipulated\
    \ media such as images, videos, or audio, generated by artificial intelligence\
    \ to falsely depict an individual saying or doing something they never actually\
    \ said or did.\_\nKey Provisions of the Colorado Election Deepfake Disclosures\
    \ Act\nThe Act prohibits the distribution of media content of candidates running\
    \ for elected office with deepfakes that are undisclosed or insufficiently labelled,\
    \ especially when done with knowledge and disregard for the content\u2019s falsity.\
    \ The Act clearly distinguishes deepfakes from AI-enhanced media by excluding\
    \ minimally edited or adjusted media content.\nTo comply with this Act, any such\
    \ communication must feature a clear and concise disclosure stating: \n\u201C\
    This (image/audio/video/multimedia) has been edited and depicts speech or conduct\
    \ that falsely appears to be authentic or truthful.\u201D \nThe above disclaimer\
    \ should also be included in the communication\u2019s metadata and, where feasible,\
    \ should be difficult to remove by future users\_\nLiability and Enforcement\n\
    For unpaid advertising violations, penalties start at $100 per violation; for\
    \ paid advertising, at least 10% of the amount spent on the communication. Additionally,\
    \ candidates depicted in undisclosed or improperly disclosed deepfakes can pursue\
    \ civil action for injunctive relief or damages, including attorney fees and costs.\_\
    \_\nExemptions\nThe Act exempts certain entities from liability, including interactive\
    \ computer services, broadcasting stations (radio, television, cable, satellite),\
    \ internet websites, regularly published newspapers and providers of technology\
    \ used in creating deepfakes, provided they comply with immunities granted by\
    \ federal law.\_\_\nLimitations of the Act\nDeepfakes can go viral in minutes,\
    \ and by the time enforcement kicks in, the reputational damage is already done.\
    \ Also, since deepfakes are often generated outside of the country, the legislation\
    \ is unlikely to have a substantial practical effect.\_\nMost deepfake detection\
    \ technologies are still catching up with the capabilities of generative AI. This\
    \ raises questions about the effectiveness of enforcement if the content in question\
    \ is not easily identifiable as fake. Attorney General Phil Weiser acknowledged\
    \ this in a September 2024 statement, warning that even AI tools built to detect\
    \ deepfakes often struggle to keep up.\_\_\nAs of today, over 30 states across\
    \ the U.S. have introduced or passed deepfake laws, reflecting a broad agreement\
    \ on the serious risks they pose. However, the penalties have varying severity.\
    \ States like New Jersey and Louisiana impose harsher penalties, including multi-year\
    \ prison sentences and fines reaching $50,000. Some other states, like Delaware\
    \ and California, rely on disclosure requirements or injunctive relief without\
    \ imposing significant financial or criminal penalties. Colorado\u2019s minimum\
    \ $100 fine for undisclosed deepfake is relatively modest.\_\nAs generative AI\
    \ continues to evolve, states may face increased pressure to update their regulatory\
    \ frameworks on deepfakes to safeguard electoral integrity and address broader\
    \ ethical concerns about privacy and consent.\_\nFurther Reading\n\nHB24-1147\
    \ Candidate Election Deepfake Disclosure Act\_\nWhat are Deepfakes, and how are\
    \ they created?\_\_\nRegulating Election Deepfakes: a comparison of state laws\_\
    \n\n\nWant quick summaries of the latest research & reporting in AI ethics delivered\
    \ to your inbox? Subscribe to the AI Ethics Brief. We publish bi-weekly."
  date: '2025-04-15T04:52:27+00:00'
  id: c7b9662fe8fb0cb593663672869eb988
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: 'AI Policy Corner: The Colorado State Deepfakes Act'
  url: https://montrealethics.ai/ai-policy-corner-the-colorado-state-deepfakes-act/
- author: Renjie Butalid
  content: Content not found
  date: Unknown Date
  id: b5477da7c638f96c3a1bd9088441e715
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: The AI Ethics Brief
  url: https://montrealethics.ai/special-edition-honouring-the-legacy-of-abhishek-gupta-1992-2024/
- author: Unknown Author
  content: "AI Policy Corner: The Turkish Artificial Intelligence Law Proposal\nMarch\
    \ 17, 2025 \n\n\u270D\uFE0F By Selen Dogan Kosterit.\nSelen is a PhD Student in\
    \ Political Science and a Graduate Lab Affiliate at the Governance and Responsible\
    \ AI Lab (GRAIL), Purdue University. \n\n\n\U0001F4CC Editor\u2019s Note: This\
    \ article is part of our AI Policy Corner series, a collaboration between the\
    \ Montreal AI Ethics Institute (MAIEI) and the Governance and Responsible AI Lab\
    \ (GRAIL) at Purdue University. The series provides concise insights into critical\
    \ AI policy developments from the local to international levels, helping our readers\
    \ stay informed about the evolving landscape of AI governance. This inaugural\
    \ piece spotlights Turkey\u2019s AI law proposal, examining its strengths and\
    \ the gaps in aligning with global AI governance frameworks. \n\nTurkey currently\
    \ lacks a specific law that directly regulates artificial intelligence (AI). However,\
    \ a law proposal on AI was submitted to the Grand National Assembly of Turkey\
    \ in June 2024. The law proposal aims to ensure the safe, ethical, and fair use\
    \ of AI technologies, guarantee the protection of personal data and privacy rights,\
    \ and create a regulatory framework for the development and use of AI systems.\_\
    \nRisk factors, harms, governance strategies, and incentives for compliance\n\n\
    Risk factors and harms: The law proposal explicitly states that the fundamental\
    \ principles of safety, transparency, fairness, accountability, and privacy must\
    \ be followed in the development and use of AI systems. Given these principles,\
    \ this proposal governs the AI-related risk factors of safety, transparency, bias,\
    \ and privacy. Furthermore, by emphasizing the protection of personal data and\
    \ mandating that AI systems shall not cause harm to users or result in discrimination,\
    \ this proposal also seeks to prevent AI-related harms, including violations of\
    \ civil or human rights, harms to safety, and harms stemming from discrimination.\n\
    \n\nGovernance strategies: The law proposal requires risk assessments to be carried\
    \ out during the development and use of AI systems, with special measures implemented\
    \ for high-risk systems. Additionally, it mandates that high-risk systems be registered\
    \ with relevant supervisory authorities and undergo a conformity assessment. Moreover,\
    \ the proposal states that supervisory authorities will be responsible for monitoring\
    \ compliance and detecting violations. Based on these provisions, the proposal\
    \ incorporates several governance strategies, such as the evaluation of AI systems\
    \ through impact assessment and conformity assessment, risk-tiering of AI systems\
    \ based on impact, registration of high-risk AI systems, and governance development\
    \ by establishing enforcement mechanisms.\n\n\nIncentives for compliance: The\
    \ law proposal declares that AI operators will be penalized with fines for engaging\
    \ in prohibited AI applications, violating obligations, or providing false information.\_\
    \n\nCriticism and Areas for Improvement\nAlthough the law proposal is a welcome\
    \ first step toward establishing AI governance in Turkey, some critics argue that\
    \ it falls short of international standards in key aspects:\n\nThe law proposal\
    \ does not specify which institution will be responsible for monitoring compliance\
    \ and detecting violations.\nWhile the EU AI Act classifies AI systems into four\
    \ risk categories and sets out specific regulations depending on each category,\
    \ the Turkish law proposal merely indicates that special measures should be adopted\
    \ for high-risk systems. It neither defines which AI systems fall into the high-risk\
    \ category nor provides details on how they should be regulated.\_\n\nRecent Developments\
    \ in AI Governance\nDespite the Turkish AI law proposal\u2019s lack of depth and\
    \ clarity, some recent developments have been promising in creating a strong AI\
    \ regulatory framework in Turkey. \nWith the establishment of a Parliamentary\
    \ AI Research Commission focused on ethical standards, plans to sign the Council\
    \ of Europe\u2019s Framework Convention on AI and Human Rights, Democracy, and\
    \ the Rule of Law, and intentions to align Turkish regulations with the EU AI\
    \ Act, Turkey seems to be on the right path toward building responsible and ethical\
    \ AI governance.\_\nFurther Reading\n\nNavigating the Future of AI Regulation\
    \ in T\xFCrkiye: Key Developments and Expectations\nT\xFCrkiye\u2019s AI research\
    \ commission eyes unveiling \u2018vision document\u2019\nT\xFCrkiye prepares legal\
    \ framework for artificial intelligence regulation\n\n\nWant quick summaries of\
    \ the latest research & reporting in AI ethics delivered to your inbox? Subscribe\
    \ to the AI Ethics Brief. We publish bi-weekly."
  date: '2025-03-17T22:25:43+00:00'
  id: d3af723f63bf70282b157ca369c51e8d
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: 'AI Policy Corner: The Turkish Artificial Intelligence Law Proposal'
  url: https://montrealethics.ai/ai-policy-corner-the-turkish-artificial-intelligence-law-proposal/
- author: Unknown Author
  content: "From Funding Crisis to AI Misuse: Critical Digital Rights Challenges from\
    \ RightsCon 2025\nMarch 2, 2025 \n\n\u270D\uFE0F Op-Ed by Seher Shafiq.\_\nSeher\
    \ is the Global Engagement Manager at Mozilla Foundation, based in Toronto, Canada.\n\
    \n\nAfter a break of almost two years, last week, the digital rights community\
    \ gathered in Taipei for RightsCon 2025 \u2013 a global convening of human rights\
    \ in the digital age. Amidst several ongoing conflicts, democratic backsliding,\
    \ economic instability, and a digital safety infrastructure that is now more than\
    \ frayed at the edges, a convening of this kind felt crucial, not optional, for\
    \ the sector.\_\nSetting the conference in Taipei was an international and impactful\
    \ choice \u2013 after many from Global South countries experienced visa restrictions\
    \ at RightsCon 2023 in Costa Rica, the RightsCon team took a break to conduct\
    \ a series of deep community consultations before choosing its next destination.\
    \ Ultimately, RightsCon landed on Taipei, citing its \u201Crecognition globally\
    \ for the freest online environment and greatest human freedom in Asia, a whole-of-society\
    \ approach to human rights, and a vibrant civil society community.\u201D\n\nRightsCon\
    \ 2025, Taipei\nRightsCon 2025 Taipei was a beautiful and chaotic mish-mash of\
    \ folks across different sectors \u2013 civil society, industry, philanthropy,\
    \ policy, research, creatives, and more \u2013 with the common denominator being\
    \ a dedication to digital rights.\_\nI am writing this on my flight back home\
    \ to Canada, where even a few hours without internet has seen major breaking news\
    \ stories in the area of human rights. Here are three themes that emerged top\
    \ of mind as I reflected on the last week:\_\n1. The unignorable elephant in the\
    \ room that followed us everywhere: the crisis being caused by funding cuts.\_\
    \n\nFireside Chat: Funding Landscape and the Future of the Digital Rights Ecosystem\
    \ (RightsCon 2025, Taipei)\_\nThe panic and urgency around this was in the air\
    \ at RightsCon right from day one. USAID\u2019s abrupt closure and funding cuts\
    \ for the sector by other governments has had a tremendous impact on the ecosystem\
    \ \u2013 life-saving projects have been cut or shut down completely, and many\
    \ organizations are being forced to close. Debora Zamd of the Human Rights Funders\
    \ Network shared sobering numbers: 2023 saw $223.3 billion given in foreign aid;\
    \ 2025 cuts are estimated at $78 billion. That\u2019s one-third of funding that\
    \ disappeared overnight, creating a gap so massive that Debora Zamd argues philanthropy\
    \ simply does not have the dollars to fill it without completely new strategies\
    \ and quick mobilization.\nI did not realize what a true crisis this was for the\
    \ ecosystem until I attended some sessions on this topic. In fact, the impact\
    \ was so immediate that RightsCon organizers added two emergency sessions on this\
    \ topic, providing a space for collaboration and strategizing a path forward,\
    \ as many organizations are on the verge of closing operations without alternative\
    \ funding solutions. Folks I spoke to at RightsCon shared that since the US set\
    \ this precedent with Trump making this policy change acceptable, other countries\
    \ have announced similar cuts, including the UK and the Netherlands, exacerbating\
    \ the sector\u2019s financial instability.\n2. The misuse of AI systems in war,\
    \ conflict, and genocide.\_\n\nTech giants and genocide: Indigenous struggles\
    \ for digital justice (RightsCon 2025, Taipei)\_\nOne of the most powerful sessions\
    \ I attended was titled \u201CTech giants and genocide: Indigenous struggles for\
    \ digital justice.\u201D The panel featured Nadim Nashif (7amleh), Jalal Abukhater\
    \ (7amleh), Htaike Htaike Aung (Myanmar ICT for Development Organization), Timnit\
    \ Gebru (Distributed Artificial Intelligence Research Institute), and Haiyuer\
    \ Kuerban (World Uyghur Congress).\_\nThe panel covered examples of AI being misused\
    \ in situations of war, at times, contributing to genocides. In Gaza, AI tools\
    \ are unethically deployed using dubious criteria to target Palestinian men, with\
    \ up to 15-20 civilian deaths permitted for each \u201Ctarget\u201D killed \u2013\
    \ all enabled by Google and Amazon via Project Nimbus. In the case of Uyghurs,\
    \ this represents the largest interment of a single ethnic group since the Second\
    \ World War, largely driven by AI and biometric tools trained to recognize Uyghur\
    \ faces and detain individuals for inconsequential reasons, such as having a beard\
    \ or fasting. Similarly, Amnesty International reported that Facebook\u2019s systems\
    \ actively promoted violence against Rohingyas and ignored early warnings about\
    \ the risk, exacerbating the genocide. During the Tigray genocide, calls for violence\
    \ went viral on Facebook and YouTube, yet these platforms failed to take meaningful\
    \ action.\n\nRightsCon 2025, Taipei\nThe panelists agreed that none of these platforms\
    \ are \u201Csystematically promoting human rights.\u201D This issue is compounded\
    \ by the fact that companies are left to self-regulate and police themselves,\
    \ with AI-driven content moderation developed in the Global North disproportionately\
    \ affecting Global Majority countries.\n3. The importance of uplifting Global\
    \ Majority perspectives in the AI ecosystem.\_\nWith fewer visa restrictions for\
    \ RightsCon 2025 Taipei, more participants from Global Majority countries attended\
    \ (also referred to as Global South by some) compared to the 2023 Costa Rica conference.\
    \ This led to more sessions centering on perspectives often overlooked in AI governance\
    \ and development.\n\nRethinking AI Development and Governance to Avoid Marginalizing\
    \ the Marginalized (RightsCon 2025, Taipei)\_\nIn the session \u201CRethinking\
    \ AI Development and Governance to Avoid Marginalizing the Marginalized,\u201D\
    \ Caleb Moses (Montreal Institute for Learning Algorithms \u2013 MILA) shared\
    \ an insight that resonated deeply. He explained that Indigenous approaches to\
    \ technology are inherently holistic, emphasizing the interconnectedness of all\
    \ parts of a system, whereas Western approaches tend to be more one-dimensional.\
    \ He illustrated this with a moral dilemma he faced\u2014how to ethically use\
    \ a large dataset of historical text from his M\u0101ori heritage in AI development.\n\
    The conference was bookended by a sobering call to action to Free Alaa, a RightsCon\
    \ community member unjustly imprisoned for the past 10 years. On day one, we watched\
    \ a video of Alaa\u2019s mother, Professor Laila Soueif\u2014140+ days into a\
    \ hunger strike\u2014pleading with the RightsCon community to take action to help\
    \ free Alaa. Four days later, at the closing ceremony, we received an update that\
    \ Professor Soueif had since collapsed from her hunger strike and is currently\
    \ hospitalized. Her video may have been her final public call to action. It was\
    \ a sombre reminder of the urgent and unfinished work in the fight for human rights\
    \ and justice \u2014 especially now.\nWhat\u2019s Next?\nRightsCon 2025 highlighted\
    \ the impact of the funding crisis, the brutal outcomes of misusing AI in warfare,\
    \ and the need to consider the Global Majority perspectives when discussing trustworthy\
    \ AI. Despite the complex issues facing the world today, RightsCon 2025 was still\
    \ a space of hope and inspiration.\_\n\nRightsCon 2025, Taipei\nWalking through\
    \ the corridors and stopping by the booths for projects like Stop Killer Robots,\
    \ Internet Society Foundation, and Amnesty International reminded me that for\
    \ all the injustice we see today, there\u2019s an army of incredibly passionate\
    \ folks working tirelessly to co-create a more just digital future.\_\nWant quick\
    \ summaries of the latest research & reporting in AI ethics delivered to your\
    \ inbox? Subscribe to the AI Ethics Brief. We publish bi-weekly."
  date: '2025-03-03T01:24:10+00:00'
  id: 9adb9edb80eb6709cf9579653e6c2120
  publication: Montreal AI Ethics Institute
  tags: *id001
  title: 'From Funding Crisis to AI Misuse: Critical Digital Rights Challenges from
    RightsCon 2025'
  url: https://montrealethics.ai/from-funding-crisis-to-ai-misuse-critical-digital-rights-challenges-from-rightscon-2025/
