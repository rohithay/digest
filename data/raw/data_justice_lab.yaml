- author: Unknown Author
  content: "Data Harm Record \nData Harm Record (Updated)\nUpdated August 2020\nJoanna\
    \ Redden, Jessica Brand and Vanesa Terzieva\_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\nThe aim of this\
    \ document is to provide a running record of \u2018data harms\u2019, harms that\
    \ have been caused by uses of algorithmic systems. The goal is to document so\
    \ that we can learn from where things have gone wrong and ideally together work\
    \ toward redressing harms and preventing further harm. The document compiles examples\
    \ of harms that have been detailed in previous research and publications. Each\
    \ listed example contains a link to the original source and often also related\
    \ information.\nThe Data Harm Record pulls together concrete examples of harm\
    \ that have been referenced in previous work so that we might gain a better \u2018\
    big picture\u2019 appreciation of how people have already been negatively affected\
    \ by uses of algorithmic systems. A survey of harms also suggests where things\
    \ may go wrong in the future and ideally stimulates more debate and interventions\
    \ into where we may want to change course. The idea is that we can learn a lot\
    \ by paying attention to where things have gone wrong and by considering data\
    \ harms in relation to each other.\nThe Data Harm Record was first published in\
    \ 2017. Over the last year we have attempted to update it with recent examples.\
    \ We have tried to capture a wide range of examples, but there are gaps in what\
    \ we have been able to identify and list here due to time, resource and language\
    \ limitations.\n\_\nBackground\nPeople working in business, government, politics\
    \ and for non-profit organizations are all developing new ways to make use of\
    \ algorithmic systems. These bodies have always collected and analysed data, but\
    \ what\u2019s changed is the size, scope and methods to analyse data. The digitization\
    \ of near everything along with major computing advances mean that it is now possible\
    \ to combine sizes and types of data previously unimaginable, and to then analyse\
    \ these staggering datasets in new ways to find patterns and make predictions.\n\
    There is an abundance of enthusiasm and optimism about how automated, predictive\
    \ and AI data systems can be used for good. Optimism persists for good reason,\
    \ there is a lot of good that can be done through new uses of data systems [1]\
    \ There is also growing consensus that with these new algorithmic systems comes\
    \ risks to individuals and society. Previous work has detailed how data analytics\
    \ can be used in ways that threaten privacy, security, as well as increase inequality\
    \ and discrimination. The danger with automated and predictive decision support\
    \ systems is that harms can be caused unintentionally and intentionally.\nAs argued\
    \ by Cathy O\u2019Neil , this is important to keep in mind as in many cases the\
    \ algorithmic systems that are leading to harm were developed with very good intentions.\
    \ The problem is that new algorithmic tools present new ways to sort, profile,\
    \ exclude, exploit, and discriminate. The complexity, opacity, and proprietary\
    \ nature of many datafied systems mean that often we don\u2019t know things have\
    \ gone wrong until after large numbers of people have been affected. Another problem\
    \ is that few people have the skills needed to interrogate and challenge these\
    \ new automated and predictive systems. What recourse do citizens have if they\
    \ have been wrongfully targeted, profiled, excluded or exploited? Government agencies,\
    \ civil society organizations and researchers across disciplines are drawing attention\
    \ to these risks.\n\_\nDefining data harms\nDictionary definitions of harm link\
    \ it to physical and material injuries, but also to potential injuries, damages\
    \ and adverse effects.[2] Solove and Citron argue that harm can be understood\
    \ as \u2018the impairment, or set back, of a person, entity, or society\u2019\
    s interests. People or entities suffer harm if they are in worse shape than they\
    \ would be had the activity not occurred\u2019.[3]\nBuilding on these definitions,\
    \ one way to understand data harms is as the adverse effects caused by uses of\
    \ data that may impair, injure, or set back a person, entity or society\u2019\
    s interests. While this definition is a start, clearly it is insufficient and\
    \ will need to be developed given the increasing ubiquity of datafied practices\
    \ all around us.\nOur legal and political systems are struggling to come to terms\
    \ with data harms. Across nations it is becoming easier for corporate and government\
    \ bodies to share data internally and externally. New data about us is being generated\
    \ by us and collected by others through new systems. Consider for example the\
    \ range of data that can be generated and collected through the Internet of Things\
    \ and also the range of harms that can be caused if the wrong people hack into\
    \ industrial systems. Increasingly, our digital selves and the digitization of\
    \ services affect the kind of lives we lead, the opportunities afforded to us,\
    \ the services we can access and the ways we are treated. All of these developments\
    \ present new types of risk and harm. For all of these reasons we need to develop\
    \ a more complex understanding and appreciation of data harms and a means to assess\
    \ current and future harms, from the perspective of people who are and may be\
    \ negatively affected by these harms.\n\_\nData violence \nThe harms can be so\
    \ significant that researchers like Anna Lauren Hoffman are arguing that we need\
    \ to go further and recognize that in many cases we are dealing with \u2018data\
    \ violence.\u2019 [4] One example of data violence is when people are wrongly\
    \ denied access to essential services and resources.\n\_We know that algorithms\
    \ and automated systems are increasingly being used in decision- making: for job\
    \ recruitment, risk assessment , credit and bail hearings in the US  among others.\
    \ Research is documenting the ways these systems can embed bias. The implementation\
    \ of algorithmic systems in areas that link people to essential services means\
    \ that the bias and errors introduced via these algorithms can cause significant\
    \ harm. Research demonstrates that the already marginalized are far more likely\
    \ to be negatively affected. To quote Virginia Eubanks: \u201CThese systems impact\
    \ all of us, but they don\u2019t impact us all equally\u201D.\n\_\nExamples\n\
    Commercial uses of data \u2013 Exploitation\nTargeting based on perceived vulnerability\n\
    Some have drawn attention to how new tools make it possible to discriminate and\
    \ socially sort with increasing precision. By combining multiple forms of data\
    \ sets a lot can be learned.[5] Newman calls this \u2018algorithmic profiling\u2019\
    \ and raises concern about how much of this profiling is invisible as citizens\
    \ are unaware of how data is collected about them across searches, transactions,\
    \ site visits, movements, etc. This data can be used to profile and sort people\
    \ into marketing categories, some highly problematic. For example, data brokers\
    \ combine data sets to identify specific groups. Much of this sorting goes under\
    \ the radar. Some of it raises serious concerns. In her testimony to Congress,\
    \ World Privacy Forum\u2019s Pam Dixon reported finding brokers selling lists\
    \ of rape victims, addresses of domestic violence shelters, sufferers of genetic\
    \ diseases, sufferers of addiction and more.\n\nAnother example, in 2015 the U.S.\
    \ Federal Trade Commission \u2018charged a data broker operation with illegally\
    \ selling payday loan applicants\u2019 financial information to a scam operation\
    \ that took millions from consumers by debiting their bank accounts and charging\
    \ their credit cards without their consent\u2019.[6]\n\_\nWhen your personal information\
    \ gets used against you \nConcerns have been raised about how credit card companies\
    \ are using personal details like where someone shops or whether or not they have\
    \ paid for marriage counselling to set rates and limits.[7] This has been called\
    \ \u2018personalization\u2019, or \u2018behavioural analysis\u2019 or \u2018behavioural\
    \ scoring\u2019 and refers to companies tailoring things to people based on what\
    \ is known about them. Croll notes that American Express used purchase history\
    \ to adjust credit limits based on where customers shopped. Croll as well as Hurley\
    \ and Adebayo , describe the case of one man who found his credit rating reduced\
    \ from $10,800 to $3,800 in 2008 because American Express determined that \u2018\
    other customers who ha[d] used their card at establishments where [he] recently\
    \ shopped have a poor repayment history with American Express\u2019.[8] This event,\
    \ in 2008, was an early example of \u2018creditworthiness by association\u2019\
    \ and is linked to ongoing practices of determining value or trustworthiness by\
    \ drawing on \u2018big data\u2019 to make predictions about people.[9]\n\_\nDiscrimination\
    \ \u2013 skin colour, ethnicity, class or religion\nCredit Scoring\nAs companies\
    \ responsible for credit scoring, background checks, and hiring make more use\
    \ of automated data systems, an individual\u2019s appearance, background, personal\
    \ details, social network, or socio-economic status may influence their ability\
    \ to get housing, insurance, access education, or a job.\nThere are new start-up\
    \ companies that make use of a range of \u2018alternative\u2019 data points to\
    \ make predictions about consumers and provide people with credit scores. In addition,\
    \ traditional credit scoring agencies are making use of data and machine learning\
    \ to develop profiles. While the argument is that these tools could open up the\
    \ potential for some not served by traditional credit scoring systems to receive\
    \ credit, there are a range of concerns about how algorithmic scoring may discriminate.\
    \ For example, a consumer\u2019s purchase history could be used, intentionally\
    \ or unintentionally, as a proxy for ethnicity or religion. If an algorithmic\
    \ system ends up penalizing one group more than others it may be hard to figure\
    \ this out given the access issues, opacity and complexity of algorithmic processes.\
    \ While there are laws in place for people to review conventional credit scores,\
    \ there are not yet measures in place for people to interrogate new generated\
    \ scores.\nIn relation to all of these examples, researchers have raised concerns\
    \ about how new data driven processes reproduce illegal redlining practices. Historically,\
    \ redlining was used to discriminate against certain groups of people by denying\
    \ some groups access, or more expensive access, to housing or insurance. This\
    \ was often done by \u2018redlining\u2019 certain communities. The issue is that\
    \ where someone lives is often associated with ethnicity and class. In this way\
    \ location facilitates racism and inequality. Critics are concerned about how\
    \ new automated and predictive data tools can be used to \u2018redline\u2019 given\
    \ the amount of detail that can be determined about us through our data. Previous\
    \ research has demonstrated the potential to accurately determine our age, gender,\
    \ sexuality, ethnicity, religion and political views through the data that can\
    \ be collected and combined about us.\nRelatedly, groups are raising concerns\
    \ about how new data driven processes may facilitate \u2018reverse redlining\u2019\
    . This is when a particular group of people is targeted, as was done with sub-prime\
    \ mortgages. Newman argues that big data was central to the subprime financial\
    \ crash in 2007 as it played a key role in the manipulation of markets but also\
    \ in the subprime mortgage industry. Online advertising and data collected about\
    \ people online was used to direct and target them for sub-prime loans. In 2012\
    \ the American Department of Justice reached a settlement with the Wells Fargo\
    \ Bank concerning allegations that it had \u2018engaged in a pattern or practice\
    \ of discrimination against qualified African-American and Hispanic borrowers\
    \ in its mortgage lending from 2004 through 2009\u2019 by pushing these borrowers\
    \ into more costly sub-prime loans. In the settlement they agreed to provide $184\
    \ million in compensation.\nThe practice of targeting low-income groups continues\
    \ in the payday loan industry. A U.S. Senate Investigation reports that data brokers\
    \ have been found selling lists that focus on citizen financial vulnerability.\
    \ For example, data brokers have compiled the following lists to sell to those\
    \ interested in targeting such groups: \u2018Rural and Barely Making It\u2019\
    , \u2018Ethnic Second-City Strugglers\u2019, \u2018Retiring on Empty: Singles\u2019\
    , \u2018Tough Start: Young Single Parents\u2019. One company was found selling\
    \ a marketing tool to \u2018identify and more effectively market to under-banked\
    \ consumers\u2019.[10]\nAs argued by Madden et al., the fact that those with low-incomes\
    \ are less likely to take privacy protection measures when online and to also\
    \ rely more on their mobile phone for online access places them at greater risk\
    \ than others for online targeting and exploitation.[11] In fact, \u2018opting\
    \ out\u2019 of being tracked becomes increasingly difficult as technologies become\
    \ more sophisticated. New tools that make cross-device tracking possible or that\
    \ are embedded the Internet of Things, mean that the objects we use everyday make\
    \ more of our lives \u2018knowable\u2019 and trackable and make \u2018opting out\u2019\
    \ even harder.[12] Newman raises concerns about how in this age of datafication,\
    \ information inequality is transferred into economic inequality, as companies\
    \ have more information about citizens that can be used to target and exploit\
    \ them to their disadvantage.[13]\nCitron and Pasquale note that \u2018evidence\
    \ suggests that credit scoring does indeed have a negative disparate impact on\
    \ traditionally disadvantaged groups\u2019. They provide a number of examples\
    \ in their article, just one is the case of All-State which was challenged in\
    \ court and agreed to a multi-million dollar settlement over their scoring procedure\
    \ which plaintiffs argued \u2018resulted in discriminatory action against approximately\
    \ five million African-American and Hispanic customers\u2019.[14] They also raise\
    \ concerns about how scoring systems and predictive tools may actually create\
    \ the situations they claim to indicate and \u201Ctake a life\u201D of their own,\
    \ for example by labelling someone a poor candidate or unemployable.[15]\nIn 2015,\
    \ Christian Haigh, a Harvard undergraduate, discovered that the prices for The\
    \ Princeton Review\u2019s online SAT tutoring packages offered to high school\
    \ students varied depending on where customers live. Julia Angwin and Jeff Larson\
    \ of ProPublica investigated Haigh\u2019s findings and found that the highest\
    \ prices were being offered to ZIP codes with a large Asian population and high\
    \ median income. The Princeton Review said that the price difference was not intentional,\
    \ but as noted by ProPublica, the pricing algorithm clearly did discriminate.\
    \ Angwin and Larson note that it is significant that in the United States \u2018\
    unintentional racial discrimination is illegal in housing and employment under\
    \ the legal doctrine known as \u2018disparate impact\u2019 which prohibits inadvertent\
    \ actions that hurt people in a protected class\u2019. However this doctrine does\
    \ not extend to the online world, making it difficult in that country (and others)\
    \ to take legal action against \u2018adverse impact\u2019 caused by unintentional\
    \ algorithmic bias.\nIn 2012, a Wall Street Journal investigation found that Staples\
    \ Inc. website displayed \u2018different prices to people after estimating their\
    \ locations\u2019 and that in what appeared to be an \u2018unintended side effect\u2019\
    \ Staples tended to show discounted prices to areas with a higher average income\
    \ and higher prices to areas with lower average incomes.[16]\nA 2017 investigation\
    \ by ProPublica and Consumer Reports showed that minority neighborhoods pay more\
    \ for car insurance than white neighborhoods with the same risk levels. The study,\
    \ which compared premiums and payouts in California, Illinois, Texas and Missouri,\
    \ showed that minority neighborhoods paid \u2018as much as 30 percent more than\
    \ other areas with similar accident costs\u2019.\n\_\n\nhttp://a.msn.com/00/en-us/BBzv1Xg?ocid=scu2\n\
    \_\nIn 2015 Facebook suspended the accounts of Native Americans because its algorithm\
    \ did not recognize their names as real. [17] The \u201Creal name\u201D policy\
    \ left hundreds of native Americans with suspended accounts and they had to prove\
    \ their identity in order to use their accounts again. Dana Lone Hill was one\
    \ of the Native Americans who had to produce multiple ID documents to prove her\
    \ identity and have her profile reinstated. Her case generated a lot of media\
    \ attention and Facebook had to review its algorithm and eliminate the possibilities\
    \ for discrimination.\n\_\nRecognition technologies\nThere are numerous reports\
    \ of the biases embedded in facial recognition systems: they have problems identifying\
    \ people with darker skin and also with gender. Algorithms that are used to focus\
    \ smartphone cameras, for border security and advertisements sometimes cannot\
    \ identify, or misidentify, people of colour. It has been reported that the problem\
    \ is that the facial recognition algorithms used across various systems have been\
    \ trained using datasets that have mostly male white faces, that these algorithms\
    \ have not been exposed to enough diversity and that this problem is also connected\
    \ to the fact that many of these systems are being developed and tested largely\
    \ by white men. As argued by Joy Buolamwini, the issue of bias and inaccuracy\
    \ becomes increasingly important as facial recognition tools are adopted by police\
    \ and security systems.\n\_\n\n\n\n\_\nExamples of problems include the New Zealand\
    \ case where one man\u2019s passport photograph was rejected when a facial recognition\
    \ program mistakenly identified him as having closed eyes. People have posted\
    \ reviews online raising questions about the ability of Microsoft\u2019s Kinect\
    \ facial recognition feature to recognize people with darker skin and of HP\u2019\
    s tracking webcams \u2018to see Black people\u2019. Recent work by Buolamwini\
    \ and Gebru involved testing tools and found \u201Cdarker skinned females to be\
    \ the most misclassified group.\u201D\nOne of the biggest developers of facial\
    \ recognition software is Amazon and their tool \u2018Rekognition\u2019 has been\
    \ in the centre of debates about machine bias and racial discrimination in AI\
    \ technologies. Amazon\u2019s Rekognition has been designed to cross-reference\
    \ photos of unknown suspects and criminals against a database of mugshots from\
    \ jails in the country .[18] When the software was tested by the American Civil\
    \ Liberties Union of Northern California it was found that people of colour are\
    \ predominantly being misidentified from the mugshot database. The ACLU used photos\
    \ of Members of Congress and cross-referenced them with the mugshot database using\
    \ Amazon\u2019s facial recognition system: 28 members of Congress were misidentified\
    \ as people from the mugshot database and over 40% of them were from ethnic minorities.\
    \ People of colour make up only around 20% of Congress, but the misidentification\
    \ rate with them is two times higher than people of Caucasian origin.[19]\nThe\
    \ ACLU argues that due to concerns about the high risk of misidentification and\
    \ discrimination Amazon\u2019s software unfit and dangerous for use. The technology\
    \ has been sold to the American government and police forces.[20]\n\_\n\n\_\n\
    In August 2019 an American Federal court agreed with a group of Illinois Facebook\
    \ users that Facebook\u2019s use of face recognition technology on their photographs\
    \ without their knowledge or consent violated their privacy rights. The ACLU had\
    \ supported their case, arguing that \u2018performing a scan of an individual\u2019\
    s face without disclosing how that information will be stored, used, or destroyed,\
    \ and without properly obtaining written consent, creates an actionable privacy\
    \ harm. Notice and informed consent empower individuals to protect their privacy\
    \ and are central to privacy laws in the United States, generally, and to BIPA,\
    \ specifically.\u2019\nSasha Costanza-Chock\u2019s work details how \u201Cnorms,\
    \ values, assumptions \u2013 are encoded in and reproduced through the design\
    \ of sociotechnical data-driven systems.\u201D Their essay on the politics of\
    \ border security systems illustrates this as well as the harmful experience of\
    \ confronting the normative politics of these systems. The injustice and harm\
    \ caused by normative security systems has also been stressed by Shadi Petosky\
    \ as has the fact that there are alternatives that are being ignored.\n\_\n Eyeo\
    \ 2019 \u2013 Sasha Costanza-Chock from Eyeo Festival on Vimeo.\n\_\nDiscrimination\
    \ \u2013 gender and ethnicity\nA study of Google ads found that men and women\
    \ are being shown different job adverts, with men receiving ads for higher paying\
    \ jobs more often.[21] The study, which used a tool called AdFisher to set up\
    \ hundreds of simulated user profiles, was designed to investigate the operation\
    \ of Google\u2019s ad settings. Although researchers could determine that men\
    \ and women are being shown different ads, they could not determine why this is\
    \ happening. Doing so would require access to more information that would need\
    \ to be provided by advertisers about who they were targeting and by Google about\
    \ how their system works.\nFacebook allows advertisers to target people based\
    \ on race, ethnicity and gender. Another ProPublica investigation revealed that\
    \ third party companies can target ads to reach people by gender, ethnicity and\
    \ race and also to be hidden from people based on these kinds of classifications.\
    \ [22] A ProPublica investigation identified that Facebook\u2019s ads software\
    \ gives advertising companies the option to exclude men or women from their ad\
    \ demographic. Job positions for Uber and truck drivers, police officers and military\
    \ were all shown to predominantly male audiences, while job vacancies for nurses,\
    \ medical assistants and care-takers targeted women almost exclusively.\n\_\n\n\
    \_\nA complaint by the American Civil Liberties Union (ACLU) submitted to the\
    \ US Equal Opportunity Commission (EEOC) mentions three women from the states\
    \ of Ohio and Illinois who were not shown job advertisements for positions in\
    \ traditionally male-dominated fields which violates federal law of gender. [23]\
    \ The ACLU argued that by targeting only men in already predominantly male fields,\
    \ women are denied the opportunity to break into particular industries.\nSome\
    \ landlords have been found to selectively target different groups of users with\
    \ housing ads, excluding people from \u201Credlined\u201D neighbourhoods, and\
    \ inner-city areas with high rates of Black and Latino residents.[24] The housing\
    \ adverts run by some American landlords were devised to selectively target certain\
    \ communities and exclude people from Hispanic or African-American origin, or\
    \ citizens with bad credit scores . The ads that they publish are invisible for\
    \ the \u201Cexcluded\u201D people, as their ethnicity deems them an \u201Cundesirable\u201D\
    \ demographic. Discriminating based on sensitive factors such as ethnicity and\
    \ nationality in the United States is prohibited by The Fair Housing Act of 1968\
    \ and ads for housing that target based on those characteristics are in fact illegal,\
    \ but nonetheless existing .[25]\n\nDiscrimination \u2013 health\nCathy O\u2019\
    Neil has produced a great deal of work demonstrating how unfair and biased algorithmic\
    \ processes can be. In one example, she tells the story of Kyle Behm, a high achieving\
    \ university student who noticed that he was repeatedly not getting the minimum\
    \ wage jobs he was applying for. All of these job applications required him to\
    \ take personality tests which included questions about mental health. Although\
    \ healthy when looking for work, Behm did suffer from bipolar disorder and had\
    \ taken time out previously to get treatment. Behm\u2019s father is a lawyer and\
    \ he became suspicious of the fairness of these tests for hiring. He decided to\
    \ investigate and found that a lot of companies were using personality tests,\
    \ like the Kronos test. These tests are used as part of automated systems to sort\
    \ through applications and in this process decide which applicants proceed and\
    \ which are \u2018red-lighted\u2019 or discarded. As O\u2019Neil details, these\
    \ tests are often highly complex, with \u2018certain patterns of responses\u2019\
    \ disqualifying people. This example raises a number of ethical questions about\
    \ the use of health information in automated systems but also about the uses of\
    \ automated systems in hiring more generally, particularly as it is unlikely that\
    \ those who have been \u2018red-lighted\u2019 will ever know they were subject\
    \ to an automated system. O\u2019Neil argues that the increasing use of automated\
    \ systems to sort and whittle down job applications creates more unfairness as\
    \ those who know or can pay for help to ensure their applications get to the top\
    \ of the pile have an advantage.\n\_\nLoss of privacy\nThis can happen unintentionally\
    \ when attempts to release data anonymously do not work. Big data makes anonymity\
    \ difficult because it is possible to re-identify data that has been anonymized\
    \ by combining multiple data points.\n\_\nPlatforms\nAs detailed by Paul Ohm,\
    \ in 2006 America Online (AOL) launched \u2018AOL Research\u2019 to \u2018embrace\
    \ the vision of an open research community\u2019. The initiative involved publicly\
    \ releasing twenty million search queries from 650,000 users of AOL\u2019s search\
    \ engine. The data, which represented three months of activity, was posted to\
    \ a public website. Although the data was anonymized, once the data was posted\
    \ some users demonstrated that it was possible to identify people\u2019s identities\
    \ using the data which included name, age and address.\nTwo New York Times reporters\
    \ Michael Barbaro and Tom Zeller Jr. cross-linked data to identify Thelma Arnold,\
    \ a sixty-two year old widow from Lilburn Georgia. Her case demonstrates the problems\
    \ with \u2018anonymisation\u2019 in an age of big data, but also the danger in\
    \ reading too much into search queries. As Barbaro and Zeller note, Ms Arnold\u2019\
    s search queries \u2018hand tremors\u2019, \u2018nicotine effects on the body\u2019\
    , \u2018dry mouth\u2019 and \u2018bipolar\u2019, could lead someone to think she\
    \ suffered from a range of health issues. Such a conclusion could have negative\
    \ effects if the organization making that conclusion was her insurance provider.\
    \ In fact, when they interviewed Arnold, Barbaro and Zeller found that Arnold\
    \ often does searches for her friends because she wants to help them.\nIn 2006\
    \ Netflix publicly released one hundred million records detailing the film ratings\
    \ of 500,000 of its users between Dec. 1999 and Dec. 2005. As Ohm reports, the\
    \ objective was to launch a competition and for those competing to use this data\
    \ to improve Netflix\u2019s recommendation algorithm.[26] Netflix anonymized the\
    \ data by assigning users a unique identifier. Researchers from the University\
    \ of Texas demonstrated not long after this release how relatively easy it was\
    \ for people to be re-identified with the data.[27] This led to a court case in\
    \ which Jane Doe argued that the data could be used to out her sexuality.[28]\
    \ Jane Doe argued that her homosexuality was being revealed by the data as it\
    \ revealed her interest in gay and lesbian themed films. She argued the data outed\
    \ her, a lesbian mother, against her wishes and could damage herself and her family.\
    \ The court case was covered by Wired in 2009.\n\_\nFitness Trackers\nSome employers\
    \ are gaining highly personal information about their employees through their\
    \ use of fitness trackers. Employers now collect health and biometric data about\
    \ their employees through the use of wearable tech and health tracking apps.[29]\
    \ Through performance monitoring employers can now collect regular reports about\
    \ staff activity. Concerns are being raised about companies that are encouraging\
    \ their female employees to use family planning apps that give employers and other\
    \ corporate entities access to details about their employee\u2019s private lives,\
    \ health, hopes and fears.[30]\nFitness apps and trackers allow its users to monitor\
    \ their calorie intake, physical activity and vital signals such as heart rate\
    \ and blood pressure. In 2018 a data leak involving MyFitnessPal exposed the accounts\
    \ of 115 million users after a security breach in their systems.[31] The user\
    \ names, email addresses and scrambled passwords to user accounts were stolen\
    \ from the parent sportswear company Under Armour.[32] Access to the fitness accounts\
    \ means access to vital information that can be used to track individuals, view\
    \ their location in live time, predict behaviour and activities or share sensitive\
    \ health information with third party organisations such as private health clinics,\
    \ insurance companies and even employers [33]\nIn 2018, Strava revealed that fitness\
    \ app data can reveal highly sensitive location information. Heat map visualisations\
    \ released by Strava showed activity captured by the app, lighting up different\
    \ user routes. This mapping involved more than 3 trillion GPS data points.[34]\
    \ A problem is that this app is used by military personnel and by releasing these\
    \ \u2018anonymous\u2019 heat maps Strava was revealing the location of military\
    \ activity. While the information on the heatmaps is an aggregate of all the user\
    \ activities, the Strava website allows the user to track running routes in detail\
    \ and eventually connect them to usernames and the individuals behind them, which\
    \ could endanger military personnel on missions overseas.[35]\nAnother fitness\
    \ tracking app \u2013 Polar Flow, also exposed the geolocation of its users through\
    \ a tool called \u201CExplore map\u201D. An investigation by De Correspondent\
    \ and Bellingcat revealed that the app makes it possible to explore sensitive\
    \ locations and locate individual users and their exercise routines. It turned\
    \ out that tracking an individual behind a username has been made readily available\
    \ and fairly easy and the names and addresses of personnel from intelligence agencies\
    \ such as the NSA, the US Secret Services and the MI6 could be uncovered.\n\_\n\
    \n\_\nSmart microchips\nSome tech companies have found even more intrusive ways\
    \ to monitor their employees. They have started using smart microchips that can\
    \ be implanted under the skin- the same technology that the US justice department\
    \ uses on prisoners on probation instead of ankle braces. The chips work similar\
    \ to the access key cards a lot of firms have, except these key cards do not track\
    \ the workers\u2019 physical and physiological condition. A Chinese mining company\
    \ has even introduced helmets that could read the brainwaves of workers and distinguish\
    \ feelings such as fatigue, distraction and even anger [36]. Earlier last year,\
    \ the American company Three Square Market began putting microchips the size of\
    \ a rice grain in their workers\u2019 hands- from 6-7 chipped employees initially,\
    \ the process expanded to include more the 100 with plans to implant chips on\
    \ all its 10 000 employees.\nThe chips can be used instead of physical IDs to\
    \ open doors, log into computers, pay at the vending machines on site, and can\
    \ be paired with the GPS on employers\u2019 smartphones to show the exact location\
    \ of every employee at any point throughout the day.[37]\nBut unlike fitness tracking\
    \ devices and key cards that could be taken off at the end of the work day, chips\
    \ are worn constantly- giving the employer infinite possibilities to track its\
    \ employees. This invasion of personal space has expanded substantially. BioTeq,\
    \ a UK- based firm is one of the new businesses that offer implants to companies\
    \ and individuals and has implanted more than 150 chips in various firms across\
    \ the UK.[38] Experts and researchers warn that microchip implanting can hide\
    \ a lot of dangers, especially for employees, as it can completely erode their\
    \ right to privacy.\n\_\nSmart devices- smart spies \nAs of 2019, over 100 million\
    \ Amazon Echos have been sold. Amazon employs thousands of people in its headquarters\
    \ to listen to what Alexa has recorded about users.[39] The recordings are transcribed\
    \ and it is argued used to eliminate gaps in Alexa\u2019s communication and introduce\
    \ new accents and words. Together with the conversations that people have had\
    \ with their Alexa devices, the devices pick up other audio in the home. Access\
    \ to the device has been requested by judges in court proceedings. There is increasing\
    \ concern about how the use of such devices can compromise privacy.\n\nConcerns\
    \ are also being raised about Smart TVs.[40] In 2017 Wikileaks published documents\
    \ suggesting a CIA operation: Weeping Angel, which allegedly involved the use\
    \ of smart TV microphones for mass surveillance.[41] Consumer Reports also compiled\
    \ information and advice for the owners of smart TVs about to avoid tracking.\n\
    \nChildren\nCompanies have been facing increasing complaints for collecting data\
    \ about children and sharing or using this to target them with advertisements\
    \ or more content designed to keep them online for longer. In 2019, the American\
    \ Federal Trade Commission ordered Google, including its subsidiary YouTube, to\
    \ pay a record $170 million to settle allegations that YouTube had illegally collected\
    \ data about children without their parent\u2019s consent.\nConcerns are also\
    \ being raised about YouTube\u2019s site for children and the YouTube algorithm,\
    \ which privileges the kind of sensational content that keeps children online\
    \ for longer. Concerns have been raised about the algorthm through privileging\
    \ content like this leading to the promotion of violent and suggestive content.\n\
    The Children\u2019s Commissioner for England published a report highlighting the\
    \ range of data that is being collected about children. The report draws attention\
    \ to the fact that while there is increasing attention to the need to be alert\
    \ to privacy infringements related to online platforms, there is also a need to\
    \ consider the privacy issues and potentials for harms introduced by smart internet\
    \ connected devices, like monitors and toys. Consumer groups have called for some\
    \ smart toys to be re-called after learning that these toys could be hacked to\
    \ enable strangers to talk to children and parents reported that their baby monitors\
    \ were hacked.\n\_\nIdentity theft, blackmail, reputation damage, distress\n\_\
    \nData breaches\nAlthough data breaches are listed under corporate uses of data,\
    \ they could also be listed here under government uses of data as breaches have\
    \ happened in both sectors. Solove and Citron argue that \u2018harm\u2019 in relation\
    \ to data breaches relates to \u2018a risk of future injury, such as identity\
    \ theft, fraud, or damaged reputations\u2019 and also to a current injury as people\
    \ experience anxiety about this future risk. They note that the anxiety and emotional\
    \ distress created about future risk is a harm that people experience \u2018in\
    \ the here and now\u2019. Identity theft is a major problem, particularly for\
    \ those of low-income who lack the resources to pay for legal representation and\
    \ challenge mistakes due to identity fraud. Further, the sudden loss of income\
    \ or errors that result from identity fraud can be disastrous for those living\
    \ from pay cheque to pay cheque. Sarah Dranoff notes that in addition to financial\
    \ loss, identity theft can lead to \u2018wrongful arrests, loss of utility service,\
    \ erroneous information on health records, improper child support garnishments,\
    \ and harassment by collection agencies\u2019.[42] A number of data breach examples\
    \ are detailed by Solove and Citron: 1) The Office of Policy Management breach\
    \ leaked people\u2019s fingerprints, background check information, and analysis\
    \ of security risks, 2) The Ashley Madison breach released information about people\u2019\
    s extramarital affairs, 3) The Target breach resulted in leaking credit card information,\
    \ bank account numbers and other financial data and 4) the Sony breach involved\
    \ employee email.\nThis regularly updated visualization by the Information is\
    \ Beautiful team demonstrates how common major data breaches are:\n\_\nhttps://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/\n\
    \_\n\nCommercial Data Breaches\nDating sites\nThe Ashley Madison data hack was\
    \ one of the biggest data in online dating site history. In 2015 the personal\
    \ data of more than 37 million users of the site was stolen.[43] Personal details\
    \ of site users was posted by an online hacking group called the Impact Team.[44]\
    \ The sites parent company Avid Live Media (ALM) faced a class action in US court.\
    \ As a result the corporation had to pay nearly quarter of its revenue \u2013\
    \ $11.2 million in settlement.[45] The hack also led to reputational damage, high-profile\
    \ resignations from site users whose names were exposed, divorce filings and two\
    \ suicides of former employees of the company.[46]\nAnother online dating site,\
    \ Adult FriendFinder, was hacked in 2015 and the highly personal data of almost\
    \ 4 million users was leaked online.[47] Just hours after the data was posted\
    \ on a dark web forum, the victims of the hack received spam and threatening emails\
    \ to expose their private information. A year later, the site suffered a second\
    \ hack, this time exposing the information from 412 million accounts.[48]\n\_\n\
    Security companies \nIn August 2019 the Guardian reported that researchers had\
    \ discovered that Suprema\u2019s Biostar 2 database was \u2018unprotected and\
    \ mostly unencrypted\u2019. The researchers said they had access to millions of\
    \ personal records which included fingerprint and facial recognition data and\
    \ usernames and passwords. The system is used by government agencies, defence\
    \ contractors and banks.\n\_\nGovernment Database Breaches \nSwedish Government\
    \ Database \nIn 2016 the Swedish government suffered a massive data breach which\
    \ endangered the identities of undercover operatives. The data breach originated\
    \ from the Swedish transport Agency which exposed the personal information of\
    \ millions of Swedish citizens and the identities of some military personnel.\
    \ The agency previously had contracted a deal with an outsourcing company \u2013\
    \ IBM, and the mishandling of data between the government agency and the private\
    \ company led to a massive leak of sensitive information.[49] The information\
    \ also exposed sensitive information about bridges, roads, ports, subway systems\
    \ in the capital and other key infrastructures. The exposure of sensitive information\
    \ in this case was the result of an absence of proper safeguards and protective\
    \ measures between the government agency and IBM.[50]\nBritish Government Database\n\
    In 2019 the Guardian reported that the fingerprints of \u201Cover 1 million people,\
    \ as well as facial recognition information, unencrypted usernames and passwords,\
    \ and personal information of employees, was discovered on a publicly accessible\
    \ database for a company used by the likes of the UK Metropolitan police, defence\
    \ contractors and banks.\u201D The company involved was called Suprema and the\
    \ breach involved their web-based Biostar 2 biometrics lock system.\nIndia\u2019\
    s Aadhaar Data Breach \nIndia\u2019s ID System Aadhaar has also suffered a data\
    \ breach that exposed the identities of more than a billion people online. In\
    \ 2018 Excel files and documents containing the names, addresses and phone numbers\
    \ of Aadhaar holders were erroneously leaked by various government websites, compromising\
    \ data and giving unauthorised access to personal information of Aadhaar ID\u2019\
    s.[51] A Tribune investigation revealed that the personal and biometric information\
    \ of more than a billion Indian citizens was being sold online for as little as\
    \ 500 rupees or \xA36. Authorities denied the allegations and said that the leaked\
    \ demographic data cannot be misused without biometric information, which was\
    \ kept safe and protected .[52]\n\_\nPhysical injury\nEsther Kaplan\u2019s investigation\
    \ into the effects of workplace data monitoring revealed how the monitoring of\
    \ employees in order to increase their productivity is leading to physical injury\
    \ in some cases. She interviewed a UPS worker who noted that the physical demands\
    \ of his job have increased since the company introduced a telematics system.\
    \ The system monitors employees in real time through tracking devices that include\
    \ \u2018delivery information acquisition devices\u2019 and sensors on delivery\
    \ trucks. The pressure to do more work in less time is leading to injury as drivers\
    \ do not have the time to lift and carry packages properly.[53]\n\_\nPolitical\
    \ uses of Data\nPolitical Manipulation and social harm\nThe damage that can be\
    \ done by fake news, bots and filter bubbles have generated much discussion recently.\
    \ Uses of automated and algorithmic processes in these cases can lead to social\
    \ and political harm as the information that informs citizens is manipulated,\
    \ potentially leading to misinformation and undermining democratic and political\
    \ processes as well as social well-being. A recent study by researchers at the\
    \ Oxford Internet Institute details the diverse ways that people are trying to\
    \ use social media to manipulate public opinion across nine countries. They note\
    \ that this is a concern given the increasing role that social media plays as\
    \ a key information source for citizens, particularly young people. Further, that\
    \ social media are fundamental in many countries to the sharing of political information.\
    \ Civil society groups are \u2018trying, but struggling, to protect themselves\
    \ and respond to active misinformation campaigns\u2019.\nWoolley and Howard define\
    \ computational propaganda as involving \u2018learning from and mimicking real\
    \ people so as to manipulate public opinion across a diverse range of platforms\
    \ and device networks\u2019. Bots, automated programs, are used to spread computational\
    \ propaganda. While bots can be used for legitimate functions, the Internet Institute\
    \ study details how bots can be used to spam, harass, silence opponents, \u2018\
    give the illusion of large-scale consensus\u2019, sway votes, defame critics,\
    \ and spread disinformation campaigns. The authors argue that \u2018computational\
    \ propaganda is one of the most powerful new tools against democracy\u2019.\n\
    Facebook- Cambridge- Analytica Scandal \nIn 2018, through the reporting of Carole\
    \ Cadwalladr, we learned about how Facebook was implicated in political manipulation\
    \ on a grand scale through its involvement with Cambridge Analytica and others.\
    \ Whistleblower Christopher Wylie revealed how the company used the data of more\
    \ than 80 million people to build a profiling system used for political advertising.[54]\
    \ The company allegedly used the psychological profiles for what a CA intern has\
    \ called \u201CPsyops\u201D- psychological operations that, much like in the military,\
    \ are used to affect and change opinion. The use of \u2018dark ads\u2019 on Facebook\
    \ have been linked to Brexit and Trump\u2019s election campaign in the United\
    \ States.\n\_\n\n\n\n\_\nBut the Cambridge Analytica scandal was not only a data\
    \ leak crisis; in fact, it can be argued that it was not a data breach at all,\
    \ as Facebook is designed for this- to collect data, analyze and exploit it.\n\
    \_\nGovernment uses of Data\nExclusion and Error\nBig data blacklisting and watch-lists\
    \ in the U.S. have wrongfully identified individuals. As detailed by Margaret\
    \ Hu, being wrongfully identified in this case can negatively affect employment,\
    \ ability to travel, and in some cases lead to wrongful detention and deportation.[55]\n\
    Hu details the problems with the American E-Verify programme, which \u2018attempts\
    \ to \u201Cverify\u201D the identity or citizenship of a worker based upon complex\
    \ statistical algorithms and multiple databases\u2019. Employers across states\
    \ use the programme to determine if a person is legally able to work in the U.S.\
    \ Hu writes that it appears that employers have wrongfully denied employment for\
    \ thousands. Hu argues that e-verify is problematic due to the unreliability of\
    \ the data that informs the database screening protocol. The problems with the\
    \ e-verify programme have also been detailed by Upturn. A study by the American\
    \ Civil Liberties Union demonstrates that errors are far more likely to affect\
    \ foreign-born employees and citizens with foreign names. People with multiple\
    \ surnames and women who change their names after marriage are also more likely\
    \ to face errors. Harm is further exacerbated by the difficulty in challenging\
    \ or correcting e-verify errors. As discussed by Alex Rosenblat and others: \u2018\
    [L]ow-wage, hourly workers, whether they are flagged for a spelling error or for\
    \ other reasons, often lack the time, resources, or legal literacy required to\
    \ navigate complex bureaucracies to correct misinformation about them in a national\
    \ database\u2019.\nHu also raises concerns about The Prioritised Enforcement Programme\
    \ (PEP), formerly the Secure Communities Programme (S-COMM). This is a data-sharing\
    \ programme between the Federal Bureau of Investigation (FBI), DHS and local law\
    \ enforcement agencies that requires local agencies to run fingerprints taken\
    \ from suspects against federal fingerprint databases (ibid: 1770). The programme\
    \ has made errors. For example, inaccurate database screening results wrongfully\
    \ targeted 5,880 US citizens for potential detention and deportation, leading\
    \ critics to question the reliability of PEP/S-COMM\u2019s algorithms and data.\
    \ Furthermore, by using the biometric data of arrestees contained in the S-COMM\
    \ databases the Immigration and Customs Enforcement (ICE) reportedly may have\
    \ wrongly apprehended approximately 3,600 US citizens, due to faulty information\
    \ feeding database screening protocols. As Hu points out, \u2018error-prone\u2019\
    \ databases and screening protocols \u2018appear to facilitate the unlawful detention\
    \ and deportation of US citizens\u2019.\nHu argues that the big data systems underlying\
    \ both E-Verify and S-COMM/PEP are causing harm by mistakenly targeting and assigning\
    \ inferential guilt to individuals. Legally speaking, this kind of digitally generated\
    \ suspicion is at odds with constitutional rights and there is a growing consensus,\
    \ at least in the U.S, on the need for substantive and binding due process when\
    \ it comes to big data governance.\nIn Arkansas, U.S., the government introduced\
    \ an algorithm to determine how many hours of home care people were entitled to.\
    \ This was something that was previously done by home care nurses. The change\
    \ meant that home care nurses were now required to help people fill in a questionnaire\
    \ with 260 questions. The responses to the questionnaire were then processed by\
    \ an algorithmic system which then determined how many home care hours people\
    \ were entitled to. The result for many was a major reduction in home care hours,\
    \ which drastically limited people\u2019s quality of life and in some cases their\
    \ ability to stay in their own homes. As with other examples listed in this record,\
    \ finding out information about how the algorithm worked proved very difficult.\n\
    Seven of those affected took the government to court with the help of Legal Aid\
    \ of Arkansas. Six of those involved in this case had their home care hours reduced\
    \ by more than 30 percent. There have been ongoing challenges to the use of this\
    \ algorithm and its effects.\n\_\n\n\_\nA similar situation has occurred in Idaho\
    \ where the government started using a data system to determine home care costs\
    \ which led to beneficiaries seeing their funds drastically reduced. Only after\
    \ an ACLU lawsuit did it become clear how limited the data being used was and\
    \ the need for system change.\nA study published in Science magazine in 2019 has\
    \ found that an algorithmic system used to identify follow up health care needs\
    \ of patients across the United States is biased against Black patients \u2013\
    \ the system dramatically underestimates the amount of care Black patients need\
    \ as compared to white patients.\n\_\n\n\_\nConcerns are being raised in the United\
    \ States about how data matching systems are being used as part of a wider strategy\
    \ to disenfranchise African American and Latino voters. In one highly publicized\
    \ example, data matching requirements in Georgia, have been linked to voter suppression\
    \ by civil rights activists and the democratic nominee. According to the \u201C\
    exact match\u201D legislation, the system that processes the voter registration\
    \ applications would only count the votes of the people with the same name or\
    \ address spelling on all documents as legitimate. The changes were introduced\
    \ by Brian Kemp\u2019s office, who at that time was Georgia\u2019s Secretary of\
    \ State and the Republican candidate in the governor\u2019s race. The new regulations\
    \ resulted in 53 000 voter applications being put on hold, as a result of \u201C\
    misspelling of names.\u201D The data inconsistencies and application suspensions\
    \ mostly affected people with foreign names, people with more than one surname,\
    \ those from minority groups and people who have recently changed their surnames\
    \ (newly married women) or have a new address.\n\_\n\n\_\nIn Australia in 2019,\
    \ after years of activism and advocacy, the federal government conceded that the\
    \ automated debt recovery system it had introduced was flawed. Government communications\
    \ suggested that anywhere from 600,000 to 900,000 \u201Crobo-debts\u201D that\
    \ had been issued to people to repay would need to be reassessed. The program\
    \ had at this point already been investigated by the Ombudsman and Senate after\
    \ numerous complaints of errors and unfair targeting of vulnerable people. The\
    \ system uses data matching and income averaging to determine if people have been\
    \ overpaid benefits. Onus was placed on those receiving letters to prove an error\
    \ had been made.\nNumerous accounts of errors were published in the press and\
    \ calls for investigation were taken up by opposition politicians. One case involved\
    \ a man who was repeatedly sent letters saying he owed the government repayment\
    \ of $4,000. This turned out to be an error. The man, who suffers from depression\
    \ and became suicidal, said he successfully convinced the government this was\
    \ an error only to receive a similar letter a few months later. He again successfully\
    \ proved this was an error. One of the ombudsman\u2019s conclusions was that better\
    \ project planning and risk management should have been done from the outset.\n\
    Cassandra Goldie, Chief Executive of the Australian Council of Social Service,\
    \ was quoted in the Guardian as saying:\n[R]obo-debt has issued thousands of debt\
    \ notices in error to parents, people with disabilities, carers and those seeking\
    \ paid work, resulting in people slapped with Centrelink debts they do not owe\
    \ or debts higher than what they owe \u2026 It has been a devastating abuse of\
    \ government power that has caused extensive harm, particularly among people who\
    \ are the most vulnerable in our community.\n\_\n\n\_\nIn October 2019, Virginia\
    \ Eubanks reported a similar practice happening in the United States. In this\
    \ case it is being reported that Government working with tech companies are\_\
    sending out debt notices to thousands of vulnerable people across the country\
    \ that allege\_people have been overpaid benefits. When people have received these\
    \ letters they have few\_options, particularly as challenging the details in the\
    \ letter may require finding pay stubs or\_other documents that are decades old.\
    \ These debts are being called zombie debts because\_of the devastating impact\
    \ they are having on the families forced to repay them, who have\_little ability\
    \ to challenge them. This is despite the fact, that much like the Australia robo-debt\_\
    scandal, people are finding error and \u2018miscalculation\u2019 in these notices.\n\
    \nSocial Exclusion \nSocial exclusion can be perpetuated by many factors including\
    \ identification systems. In a number of countries ethnic groups are being routinely\
    \ excluded and labelled as different through the use of national IDs. Privacy\
    \ International research of national identification systems raises concerns about\
    \ how ID systems can be used in ways that can lead to intentional and unintentional\
    \ exclusion. Such exclusion can lead to great harm by affecting people\u2019s\
    \ survival as ID cards are linked to the ability to access food, fuel, work and\
    \ education.\nIn India, data errors linked to the world\u2019s biggest biometric\
    \ identification system-Aadhaar are being linked to deaths due to starvation as\
    \ people, through data system errors, are being left without access to food and\
    \ other life essentials. In some cases this can be because of data system errors\
    \ such as ID\u2019s not being matched to the right person or people\u2019s finger\
    \ prints not registering. Aadhaar, India\u2019s identification database, contains\
    \ the names, addresses, phone numbers and biometrical specifics (fingerprints,\
    \ palm veins and print, face and iris recognition, DNA, hand geometry, retina)\
    \ of 80% of India\u2019s population.[56] The Aadhaar ID system started as a completely\
    \ voluntary ID card system run by the government on the private servers of HCL,\
    \ but quickly became a vital aspect of identification in India and more and more\
    \ government services have made the use of Aadhaar mandatory. As of 2019 access\
    \ to fuel, food, financial subsidies, health services, job positions and school\
    \ scholarships is open almost exclusively to Aadhaar number holders.\nOther examples\
    \ of data failure include attempts to automate welfare services in the U.S. Virginia\
    \ Eubanks details the system failures that devastated the lives of many in Indiana,\
    \ Florida and Texas at great cost to taxpayers. The automated system errors led\
    \ to people losing access to their Medicaid, food stamps and benefits. The changes\
    \ made to the system led to crisis, hospitalization and as Eubanks reports, death.\
    \ These states cancelled their contracts and were then sued.\n\_\n\n\_\nBig data\
    \ applications used by governments rely on combining multiple data sets. As noted\
    \ by Logan and Ferguson, \u2018small data (i.e. individual level discrete data\
    \ points) \u2026 provides the building blocks for all data-driven systems\u2019\
    . The accuracy of big data applications will be affected by the accuracy of small\
    \ data. We already know there are issues with government data, just two examples:\
    \ 1) in the United States, in 2011 the Los Angeles Times reported that nearly\
    \ 1500 people were unlawfully arrested in the previous five years due to invalid\
    \ warrants and 2) in New York, a Legal Action Center study of rap sheet records\
    \ \u2018found that sixty-two percent contained at least one significant error\
    \ and that thirty-two percent contained multiple errors\u2019. [57]\n\_\nHarms\
    \ due to algorithm / machine bias\nResearch into predictive policing and predictive\
    \ sentencing shows the potential to over-monitor and criminalize marginalized\
    \ communities and the poor.[58]\nJournalists working with ProPublica are investigating\
    \ algorithmic injustice. Their article titled \u2018Machine Bias\u2019 in particular,\
    \ has received a great deal of attention. Julia Angwin, Jeff Larson, Surya Mattu\
    \ and Lauren Kirchner\u2019s investigation was a response to concerns being raised\
    \ by various communities about judicial processes of risk assessment. These processes\
    \ of risk assessment involved computer programs that produce scores predicting\
    \ the likelihood that people charged with crimes would commit future crimes. These\
    \ scores are being integrated throughout the US criminal justice system and influencing\
    \ decisions about bond amounts and sentencing. The ProPublica journalists looked\
    \ at the risk scores assigned to 7,000 people and checked to see how many were\
    \ charged with new crimes. They found that the scores were \u2018remarkably unreliable\
    \ in forecasting violent crime\u2019. They found that only 61%, just over half,\
    \ of those predicted to commit future crimes did. But the big issue is bias. They\
    \ found that the system was much more likely to flag Black defendants as future\
    \ criminals, wrongly labelling them as future criminals at twice the rate as white\
    \ defendants. White people were also wrongly labelled as low risk more often than\
    \ Black defendants. The challenge is that these risk scores and the algorithm\
    \ that determines them is produced by a for profit company, so researchers were\
    \ not able to interrogate the algorithm only the outcomes. ProPublica reports\
    \ that the software is one of the most widely used tools in the country.\nKristian\
    \ Lum and William Isaac, of the Human Rights Data Analysis Group, published an\
    \ article detailing bias in predictive policing. They note that because predictive\
    \ policing tools rely on historical data, predictive policing should be understood\
    \ as predicting where police are likely to make arrests and not necessarily where\
    \ crime is happening. As noted by Lum and Isaac, as well as by O\u2019Neil, if\
    \ nuisance crimes like vagrancy are added to these models this further complicates\
    \ matters and there is an over policing of poor communities, more arrests, and\
    \ you have a feedback loop of injustice. Lum and Isaac used a range of data sources\
    \ to produce an estimate of illicit drug use from non-criminal justice, population\
    \ based data sources which they then compared to police records. They found that\
    \ while drug arrests tend to happen in areas with more BIPOC and low income communities,\
    \ drug use is fairly evenly distributed across all communities. Using one of the\
    \ most popular predictive policing tools, they find that the tool targets Black\
    \ people twice as much as whites even though their data on drug use shows that\
    \ drug use is roughly equivalent across racial classifications. Similarly they\
    \ find that low income households are targeted by police at much higher rates\
    \ than higher income households.\nO\u2019Neil describes how crime prediction software,\
    \ as used by the police in Pennsylvania leads to a biased feedback loop. In this\
    \ case the police include nuisance crimes, such as vagrancy, in their prediction\
    \ model. The inclusion of nuisance crimes, or so-called antisocial behaviour,\
    \ in a model that predicts where future crimes will occur distorts the analysis\
    \ and \u2018creates a pernicious feedback loop\u2019 by drawing more police into\
    \ the areas where there is likely to be vagrancy. This leads to more punishment\
    \ and recorded crimes in these areas, poor areas where there is likely to be vagrancy.\
    \ O\u2019Neil draws attention to specific examples of problems: Pennsylvania police\
    \ use of PredPol, the NYCPD use of CompStat and the Philadelphia police use of\
    \ Hunchlab.[59]\nAmnesty International also carried out an investigation of predictive\
    \ policing algorithms. They published a detailed report about the Gang Matrix\
    \ \u2013 the London Metropolitan Police database- and the implications it has\
    \ on marginalized communities. The Gang Matrix contains information about individuals\
    \ who are suspected gang members in the city of London. Created as a risk-management\
    \ tool after the riots in London in 2011, the database has proven inefficient\
    \ and been criticized as discriminating against young Black men often based on\
    \ nothing more substantial than their cultural preferences. The database has over\
    \ 3800 suspects and gathers intelligence about them from various sources online,\
    \ using data such as the websites the individuals visit, the songs they stream,\
    \ the content they watch on YouTube and more sensitive data such as ethnicity\
    \ and nationality. In 2018 the mayor of London Sadiq Khan commissioned a Review\
    \ of the Metropolitan Police Service Gang Matrix and, according to the paper,\
    \ there is a disproportionate number of Black men included in the list .[60] 78%\
    \ of the individuals on the list are young Black men aged under 25 and altogether\
    \ 80% of all the suspects on the list are Black. In reality, however, only 27%\
    \ of the people actually responsible for gang crime are Black.\n\_\n\n\_\nThe\
    \ highly controversial database perpetuates racial profiling and unjust prosecution\
    \ of people who have not committed any serious offences and can have serious repercussions\
    \ for the individuals, who are being routinely marginalized.[61] The information\
    \ on the database is being shared with jobcentre and housing workers, head teachers\
    \ and school principals and representatives from local hospitals.\nGang labelling\
    \ can not only affect the individuals listed, but their families as well. It also\
    \ can prevent young people from moving on with their lives. In 2012, the Metropolitan\
    \ Police threatened to evict the family of a young Black man that was suspected\
    \ for gang activity. The mother of the young man pursuing education at Cambridge\
    \ University, received a threatening letter from the MPS that the family was going\
    \ to lose their home, because their son was involved in gang activities. Although\
    \ this young man was not associated with the area where he used to live, the \u201C\
    gangster\u201D label continued to follow him even after he tried to move on.[62]\
    \ In 2013 another young Black man was expelled from college, after the college\
    \ authorities found that he had been listed in the Matrix.[63] In another case,\
    \ Paul, a 21 year old graduate was denied the position because his name was still\
    \ on the matrix for an offence committed when he was 12 years old.[64]\n\_\nHow\
    \ can harms be prevented?\nUltimately the goal of this Data Harm Record is to\
    \ stimulate more debate and critical interrogation of how automated and predictive\
    \ data systems are being used across sectors and areas of life.\nThe goal is to\
    \ maintain the Data Harm Record as a running record. Please let us know of any\
    \ cases you think we should add by sending a message here.\nIt is hoped that this\
    \ work contributes to the work of others in this area, many referenced in this\
    \ page, who are trying help us gain a better appreciation of: a) how uses of automated\
    \ and predictive systems are affecting people, b) the kind of datafied world we\
    \ are creating and experiencing, c) the fact that datafication practices affect\
    \ people differently, d) how datafication is political and may lead to practices\
    \ that intentionally or unintentionally discriminate, be unfair, and increase\
    \ inequality and e) how to challenge and redress data harms.\nThere are a range\
    \ of individuals and groups coming together to develop ideas about how data harms\
    \ can be prevented.[65] Researchers, civil society organizations, government bodies\
    \ and activists have all, in different ways, identified the need for greater transparency,\
    \ accountability, systems of oversight and due process, and the means for citizens\
    \ to interrogate and intervene in the datafied processes that affect them. It\
    \ is hoped that this record demonstrates the urgent need for more public debate\
    \ and attention to developing systems of transparency, accountability, oversight\
    \ and citizen intervention.\nFor example, O\u2019Neil argues that auditing should\
    \ be done across the stages of data projects and include auditing: the integrity\
    \ of the data; the terms being used; definitions of success; the accuracy of models;\
    \ who the models fail; the long-term effects of the algorithms being used; and\
    \ the feedback loops created through new big data applications. The Our Data Bodies\
    \ team is based in marginalized communities and interrogating data practices from\
    \ a human rights perspective. We at the Data Justice Lab are working on another\
    \ project, Towards Democratic Auditing, to investigate how to increase citizen\
    \ participation and intervention where these systems are being implemented. AI\
    \ Now, note the need for greater involvement with civil society groups, particularly\
    \ groups advocating for social justice who have long-standing experience identifying\
    \ and challenging the biases embedded in social systems. Researchers at AI Now\
    \ have argued that government uses of automated and artificial intelligence systems\
    \ in the delivery of core services in criminal justice, healthcare, welfare and\
    \ education should stop until the risks and harms can be fully assessed and we\
    \ can decide on where, given the risks involved, there should be no go areas for\
    \ uses of automated systems because the risks are too great.\n\_\n\_\nNotes\n\
    [1] For example see: a) www.datakind.org, b) Gangadharan, SP (2013) \u2018How\
    \ can big data be used for social good\u2019, Guardian, 30 May, available: https://www.theguardian.com/sustainable-business/how-can-big-data-social-good,\
    \ c) Raghupathi, W and Raghupathi, V (2014) \u2018Big data analytics in healthcare:\
    \ promise and potential\u2019 Health Information Science and Systems 2(3), available:\
    \ https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC4341817/ d) Mayer-Sch\xF6nberger,\
    \ Viktor and Cukier, Kenneth. 2013. Big Data: A Revolution That Will Transform\
    \ How We Live, Work, and Think. New York: Houghton Mifflin Harcourt, e) Manyika,\
    \ James, Chui, Michael, Brown, Brad, Bughin, Jacques, Dobbs, Richard, Roxburgh,\
    \ Charles and Hung Byers, Angela. 2011. \u201CBig Data: The Next Frontier for\
    \ Innovation, Competition, and Productivity.\u201D McKinsey Global Institute,\
    \ f) Armah, Nii Ayi. 2013. \u201CBig Data Analysis: The Next Frontier.\u201D Bank\
    \ of Canada Review. Summer.\n[2] Cambridge Dictionary \u2018harm\u2019, available:\
    \ https://dictionary.cambridge.org/dictionary/english/harm, Oxford Living Dictionaries\
    \ \u2018harm\u2019, available: https://en.oxforddictionaries.com/definition/harm\n\
    [3] See Citron, D K and Pasquale, F (2014) The scored society: due process for\
    \ automated Predictions. Washington Law Review, 89: 1-33.\n[4] Medium (2018) Data\
    \ Violence and How Bad Engineering Can Damage Society. [Online]. Available on:\
    \ https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4\n\
    [5] See Lyon, D (2015) Surveillance as Social Sorting: Privacy, Risk and Automated\
    \ Discrimination, New York: Routledge.\n[6] Federal Trade Commission (2015) FTC\
    \ charges data brokers with helping scammer take more than $7 million from Consumers\u2019\
    \ Accounts, 12 August, available: https://www.ftc.gov/news-events/press-releases/2015/08/ftc-charges-data-brokers-helping-scammer-take-more-7-million\n\
    [7] Andrews, Lori. 2013. I Know Who You Are and I Saw What You Did: Social Networks\
    \ and the Death of \nPrivacy, New York: Free Press.\n[8] As cited in Hurley, M\
    \ and Adebayo, J (2016) Credit scoring in the era of big data, Yale Journal of\
    \ Law and \nTechnology, 18(1), p.151.\n[9] Ibid, p. 151\n[10] Office of Oversight\
    \ and Investigations Majority Staff (2013) A Review of the Data Broker Industry:\
    \ Collection, Use, and Sale of Consumer Data for Marketing Purposes, Staff Report\
    \ for Chairman Rockefeller, Dec. 18, available: https://www.commerce.senate.gov/public/_cache/files/0d2b3642-6221-4888-a631-08f2f255b577/AE5D72CBE7F44F5BFC846BECE22C875B.12.18.13-senate-commerce-committee-report-on-data-broker-industry.pdf\n\
    [11] Madden, M, Gilman, M, Levy, K and Marwick, A (2017) \u2018Privacy, Poverty,\
    \ and Big Data: A Matrix of Vulnerabilities for Poor Americans\u2019, Washington\
    \ University Law Review, 95(1)\n[12] Whitener, M (2015) \u2018Cookies are so yesterday;\
    \ Cross-Device Tracking is In \u2013 Some Tips\u2019, Privacy Advisor, 27 Jan.\
    \ available: https://iapp.org/news/a/cookies-are-so-yesterday-cross-device-tracking-is-insome-tips/\n\
    [13] Newman, N (2014) \u2018How big data enables economic harm to consumers, especially\
    \ to low-income and other vulnerable sectors of the population\u2019, Public Comments\
    \ to FTC, available: https://www.ftc.gov/\nsystem/files/documents/public_comments/2014/08/00015-92370.pdf\n\
    [14] As cited in Citron, D K and Pasquale, F (2014) The scored society: due process\
    \ for automated\nPredictions. Washington Law Review, 89, p. 15.\n[15] Ibid\n[16]\
    \ Valentino-DeVries, J, Singer-Vine, J., and Soltani, A (2012) \u2018Watched:\
    \ Websites vary prices, deals based on users\u2019 information\u2019, The Wall\
    \ Street Journal, 24 Dec., A1\n[17] The Guardian (2015) Facebook Still Suspending\
    \ Native Americans Over \u2018Real Name\u2019 Policy. [Online]. Available on:https://www.theguardian.com/technology/2015/feb/16/facebook-real-name-policy-suspends-native-americans?source=post_elevate_sequence_page\u2014\
    \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n[18] The New York Times (2019)\
    \ Amazon Faces Investor Pressure Over Facial Recognition. [Online]. Available\
    \ on: https://www.nytimes.com/2019/05/20/technology/amazon-facial-recognition.html\n\
    [19] The Guardian (2018) Amazon Face Recognition Falsely Matches 28 Lawmakers\
    \ with Mugshots, ACLU says. [Online]. Available on: https://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\n\
    [20] ACLU (2018) Amazon Teams up With Law Enforcement to Deploy Dangerous New\
    \ Face Recognition Technology. [Online]. Available on: https://www.aclunc.org/blog/amazon-teams-law-enforcement-deploy-dangerous-new-face-recognition-technology\n\
    [21] Datta, A, Tschantz, MC and Datta, A (2015) \u2018Automated Experiments on\
    \ Ad Privacy Settings\u2019, Proceedings on Privacy Enhancing Technologies, available:\
    \ https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml\n\
    [22] ProPublica (2016) Facebook Lets Advertisers Exclude Users by Race. [Online].\
    \ Available on: https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race\n\
    [23] BBC (2018) Facebook Accused of Job Ad Gender Discrimination. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/technology-45569227\n[24] Financial Times (2018)\
    \ Facebook \u201CDark Ads\u201D and Discrimination. [Online]. Available on: https://search.proquest.com/docview/2129787570?accountid=9883&rfr_id=info%3Axri%2Fsid%3Aprimo\n\
    [25] The Guardian (2019) Facebook Charged with Housing Discrimination in targeted\
    \ Ads. [Online]. Available on: https://www.theguardian.com/technology/2019/mar/28/facebook-ads-housing-discrimination-charges-us-government-hud\n\
    [26] Ohm, P. (2010). \u201CBroken Promises of Privacy: responding to the surprising\
    \ failure of anonymization\u201D, UCLA Law Review, vol 57 (2010) pp 1701\u2013\
    1777\n[27] Arvind Narayanan & Vitaly Shmatikov (2008), How to Break the Anonymity\
    \ of the Netflix Prize Dataset, available: https://arxiv.org/abs/cs/0610105\n\
    [28] Singel, R (2009) Netflix spilled your Brokeback Mountain secret, lawsuit\
    \ claims, Wired, 17 December, available: https://www.wired.com/2009/12/netflix-privacy-lawsuit/\n\
    [29] The Washington Post (2019) With Fitness Trackers in the Workplace, Bosses\
    \ Can Monitor Your Every Step- And Possibly More. [Online]. Available on: https://www.washingtonpost.com/business/economy/with-fitness-trackers-in-the-workplace-bosses-can-monitor-your-every-step\u2013\
    and-possibly-more/2019/02/15/75ee0848-2a45-11e9-b011-d8500644dc98_story.html?utm_term=.b48be1cf9096\n\
    [30] The Guardian (2019) There\u2019s a Dark Side to Women\u2019s Health Apps:\
    \ \u201CMenstrual Surveillance\u201D. [Online]. Available::https://www.theguardian.com/world/2019/apr/13/theres-a-dark-side-to-womens-health-apps-menstrual-surveillance\n\
    [31] BBC (2018) MyFitnessPal Breach Affects Millions of Under Armour Users. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-43592470\n[32] The Guardian\
    \ (2018) Personal Data of a Billion Indians Sold Online for \xA36, Report Claims.\
    \ [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [33] Reuters (2019) Your Health App Could be Sharing Your Medical Data. [Online].\
    \ Available on: https://www.reuters.com/article/us-health-apps-privacy/your-health-app-could-be-sharing-your-medical-data-idUSKCN1R326W\n\
    [34] The Guardian (2018) Fitness Tracking App Strava Gives Away Locations of Secret\
    \ US Army Bases. [Online]. Available on:https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases\n\
    [35] The Guardian (2018) Strava Suggest Military Users Opt Out of Heatmap as Row\
    \ Deepens. [Online]. Available on:https://www.theguardian.com/technology/2018/jan/29/strava-secret-army-base-locations-heatmap-public-users-military-ban\n\
    [36] The Guardian (2018) Employers Are Monitoring Computers, Toilet Breaks- Even\
    \ Emotions. Is Your Boss Watching You? [Online]. Available on: https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you\n\
    [37] Ibid.\n[38] The Guardian (2018) Alarm Over Talks to Implant UK Employees\
    \ with Microchips. [Online]. Available on: https://www.theguardian.com/technology/2018/nov/11/alarm-over-talks-to-implant-uk-employees-with-microchips\n\
    [39]Bloomberg (2019) Amazon Workers Are Listening to What You Tell Alexa. [Online].\
    \ Available on: https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio\n\
    [40] BBC (2015) Not in Front of the Telly: Warning Over \u2018Listening\u2019\
    \ TV. [Online]. Available on: https://www.bbc.co.uk/news/technology-31296188\n\
    [41] The Guardian (2017) Wikileaks Publishes \u2018Biggest leak Ever of Secret\
    \ CIA Documents\u2019. [Online]. Available on: https://www.theguardian.com/media/2017/mar/07/wikileaks-publishes-biggest-ever-leak-of-secret-cia-documents-hacking-surveillance\n\
    [42] Dranoff, S (2014) \u2018Identity Theft: A Low-Income Issue\u2019, Dialogue,\
    \ Winter, available: https://www.\namericanbar.org/groups/legal_services/publications/dialogue/volume/17/winter-2014/identity-theft\u2013\
    a-lowincome-issue.html\n[43] BBC (2015) Ashley Madison Infidelity Site Customer\
    \ Data Leaked. [Online]. Available on: https://www.bbc.co.uk/news/business-33984017\n\
    [44] The Guardian (2015) Infidelity Site Ashley Madison Hacked as Attackers Demand\
    \ Total Shutdown. [Online]. Available on: https://www.theguardian.com/technology/2015/jul/20/ashley-madison-hacked-cheating-site-total-shutdown\n\
    [45] Reuters (2017) Ashley Madison Parent in $11.2 Million Settlement Over Data\
    \ Breach. [Online]. Available on: https://www.reuters.com/article/us-ashleymadison-settlement-idUSKBN19Z2F0\n\
    [46] BBC (2015) Ashley Madison: \u2018Suicides\u2019 Over Website Hacks. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-34044506\n[47] The Guardian\
    \ (2015) Dating Site Hackers Expose the Details of Millions of Users. [Online].\
    \ Available on: https://www.theguardian.com/lifeandstyle/2015/may/21/adult-friendfinder-dating-site-hackers-expose-users-millions\n\
    [48] BBC (2016) Up To 400 Million Accounts in Adult Friend Finder Breach. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-37974266\n[49] BBC (2017)\
    \ Sweden Data Leak a \u2018Disaster\u2019, Says PM. [Online]. Available on: https://www.bbc.co.uk/news/technology-40705473\n\
    [50] The New York Times (2017) Swedish Government Scrambles to Contain Damage\
    \ From Data Breach. [Online]. Available on:https://www.nytimes.com/2017/07/25/world/europe/ibm-sweden-data-outsourcing.html\n\
    [51] BBC (2018) Aadhaar: \u201CLeak\u201D in World\u2019s Biggest Database Worries\
    \ Indians. [Online]. Available at: https://www.bbc.co.uk/news/world-asia-india-42575443\n\
    [52] The Guardian (2018) Personal Data of a Billion Indians Sold Online for \xA3\
    6, Report Claims. [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [53] Kaplan, E (2015) \u2018The Spy Who Fired me\u2019, Harper\u2019s, March,\
    \ available: https://harpers.org/archive/2015/03/\nthe-spy-who-fired-me/3/\n[54]\
    \ The Guardian (2018) \u201CI Made Steve Bannon\u2019s Psychological Warfare Tool\u201D\
    : Meet the Data War Whistleblower. [Online]. Available on: https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump\n\
    [55] Hu, M. (2015) \u2018Big Data Blacklisting\u2019, Florida Law Review, 67:\
    \ 1735-1809.\n[56] Dixon, P. (2017) A Failure to \u201CDo No Harm\u201D \u2013\
    \ India\u2019s Aadhaar Biometric ID Program and its Inability to Protect Privacy\
    \ in Relation to Measures in Europe and the U.S. Health Technol,7(4): 539-567.\
    \ [Online]. Available on: https://link.springer.com/content/pdf/10.1007%2Fs12553-017-0202-6.pdf\n\
    [57] Logan, WA and Ferguson, AG (2016) \u2018Policing Criminal Justice Data\u2019\
    , Minnesota Law Review 541, available: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2761069\n\
    [58] See: Sullivan, E and Greene, R (2015) States predict inmates\u2019 future\
    \ crimes with secretive\nSurveys. AP, Feb. 24, available at: http://bigstory.ap.org/article/;\
    \ Barocas, S and Selbst, A D (2016) Big data\u2019s disparate impact. California\
    \ Law Review 104: 671-732; Starr, S (2016) The odds of justice: actuarial risk\
    \ prediction and the criminal justice system. Chance 29(1): 49-51.\n[59] O\u2019\
    Neil, C (2016) Weapons of Math Destruction, London: Allen Lane, p. 84-87.\n[60]BBC\
    \ (2018) Met Police \u201CGang Matrix\u201D Requires Overhaul. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/uk-england-london-46646260\n\_\n[61] Evening\
    \ Standard (2018) Sadiq Khan Calls for Overhaul of Scotland Yard\u2019s Gang Matrix\
    \ as 4 in 5 Names on it are Shown to be Black. [Online]. Available on: https://www.standard.co.uk/news/crime/sadiq-khan-calls-for-overhaul-of-scotland-yards-gang-matrix-as-4-in-5-names-on-it-are-shown-to-be-a4024006.html\n\
    [62] Amnesty (2018) Trapped in the Matrix: Secrecy, stigma, and bias in the Met\u2019\
    s Gangs Database. London: Amnesty International United Kingdom Section. Available\
    \ on: https://www.amnesty.org.uk/files/2018-05/Trapped%20in%20the%20Matrix%20Amnesty%20report.pdf?HSxuOpdpZW_8neOqHt_Kxu1DKk_gHtSL\n\
    [63] StopWatch (2018) Being Matrixed: The (Over)policing of Gang Suspects in London.\
    \ [Online]Available on: http://www.stop-watch.org/uploads/documents/Being_Matrixed.pdf\n\
    [64] The Guardian (2018) Met Gang Matrix May be Discriminatory, Review Finds.\
    \ [Online]. Available on: https://www.theguardian.com/uk-news/2018/dec/21/metropolitan-police-gangs-matrix-review-london-mayor-discriminatory\n\
    [65] Throughout the record the hyperlinks provided link to individuals and groups\
    \ whose work raises concerns and also provides recommendations about how to reduce\
    \ data harms. In addition to those links, some examples of others doing work in\
    \ this area include those working as part of the FAT / ML Fairness, Accountability\
    \ and Transparency in Machine Learning group and the Algorithmic Justice League.\n\
    Share this:\nClick to share on X (Opens in new window)\nX\n\nClick to share on\
    \ Facebook (Opens in new window)\nFacebook\nLike this:Like Loading..."
  date: '2017-12-06T13:11:28+00:00'
  id: 5ff307b1c2907c4b7f9515323cb0b767
  publication: Data Justice Lab
  tags: &id001
  - justice
  - data-rights
  - critical-studies
  title: Data Justice Lab
  url: https://datajusticelab.org/data-harm-record/#_ftn1
- author: Unknown Author
  content: Content not found
  date: Unknown Date
  id: 699677195a5c3e61dfc25dd94c18cf66
  publication: Data Justice Lab
  tags: *id001
  title: Unknown Title
  url: https://pdfs.semanticscholar.org/1d17/4f0e3c391368d0f3384a144a6c7487f2a143.pdf
- author: Unknown Author
  content: "Data Harm Record \nData Harm Record (Updated)\nUpdated August 2020\nJoanna\
    \ Redden, Jessica Brand and Vanesa Terzieva\_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\nThe aim of this\
    \ document is to provide a running record of \u2018data harms\u2019, harms that\
    \ have been caused by uses of algorithmic systems. The goal is to document so\
    \ that we can learn from where things have gone wrong and ideally together work\
    \ toward redressing harms and preventing further harm. The document compiles examples\
    \ of harms that have been detailed in previous research and publications. Each\
    \ listed example contains a link to the original source and often also related\
    \ information.\nThe Data Harm Record pulls together concrete examples of harm\
    \ that have been referenced in previous work so that we might gain a better \u2018\
    big picture\u2019 appreciation of how people have already been negatively affected\
    \ by uses of algorithmic systems. A survey of harms also suggests where things\
    \ may go wrong in the future and ideally stimulates more debate and interventions\
    \ into where we may want to change course. The idea is that we can learn a lot\
    \ by paying attention to where things have gone wrong and by considering data\
    \ harms in relation to each other.\nThe Data Harm Record was first published in\
    \ 2017. Over the last year we have attempted to update it with recent examples.\
    \ We have tried to capture a wide range of examples, but there are gaps in what\
    \ we have been able to identify and list here due to time, resource and language\
    \ limitations.\n\_\nBackground\nPeople working in business, government, politics\
    \ and for non-profit organizations are all developing new ways to make use of\
    \ algorithmic systems. These bodies have always collected and analysed data, but\
    \ what\u2019s changed is the size, scope and methods to analyse data. The digitization\
    \ of near everything along with major computing advances mean that it is now possible\
    \ to combine sizes and types of data previously unimaginable, and to then analyse\
    \ these staggering datasets in new ways to find patterns and make predictions.\n\
    There is an abundance of enthusiasm and optimism about how automated, predictive\
    \ and AI data systems can be used for good. Optimism persists for good reason,\
    \ there is a lot of good that can be done through new uses of data systems [1]\
    \ There is also growing consensus that with these new algorithmic systems comes\
    \ risks to individuals and society. Previous work has detailed how data analytics\
    \ can be used in ways that threaten privacy, security, as well as increase inequality\
    \ and discrimination. The danger with automated and predictive decision support\
    \ systems is that harms can be caused unintentionally and intentionally.\nAs argued\
    \ by Cathy O\u2019Neil , this is important to keep in mind as in many cases the\
    \ algorithmic systems that are leading to harm were developed with very good intentions.\
    \ The problem is that new algorithmic tools present new ways to sort, profile,\
    \ exclude, exploit, and discriminate. The complexity, opacity, and proprietary\
    \ nature of many datafied systems mean that often we don\u2019t know things have\
    \ gone wrong until after large numbers of people have been affected. Another problem\
    \ is that few people have the skills needed to interrogate and challenge these\
    \ new automated and predictive systems. What recourse do citizens have if they\
    \ have been wrongfully targeted, profiled, excluded or exploited? Government agencies,\
    \ civil society organizations and researchers across disciplines are drawing attention\
    \ to these risks.\n\_\nDefining data harms\nDictionary definitions of harm link\
    \ it to physical and material injuries, but also to potential injuries, damages\
    \ and adverse effects.[2] Solove and Citron argue that harm can be understood\
    \ as \u2018the impairment, or set back, of a person, entity, or society\u2019\
    s interests. People or entities suffer harm if they are in worse shape than they\
    \ would be had the activity not occurred\u2019.[3]\nBuilding on these definitions,\
    \ one way to understand data harms is as the adverse effects caused by uses of\
    \ data that may impair, injure, or set back a person, entity or society\u2019\
    s interests. While this definition is a start, clearly it is insufficient and\
    \ will need to be developed given the increasing ubiquity of datafied practices\
    \ all around us.\nOur legal and political systems are struggling to come to terms\
    \ with data harms. Across nations it is becoming easier for corporate and government\
    \ bodies to share data internally and externally. New data about us is being generated\
    \ by us and collected by others through new systems. Consider for example the\
    \ range of data that can be generated and collected through the Internet of Things\
    \ and also the range of harms that can be caused if the wrong people hack into\
    \ industrial systems. Increasingly, our digital selves and the digitization of\
    \ services affect the kind of lives we lead, the opportunities afforded to us,\
    \ the services we can access and the ways we are treated. All of these developments\
    \ present new types of risk and harm. For all of these reasons we need to develop\
    \ a more complex understanding and appreciation of data harms and a means to assess\
    \ current and future harms, from the perspective of people who are and may be\
    \ negatively affected by these harms.\n\_\nData violence \nThe harms can be so\
    \ significant that researchers like Anna Lauren Hoffman are arguing that we need\
    \ to go further and recognize that in many cases we are dealing with \u2018data\
    \ violence.\u2019 [4] One example of data violence is when people are wrongly\
    \ denied access to essential services and resources.\n\_We know that algorithms\
    \ and automated systems are increasingly being used in decision- making: for job\
    \ recruitment, risk assessment , credit and bail hearings in the US  among others.\
    \ Research is documenting the ways these systems can embed bias. The implementation\
    \ of algorithmic systems in areas that link people to essential services means\
    \ that the bias and errors introduced via these algorithms can cause significant\
    \ harm. Research demonstrates that the already marginalized are far more likely\
    \ to be negatively affected. To quote Virginia Eubanks: \u201CThese systems impact\
    \ all of us, but they don\u2019t impact us all equally\u201D.\n\_\nExamples\n\
    Commercial uses of data \u2013 Exploitation\nTargeting based on perceived vulnerability\n\
    Some have drawn attention to how new tools make it possible to discriminate and\
    \ socially sort with increasing precision. By combining multiple forms of data\
    \ sets a lot can be learned.[5] Newman calls this \u2018algorithmic profiling\u2019\
    \ and raises concern about how much of this profiling is invisible as citizens\
    \ are unaware of how data is collected about them across searches, transactions,\
    \ site visits, movements, etc. This data can be used to profile and sort people\
    \ into marketing categories, some highly problematic. For example, data brokers\
    \ combine data sets to identify specific groups. Much of this sorting goes under\
    \ the radar. Some of it raises serious concerns. In her testimony to Congress,\
    \ World Privacy Forum\u2019s Pam Dixon reported finding brokers selling lists\
    \ of rape victims, addresses of domestic violence shelters, sufferers of genetic\
    \ diseases, sufferers of addiction and more.\n\nAnother example, in 2015 the U.S.\
    \ Federal Trade Commission \u2018charged a data broker operation with illegally\
    \ selling payday loan applicants\u2019 financial information to a scam operation\
    \ that took millions from consumers by debiting their bank accounts and charging\
    \ their credit cards without their consent\u2019.[6]\n\_\nWhen your personal information\
    \ gets used against you \nConcerns have been raised about how credit card companies\
    \ are using personal details like where someone shops or whether or not they have\
    \ paid for marriage counselling to set rates and limits.[7] This has been called\
    \ \u2018personalization\u2019, or \u2018behavioural analysis\u2019 or \u2018behavioural\
    \ scoring\u2019 and refers to companies tailoring things to people based on what\
    \ is known about them. Croll notes that American Express used purchase history\
    \ to adjust credit limits based on where customers shopped. Croll as well as Hurley\
    \ and Adebayo , describe the case of one man who found his credit rating reduced\
    \ from $10,800 to $3,800 in 2008 because American Express determined that \u2018\
    other customers who ha[d] used their card at establishments where [he] recently\
    \ shopped have a poor repayment history with American Express\u2019.[8] This event,\
    \ in 2008, was an early example of \u2018creditworthiness by association\u2019\
    \ and is linked to ongoing practices of determining value or trustworthiness by\
    \ drawing on \u2018big data\u2019 to make predictions about people.[9]\n\_\nDiscrimination\
    \ \u2013 skin colour, ethnicity, class or religion\nCredit Scoring\nAs companies\
    \ responsible for credit scoring, background checks, and hiring make more use\
    \ of automated data systems, an individual\u2019s appearance, background, personal\
    \ details, social network, or socio-economic status may influence their ability\
    \ to get housing, insurance, access education, or a job.\nThere are new start-up\
    \ companies that make use of a range of \u2018alternative\u2019 data points to\
    \ make predictions about consumers and provide people with credit scores. In addition,\
    \ traditional credit scoring agencies are making use of data and machine learning\
    \ to develop profiles. While the argument is that these tools could open up the\
    \ potential for some not served by traditional credit scoring systems to receive\
    \ credit, there are a range of concerns about how algorithmic scoring may discriminate.\
    \ For example, a consumer\u2019s purchase history could be used, intentionally\
    \ or unintentionally, as a proxy for ethnicity or religion. If an algorithmic\
    \ system ends up penalizing one group more than others it may be hard to figure\
    \ this out given the access issues, opacity and complexity of algorithmic processes.\
    \ While there are laws in place for people to review conventional credit scores,\
    \ there are not yet measures in place for people to interrogate new generated\
    \ scores.\nIn relation to all of these examples, researchers have raised concerns\
    \ about how new data driven processes reproduce illegal redlining practices. Historically,\
    \ redlining was used to discriminate against certain groups of people by denying\
    \ some groups access, or more expensive access, to housing or insurance. This\
    \ was often done by \u2018redlining\u2019 certain communities. The issue is that\
    \ where someone lives is often associated with ethnicity and class. In this way\
    \ location facilitates racism and inequality. Critics are concerned about how\
    \ new automated and predictive data tools can be used to \u2018redline\u2019 given\
    \ the amount of detail that can be determined about us through our data. Previous\
    \ research has demonstrated the potential to accurately determine our age, gender,\
    \ sexuality, ethnicity, religion and political views through the data that can\
    \ be collected and combined about us.\nRelatedly, groups are raising concerns\
    \ about how new data driven processes may facilitate \u2018reverse redlining\u2019\
    . This is when a particular group of people is targeted, as was done with sub-prime\
    \ mortgages. Newman argues that big data was central to the subprime financial\
    \ crash in 2007 as it played a key role in the manipulation of markets but also\
    \ in the subprime mortgage industry. Online advertising and data collected about\
    \ people online was used to direct and target them for sub-prime loans. In 2012\
    \ the American Department of Justice reached a settlement with the Wells Fargo\
    \ Bank concerning allegations that it had \u2018engaged in a pattern or practice\
    \ of discrimination against qualified African-American and Hispanic borrowers\
    \ in its mortgage lending from 2004 through 2009\u2019 by pushing these borrowers\
    \ into more costly sub-prime loans. In the settlement they agreed to provide $184\
    \ million in compensation.\nThe practice of targeting low-income groups continues\
    \ in the payday loan industry. A U.S. Senate Investigation reports that data brokers\
    \ have been found selling lists that focus on citizen financial vulnerability.\
    \ For example, data brokers have compiled the following lists to sell to those\
    \ interested in targeting such groups: \u2018Rural and Barely Making It\u2019\
    , \u2018Ethnic Second-City Strugglers\u2019, \u2018Retiring on Empty: Singles\u2019\
    , \u2018Tough Start: Young Single Parents\u2019. One company was found selling\
    \ a marketing tool to \u2018identify and more effectively market to under-banked\
    \ consumers\u2019.[10]\nAs argued by Madden et al., the fact that those with low-incomes\
    \ are less likely to take privacy protection measures when online and to also\
    \ rely more on their mobile phone for online access places them at greater risk\
    \ than others for online targeting and exploitation.[11] In fact, \u2018opting\
    \ out\u2019 of being tracked becomes increasingly difficult as technologies become\
    \ more sophisticated. New tools that make cross-device tracking possible or that\
    \ are embedded the Internet of Things, mean that the objects we use everyday make\
    \ more of our lives \u2018knowable\u2019 and trackable and make \u2018opting out\u2019\
    \ even harder.[12] Newman raises concerns about how in this age of datafication,\
    \ information inequality is transferred into economic inequality, as companies\
    \ have more information about citizens that can be used to target and exploit\
    \ them to their disadvantage.[13]\nCitron and Pasquale note that \u2018evidence\
    \ suggests that credit scoring does indeed have a negative disparate impact on\
    \ traditionally disadvantaged groups\u2019. They provide a number of examples\
    \ in their article, just one is the case of All-State which was challenged in\
    \ court and agreed to a multi-million dollar settlement over their scoring procedure\
    \ which plaintiffs argued \u2018resulted in discriminatory action against approximately\
    \ five million African-American and Hispanic customers\u2019.[14] They also raise\
    \ concerns about how scoring systems and predictive tools may actually create\
    \ the situations they claim to indicate and \u201Ctake a life\u201D of their own,\
    \ for example by labelling someone a poor candidate or unemployable.[15]\nIn 2015,\
    \ Christian Haigh, a Harvard undergraduate, discovered that the prices for The\
    \ Princeton Review\u2019s online SAT tutoring packages offered to high school\
    \ students varied depending on where customers live. Julia Angwin and Jeff Larson\
    \ of ProPublica investigated Haigh\u2019s findings and found that the highest\
    \ prices were being offered to ZIP codes with a large Asian population and high\
    \ median income. The Princeton Review said that the price difference was not intentional,\
    \ but as noted by ProPublica, the pricing algorithm clearly did discriminate.\
    \ Angwin and Larson note that it is significant that in the United States \u2018\
    unintentional racial discrimination is illegal in housing and employment under\
    \ the legal doctrine known as \u2018disparate impact\u2019 which prohibits inadvertent\
    \ actions that hurt people in a protected class\u2019. However this doctrine does\
    \ not extend to the online world, making it difficult in that country (and others)\
    \ to take legal action against \u2018adverse impact\u2019 caused by unintentional\
    \ algorithmic bias.\nIn 2012, a Wall Street Journal investigation found that Staples\
    \ Inc. website displayed \u2018different prices to people after estimating their\
    \ locations\u2019 and that in what appeared to be an \u2018unintended side effect\u2019\
    \ Staples tended to show discounted prices to areas with a higher average income\
    \ and higher prices to areas with lower average incomes.[16]\nA 2017 investigation\
    \ by ProPublica and Consumer Reports showed that minority neighborhoods pay more\
    \ for car insurance than white neighborhoods with the same risk levels. The study,\
    \ which compared premiums and payouts in California, Illinois, Texas and Missouri,\
    \ showed that minority neighborhoods paid \u2018as much as 30 percent more than\
    \ other areas with similar accident costs\u2019.\n\_\n\nhttp://a.msn.com/00/en-us/BBzv1Xg?ocid=scu2\n\
    \_\nIn 2015 Facebook suspended the accounts of Native Americans because its algorithm\
    \ did not recognize their names as real. [17] The \u201Creal name\u201D policy\
    \ left hundreds of native Americans with suspended accounts and they had to prove\
    \ their identity in order to use their accounts again. Dana Lone Hill was one\
    \ of the Native Americans who had to produce multiple ID documents to prove her\
    \ identity and have her profile reinstated. Her case generated a lot of media\
    \ attention and Facebook had to review its algorithm and eliminate the possibilities\
    \ for discrimination.\n\_\nRecognition technologies\nThere are numerous reports\
    \ of the biases embedded in facial recognition systems: they have problems identifying\
    \ people with darker skin and also with gender. Algorithms that are used to focus\
    \ smartphone cameras, for border security and advertisements sometimes cannot\
    \ identify, or misidentify, people of colour. It has been reported that the problem\
    \ is that the facial recognition algorithms used across various systems have been\
    \ trained using datasets that have mostly male white faces, that these algorithms\
    \ have not been exposed to enough diversity and that this problem is also connected\
    \ to the fact that many of these systems are being developed and tested largely\
    \ by white men. As argued by Joy Buolamwini, the issue of bias and inaccuracy\
    \ becomes increasingly important as facial recognition tools are adopted by police\
    \ and security systems.\n\_\n\n\n\n\_\nExamples of problems include the New Zealand\
    \ case where one man\u2019s passport photograph was rejected when a facial recognition\
    \ program mistakenly identified him as having closed eyes. People have posted\
    \ reviews online raising questions about the ability of Microsoft\u2019s Kinect\
    \ facial recognition feature to recognize people with darker skin and of HP\u2019\
    s tracking webcams \u2018to see Black people\u2019. Recent work by Buolamwini\
    \ and Gebru involved testing tools and found \u201Cdarker skinned females to be\
    \ the most misclassified group.\u201D\nOne of the biggest developers of facial\
    \ recognition software is Amazon and their tool \u2018Rekognition\u2019 has been\
    \ in the centre of debates about machine bias and racial discrimination in AI\
    \ technologies. Amazon\u2019s Rekognition has been designed to cross-reference\
    \ photos of unknown suspects and criminals against a database of mugshots from\
    \ jails in the country .[18] When the software was tested by the American Civil\
    \ Liberties Union of Northern California it was found that people of colour are\
    \ predominantly being misidentified from the mugshot database. The ACLU used photos\
    \ of Members of Congress and cross-referenced them with the mugshot database using\
    \ Amazon\u2019s facial recognition system: 28 members of Congress were misidentified\
    \ as people from the mugshot database and over 40% of them were from ethnic minorities.\
    \ People of colour make up only around 20% of Congress, but the misidentification\
    \ rate with them is two times higher than people of Caucasian origin.[19]\nThe\
    \ ACLU argues that due to concerns about the high risk of misidentification and\
    \ discrimination Amazon\u2019s software unfit and dangerous for use. The technology\
    \ has been sold to the American government and police forces.[20]\n\_\n\n\_\n\
    In August 2019 an American Federal court agreed with a group of Illinois Facebook\
    \ users that Facebook\u2019s use of face recognition technology on their photographs\
    \ without their knowledge or consent violated their privacy rights. The ACLU had\
    \ supported their case, arguing that \u2018performing a scan of an individual\u2019\
    s face without disclosing how that information will be stored, used, or destroyed,\
    \ and without properly obtaining written consent, creates an actionable privacy\
    \ harm. Notice and informed consent empower individuals to protect their privacy\
    \ and are central to privacy laws in the United States, generally, and to BIPA,\
    \ specifically.\u2019\nSasha Costanza-Chock\u2019s work details how \u201Cnorms,\
    \ values, assumptions \u2013 are encoded in and reproduced through the design\
    \ of sociotechnical data-driven systems.\u201D Their essay on the politics of\
    \ border security systems illustrates this as well as the harmful experience of\
    \ confronting the normative politics of these systems. The injustice and harm\
    \ caused by normative security systems has also been stressed by Shadi Petosky\
    \ as has the fact that there are alternatives that are being ignored.\n\_\n Eyeo\
    \ 2019 \u2013 Sasha Costanza-Chock from Eyeo Festival on Vimeo.\n\_\nDiscrimination\
    \ \u2013 gender and ethnicity\nA study of Google ads found that men and women\
    \ are being shown different job adverts, with men receiving ads for higher paying\
    \ jobs more often.[21] The study, which used a tool called AdFisher to set up\
    \ hundreds of simulated user profiles, was designed to investigate the operation\
    \ of Google\u2019s ad settings. Although researchers could determine that men\
    \ and women are being shown different ads, they could not determine why this is\
    \ happening. Doing so would require access to more information that would need\
    \ to be provided by advertisers about who they were targeting and by Google about\
    \ how their system works.\nFacebook allows advertisers to target people based\
    \ on race, ethnicity and gender. Another ProPublica investigation revealed that\
    \ third party companies can target ads to reach people by gender, ethnicity and\
    \ race and also to be hidden from people based on these kinds of classifications.\
    \ [22] A ProPublica investigation identified that Facebook\u2019s ads software\
    \ gives advertising companies the option to exclude men or women from their ad\
    \ demographic. Job positions for Uber and truck drivers, police officers and military\
    \ were all shown to predominantly male audiences, while job vacancies for nurses,\
    \ medical assistants and care-takers targeted women almost exclusively.\n\_\n\n\
    \_\nA complaint by the American Civil Liberties Union (ACLU) submitted to the\
    \ US Equal Opportunity Commission (EEOC) mentions three women from the states\
    \ of Ohio and Illinois who were not shown job advertisements for positions in\
    \ traditionally male-dominated fields which violates federal law of gender. [23]\
    \ The ACLU argued that by targeting only men in already predominantly male fields,\
    \ women are denied the opportunity to break into particular industries.\nSome\
    \ landlords have been found to selectively target different groups of users with\
    \ housing ads, excluding people from \u201Credlined\u201D neighbourhoods, and\
    \ inner-city areas with high rates of Black and Latino residents.[24] The housing\
    \ adverts run by some American landlords were devised to selectively target certain\
    \ communities and exclude people from Hispanic or African-American origin, or\
    \ citizens with bad credit scores . The ads that they publish are invisible for\
    \ the \u201Cexcluded\u201D people, as their ethnicity deems them an \u201Cundesirable\u201D\
    \ demographic. Discriminating based on sensitive factors such as ethnicity and\
    \ nationality in the United States is prohibited by The Fair Housing Act of 1968\
    \ and ads for housing that target based on those characteristics are in fact illegal,\
    \ but nonetheless existing .[25]\n\nDiscrimination \u2013 health\nCathy O\u2019\
    Neil has produced a great deal of work demonstrating how unfair and biased algorithmic\
    \ processes can be. In one example, she tells the story of Kyle Behm, a high achieving\
    \ university student who noticed that he was repeatedly not getting the minimum\
    \ wage jobs he was applying for. All of these job applications required him to\
    \ take personality tests which included questions about mental health. Although\
    \ healthy when looking for work, Behm did suffer from bipolar disorder and had\
    \ taken time out previously to get treatment. Behm\u2019s father is a lawyer and\
    \ he became suspicious of the fairness of these tests for hiring. He decided to\
    \ investigate and found that a lot of companies were using personality tests,\
    \ like the Kronos test. These tests are used as part of automated systems to sort\
    \ through applications and in this process decide which applicants proceed and\
    \ which are \u2018red-lighted\u2019 or discarded. As O\u2019Neil details, these\
    \ tests are often highly complex, with \u2018certain patterns of responses\u2019\
    \ disqualifying people. This example raises a number of ethical questions about\
    \ the use of health information in automated systems but also about the uses of\
    \ automated systems in hiring more generally, particularly as it is unlikely that\
    \ those who have been \u2018red-lighted\u2019 will ever know they were subject\
    \ to an automated system. O\u2019Neil argues that the increasing use of automated\
    \ systems to sort and whittle down job applications creates more unfairness as\
    \ those who know or can pay for help to ensure their applications get to the top\
    \ of the pile have an advantage.\n\_\nLoss of privacy\nThis can happen unintentionally\
    \ when attempts to release data anonymously do not work. Big data makes anonymity\
    \ difficult because it is possible to re-identify data that has been anonymized\
    \ by combining multiple data points.\n\_\nPlatforms\nAs detailed by Paul Ohm,\
    \ in 2006 America Online (AOL) launched \u2018AOL Research\u2019 to \u2018embrace\
    \ the vision of an open research community\u2019. The initiative involved publicly\
    \ releasing twenty million search queries from 650,000 users of AOL\u2019s search\
    \ engine. The data, which represented three months of activity, was posted to\
    \ a public website. Although the data was anonymized, once the data was posted\
    \ some users demonstrated that it was possible to identify people\u2019s identities\
    \ using the data which included name, age and address.\nTwo New York Times reporters\
    \ Michael Barbaro and Tom Zeller Jr. cross-linked data to identify Thelma Arnold,\
    \ a sixty-two year old widow from Lilburn Georgia. Her case demonstrates the problems\
    \ with \u2018anonymisation\u2019 in an age of big data, but also the danger in\
    \ reading too much into search queries. As Barbaro and Zeller note, Ms Arnold\u2019\
    s search queries \u2018hand tremors\u2019, \u2018nicotine effects on the body\u2019\
    , \u2018dry mouth\u2019 and \u2018bipolar\u2019, could lead someone to think she\
    \ suffered from a range of health issues. Such a conclusion could have negative\
    \ effects if the organization making that conclusion was her insurance provider.\
    \ In fact, when they interviewed Arnold, Barbaro and Zeller found that Arnold\
    \ often does searches for her friends because she wants to help them.\nIn 2006\
    \ Netflix publicly released one hundred million records detailing the film ratings\
    \ of 500,000 of its users between Dec. 1999 and Dec. 2005. As Ohm reports, the\
    \ objective was to launch a competition and for those competing to use this data\
    \ to improve Netflix\u2019s recommendation algorithm.[26] Netflix anonymized the\
    \ data by assigning users a unique identifier. Researchers from the University\
    \ of Texas demonstrated not long after this release how relatively easy it was\
    \ for people to be re-identified with the data.[27] This led to a court case in\
    \ which Jane Doe argued that the data could be used to out her sexuality.[28]\
    \ Jane Doe argued that her homosexuality was being revealed by the data as it\
    \ revealed her interest in gay and lesbian themed films. She argued the data outed\
    \ her, a lesbian mother, against her wishes and could damage herself and her family.\
    \ The court case was covered by Wired in 2009.\n\_\nFitness Trackers\nSome employers\
    \ are gaining highly personal information about their employees through their\
    \ use of fitness trackers. Employers now collect health and biometric data about\
    \ their employees through the use of wearable tech and health tracking apps.[29]\
    \ Through performance monitoring employers can now collect regular reports about\
    \ staff activity. Concerns are being raised about companies that are encouraging\
    \ their female employees to use family planning apps that give employers and other\
    \ corporate entities access to details about their employee\u2019s private lives,\
    \ health, hopes and fears.[30]\nFitness apps and trackers allow its users to monitor\
    \ their calorie intake, physical activity and vital signals such as heart rate\
    \ and blood pressure. In 2018 a data leak involving MyFitnessPal exposed the accounts\
    \ of 115 million users after a security breach in their systems.[31] The user\
    \ names, email addresses and scrambled passwords to user accounts were stolen\
    \ from the parent sportswear company Under Armour.[32] Access to the fitness accounts\
    \ means access to vital information that can be used to track individuals, view\
    \ their location in live time, predict behaviour and activities or share sensitive\
    \ health information with third party organisations such as private health clinics,\
    \ insurance companies and even employers [33]\nIn 2018, Strava revealed that fitness\
    \ app data can reveal highly sensitive location information. Heat map visualisations\
    \ released by Strava showed activity captured by the app, lighting up different\
    \ user routes. This mapping involved more than 3 trillion GPS data points.[34]\
    \ A problem is that this app is used by military personnel and by releasing these\
    \ \u2018anonymous\u2019 heat maps Strava was revealing the location of military\
    \ activity. While the information on the heatmaps is an aggregate of all the user\
    \ activities, the Strava website allows the user to track running routes in detail\
    \ and eventually connect them to usernames and the individuals behind them, which\
    \ could endanger military personnel on missions overseas.[35]\nAnother fitness\
    \ tracking app \u2013 Polar Flow, also exposed the geolocation of its users through\
    \ a tool called \u201CExplore map\u201D. An investigation by De Correspondent\
    \ and Bellingcat revealed that the app makes it possible to explore sensitive\
    \ locations and locate individual users and their exercise routines. It turned\
    \ out that tracking an individual behind a username has been made readily available\
    \ and fairly easy and the names and addresses of personnel from intelligence agencies\
    \ such as the NSA, the US Secret Services and the MI6 could be uncovered.\n\_\n\
    \n\_\nSmart microchips\nSome tech companies have found even more intrusive ways\
    \ to monitor their employees. They have started using smart microchips that can\
    \ be implanted under the skin- the same technology that the US justice department\
    \ uses on prisoners on probation instead of ankle braces. The chips work similar\
    \ to the access key cards a lot of firms have, except these key cards do not track\
    \ the workers\u2019 physical and physiological condition. A Chinese mining company\
    \ has even introduced helmets that could read the brainwaves of workers and distinguish\
    \ feelings such as fatigue, distraction and even anger [36]. Earlier last year,\
    \ the American company Three Square Market began putting microchips the size of\
    \ a rice grain in their workers\u2019 hands- from 6-7 chipped employees initially,\
    \ the process expanded to include more the 100 with plans to implant chips on\
    \ all its 10 000 employees.\nThe chips can be used instead of physical IDs to\
    \ open doors, log into computers, pay at the vending machines on site, and can\
    \ be paired with the GPS on employers\u2019 smartphones to show the exact location\
    \ of every employee at any point throughout the day.[37]\nBut unlike fitness tracking\
    \ devices and key cards that could be taken off at the end of the work day, chips\
    \ are worn constantly- giving the employer infinite possibilities to track its\
    \ employees. This invasion of personal space has expanded substantially. BioTeq,\
    \ a UK- based firm is one of the new businesses that offer implants to companies\
    \ and individuals and has implanted more than 150 chips in various firms across\
    \ the UK.[38] Experts and researchers warn that microchip implanting can hide\
    \ a lot of dangers, especially for employees, as it can completely erode their\
    \ right to privacy.\n\_\nSmart devices- smart spies \nAs of 2019, over 100 million\
    \ Amazon Echos have been sold. Amazon employs thousands of people in its headquarters\
    \ to listen to what Alexa has recorded about users.[39] The recordings are transcribed\
    \ and it is argued used to eliminate gaps in Alexa\u2019s communication and introduce\
    \ new accents and words. Together with the conversations that people have had\
    \ with their Alexa devices, the devices pick up other audio in the home. Access\
    \ to the device has been requested by judges in court proceedings. There is increasing\
    \ concern about how the use of such devices can compromise privacy.\n\nConcerns\
    \ are also being raised about Smart TVs.[40] In 2017 Wikileaks published documents\
    \ suggesting a CIA operation: Weeping Angel, which allegedly involved the use\
    \ of smart TV microphones for mass surveillance.[41] Consumer Reports also compiled\
    \ information and advice for the owners of smart TVs about to avoid tracking.\n\
    \nChildren\nCompanies have been facing increasing complaints for collecting data\
    \ about children and sharing or using this to target them with advertisements\
    \ or more content designed to keep them online for longer. In 2019, the American\
    \ Federal Trade Commission ordered Google, including its subsidiary YouTube, to\
    \ pay a record $170 million to settle allegations that YouTube had illegally collected\
    \ data about children without their parent\u2019s consent.\nConcerns are also\
    \ being raised about YouTube\u2019s site for children and the YouTube algorithm,\
    \ which privileges the kind of sensational content that keeps children online\
    \ for longer. Concerns have been raised about the algorthm through privileging\
    \ content like this leading to the promotion of violent and suggestive content.\n\
    The Children\u2019s Commissioner for England published a report highlighting the\
    \ range of data that is being collected about children. The report draws attention\
    \ to the fact that while there is increasing attention to the need to be alert\
    \ to privacy infringements related to online platforms, there is also a need to\
    \ consider the privacy issues and potentials for harms introduced by smart internet\
    \ connected devices, like monitors and toys. Consumer groups have called for some\
    \ smart toys to be re-called after learning that these toys could be hacked to\
    \ enable strangers to talk to children and parents reported that their baby monitors\
    \ were hacked.\n\_\nIdentity theft, blackmail, reputation damage, distress\n\_\
    \nData breaches\nAlthough data breaches are listed under corporate uses of data,\
    \ they could also be listed here under government uses of data as breaches have\
    \ happened in both sectors. Solove and Citron argue that \u2018harm\u2019 in relation\
    \ to data breaches relates to \u2018a risk of future injury, such as identity\
    \ theft, fraud, or damaged reputations\u2019 and also to a current injury as people\
    \ experience anxiety about this future risk. They note that the anxiety and emotional\
    \ distress created about future risk is a harm that people experience \u2018in\
    \ the here and now\u2019. Identity theft is a major problem, particularly for\
    \ those of low-income who lack the resources to pay for legal representation and\
    \ challenge mistakes due to identity fraud. Further, the sudden loss of income\
    \ or errors that result from identity fraud can be disastrous for those living\
    \ from pay cheque to pay cheque. Sarah Dranoff notes that in addition to financial\
    \ loss, identity theft can lead to \u2018wrongful arrests, loss of utility service,\
    \ erroneous information on health records, improper child support garnishments,\
    \ and harassment by collection agencies\u2019.[42] A number of data breach examples\
    \ are detailed by Solove and Citron: 1) The Office of Policy Management breach\
    \ leaked people\u2019s fingerprints, background check information, and analysis\
    \ of security risks, 2) The Ashley Madison breach released information about people\u2019\
    s extramarital affairs, 3) The Target breach resulted in leaking credit card information,\
    \ bank account numbers and other financial data and 4) the Sony breach involved\
    \ employee email.\nThis regularly updated visualization by the Information is\
    \ Beautiful team demonstrates how common major data breaches are:\n\_\nhttps://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/\n\
    \_\n\nCommercial Data Breaches\nDating sites\nThe Ashley Madison data hack was\
    \ one of the biggest data in online dating site history. In 2015 the personal\
    \ data of more than 37 million users of the site was stolen.[43] Personal details\
    \ of site users was posted by an online hacking group called the Impact Team.[44]\
    \ The sites parent company Avid Live Media (ALM) faced a class action in US court.\
    \ As a result the corporation had to pay nearly quarter of its revenue \u2013\
    \ $11.2 million in settlement.[45] The hack also led to reputational damage, high-profile\
    \ resignations from site users whose names were exposed, divorce filings and two\
    \ suicides of former employees of the company.[46]\nAnother online dating site,\
    \ Adult FriendFinder, was hacked in 2015 and the highly personal data of almost\
    \ 4 million users was leaked online.[47] Just hours after the data was posted\
    \ on a dark web forum, the victims of the hack received spam and threatening emails\
    \ to expose their private information. A year later, the site suffered a second\
    \ hack, this time exposing the information from 412 million accounts.[48]\n\_\n\
    Security companies \nIn August 2019 the Guardian reported that researchers had\
    \ discovered that Suprema\u2019s Biostar 2 database was \u2018unprotected and\
    \ mostly unencrypted\u2019. The researchers said they had access to millions of\
    \ personal records which included fingerprint and facial recognition data and\
    \ usernames and passwords. The system is used by government agencies, defence\
    \ contractors and banks.\n\_\nGovernment Database Breaches \nSwedish Government\
    \ Database \nIn 2016 the Swedish government suffered a massive data breach which\
    \ endangered the identities of undercover operatives. The data breach originated\
    \ from the Swedish transport Agency which exposed the personal information of\
    \ millions of Swedish citizens and the identities of some military personnel.\
    \ The agency previously had contracted a deal with an outsourcing company \u2013\
    \ IBM, and the mishandling of data between the government agency and the private\
    \ company led to a massive leak of sensitive information.[49] The information\
    \ also exposed sensitive information about bridges, roads, ports, subway systems\
    \ in the capital and other key infrastructures. The exposure of sensitive information\
    \ in this case was the result of an absence of proper safeguards and protective\
    \ measures between the government agency and IBM.[50]\nBritish Government Database\n\
    In 2019 the Guardian reported that the fingerprints of \u201Cover 1 million people,\
    \ as well as facial recognition information, unencrypted usernames and passwords,\
    \ and personal information of employees, was discovered on a publicly accessible\
    \ database for a company used by the likes of the UK Metropolitan police, defence\
    \ contractors and banks.\u201D The company involved was called Suprema and the\
    \ breach involved their web-based Biostar 2 biometrics lock system.\nIndia\u2019\
    s Aadhaar Data Breach \nIndia\u2019s ID System Aadhaar has also suffered a data\
    \ breach that exposed the identities of more than a billion people online. In\
    \ 2018 Excel files and documents containing the names, addresses and phone numbers\
    \ of Aadhaar holders were erroneously leaked by various government websites, compromising\
    \ data and giving unauthorised access to personal information of Aadhaar ID\u2019\
    s.[51] A Tribune investigation revealed that the personal and biometric information\
    \ of more than a billion Indian citizens was being sold online for as little as\
    \ 500 rupees or \xA36. Authorities denied the allegations and said that the leaked\
    \ demographic data cannot be misused without biometric information, which was\
    \ kept safe and protected .[52]\n\_\nPhysical injury\nEsther Kaplan\u2019s investigation\
    \ into the effects of workplace data monitoring revealed how the monitoring of\
    \ employees in order to increase their productivity is leading to physical injury\
    \ in some cases. She interviewed a UPS worker who noted that the physical demands\
    \ of his job have increased since the company introduced a telematics system.\
    \ The system monitors employees in real time through tracking devices that include\
    \ \u2018delivery information acquisition devices\u2019 and sensors on delivery\
    \ trucks. The pressure to do more work in less time is leading to injury as drivers\
    \ do not have the time to lift and carry packages properly.[53]\n\_\nPolitical\
    \ uses of Data\nPolitical Manipulation and social harm\nThe damage that can be\
    \ done by fake news, bots and filter bubbles have generated much discussion recently.\
    \ Uses of automated and algorithmic processes in these cases can lead to social\
    \ and political harm as the information that informs citizens is manipulated,\
    \ potentially leading to misinformation and undermining democratic and political\
    \ processes as well as social well-being. A recent study by researchers at the\
    \ Oxford Internet Institute details the diverse ways that people are trying to\
    \ use social media to manipulate public opinion across nine countries. They note\
    \ that this is a concern given the increasing role that social media plays as\
    \ a key information source for citizens, particularly young people. Further, that\
    \ social media are fundamental in many countries to the sharing of political information.\
    \ Civil society groups are \u2018trying, but struggling, to protect themselves\
    \ and respond to active misinformation campaigns\u2019.\nWoolley and Howard define\
    \ computational propaganda as involving \u2018learning from and mimicking real\
    \ people so as to manipulate public opinion across a diverse range of platforms\
    \ and device networks\u2019. Bots, automated programs, are used to spread computational\
    \ propaganda. While bots can be used for legitimate functions, the Internet Institute\
    \ study details how bots can be used to spam, harass, silence opponents, \u2018\
    give the illusion of large-scale consensus\u2019, sway votes, defame critics,\
    \ and spread disinformation campaigns. The authors argue that \u2018computational\
    \ propaganda is one of the most powerful new tools against democracy\u2019.\n\
    Facebook- Cambridge- Analytica Scandal \nIn 2018, through the reporting of Carole\
    \ Cadwalladr, we learned about how Facebook was implicated in political manipulation\
    \ on a grand scale through its involvement with Cambridge Analytica and others.\
    \ Whistleblower Christopher Wylie revealed how the company used the data of more\
    \ than 80 million people to build a profiling system used for political advertising.[54]\
    \ The company allegedly used the psychological profiles for what a CA intern has\
    \ called \u201CPsyops\u201D- psychological operations that, much like in the military,\
    \ are used to affect and change opinion. The use of \u2018dark ads\u2019 on Facebook\
    \ have been linked to Brexit and Trump\u2019s election campaign in the United\
    \ States.\n\_\n\n\n\n\_\nBut the Cambridge Analytica scandal was not only a data\
    \ leak crisis; in fact, it can be argued that it was not a data breach at all,\
    \ as Facebook is designed for this- to collect data, analyze and exploit it.\n\
    \_\nGovernment uses of Data\nExclusion and Error\nBig data blacklisting and watch-lists\
    \ in the U.S. have wrongfully identified individuals. As detailed by Margaret\
    \ Hu, being wrongfully identified in this case can negatively affect employment,\
    \ ability to travel, and in some cases lead to wrongful detention and deportation.[55]\n\
    Hu details the problems with the American E-Verify programme, which \u2018attempts\
    \ to \u201Cverify\u201D the identity or citizenship of a worker based upon complex\
    \ statistical algorithms and multiple databases\u2019. Employers across states\
    \ use the programme to determine if a person is legally able to work in the U.S.\
    \ Hu writes that it appears that employers have wrongfully denied employment for\
    \ thousands. Hu argues that e-verify is problematic due to the unreliability of\
    \ the data that informs the database screening protocol. The problems with the\
    \ e-verify programme have also been detailed by Upturn. A study by the American\
    \ Civil Liberties Union demonstrates that errors are far more likely to affect\
    \ foreign-born employees and citizens with foreign names. People with multiple\
    \ surnames and women who change their names after marriage are also more likely\
    \ to face errors. Harm is further exacerbated by the difficulty in challenging\
    \ or correcting e-verify errors. As discussed by Alex Rosenblat and others: \u2018\
    [L]ow-wage, hourly workers, whether they are flagged for a spelling error or for\
    \ other reasons, often lack the time, resources, or legal literacy required to\
    \ navigate complex bureaucracies to correct misinformation about them in a national\
    \ database\u2019.\nHu also raises concerns about The Prioritised Enforcement Programme\
    \ (PEP), formerly the Secure Communities Programme (S-COMM). This is a data-sharing\
    \ programme between the Federal Bureau of Investigation (FBI), DHS and local law\
    \ enforcement agencies that requires local agencies to run fingerprints taken\
    \ from suspects against federal fingerprint databases (ibid: 1770). The programme\
    \ has made errors. For example, inaccurate database screening results wrongfully\
    \ targeted 5,880 US citizens for potential detention and deportation, leading\
    \ critics to question the reliability of PEP/S-COMM\u2019s algorithms and data.\
    \ Furthermore, by using the biometric data of arrestees contained in the S-COMM\
    \ databases the Immigration and Customs Enforcement (ICE) reportedly may have\
    \ wrongly apprehended approximately 3,600 US citizens, due to faulty information\
    \ feeding database screening protocols. As Hu points out, \u2018error-prone\u2019\
    \ databases and screening protocols \u2018appear to facilitate the unlawful detention\
    \ and deportation of US citizens\u2019.\nHu argues that the big data systems underlying\
    \ both E-Verify and S-COMM/PEP are causing harm by mistakenly targeting and assigning\
    \ inferential guilt to individuals. Legally speaking, this kind of digitally generated\
    \ suspicion is at odds with constitutional rights and there is a growing consensus,\
    \ at least in the U.S, on the need for substantive and binding due process when\
    \ it comes to big data governance.\nIn Arkansas, U.S., the government introduced\
    \ an algorithm to determine how many hours of home care people were entitled to.\
    \ This was something that was previously done by home care nurses. The change\
    \ meant that home care nurses were now required to help people fill in a questionnaire\
    \ with 260 questions. The responses to the questionnaire were then processed by\
    \ an algorithmic system which then determined how many home care hours people\
    \ were entitled to. The result for many was a major reduction in home care hours,\
    \ which drastically limited people\u2019s quality of life and in some cases their\
    \ ability to stay in their own homes. As with other examples listed in this record,\
    \ finding out information about how the algorithm worked proved very difficult.\n\
    Seven of those affected took the government to court with the help of Legal Aid\
    \ of Arkansas. Six of those involved in this case had their home care hours reduced\
    \ by more than 30 percent. There have been ongoing challenges to the use of this\
    \ algorithm and its effects.\n\_\n\n\_\nA similar situation has occurred in Idaho\
    \ where the government started using a data system to determine home care costs\
    \ which led to beneficiaries seeing their funds drastically reduced. Only after\
    \ an ACLU lawsuit did it become clear how limited the data being used was and\
    \ the need for system change.\nA study published in Science magazine in 2019 has\
    \ found that an algorithmic system used to identify follow up health care needs\
    \ of patients across the United States is biased against Black patients \u2013\
    \ the system dramatically underestimates the amount of care Black patients need\
    \ as compared to white patients.\n\_\n\n\_\nConcerns are being raised in the United\
    \ States about how data matching systems are being used as part of a wider strategy\
    \ to disenfranchise African American and Latino voters. In one highly publicized\
    \ example, data matching requirements in Georgia, have been linked to voter suppression\
    \ by civil rights activists and the democratic nominee. According to the \u201C\
    exact match\u201D legislation, the system that processes the voter registration\
    \ applications would only count the votes of the people with the same name or\
    \ address spelling on all documents as legitimate. The changes were introduced\
    \ by Brian Kemp\u2019s office, who at that time was Georgia\u2019s Secretary of\
    \ State and the Republican candidate in the governor\u2019s race. The new regulations\
    \ resulted in 53 000 voter applications being put on hold, as a result of \u201C\
    misspelling of names.\u201D The data inconsistencies and application suspensions\
    \ mostly affected people with foreign names, people with more than one surname,\
    \ those from minority groups and people who have recently changed their surnames\
    \ (newly married women) or have a new address.\n\_\n\n\_\nIn Australia in 2019,\
    \ after years of activism and advocacy, the federal government conceded that the\
    \ automated debt recovery system it had introduced was flawed. Government communications\
    \ suggested that anywhere from 600,000 to 900,000 \u201Crobo-debts\u201D that\
    \ had been issued to people to repay would need to be reassessed. The program\
    \ had at this point already been investigated by the Ombudsman and Senate after\
    \ numerous complaints of errors and unfair targeting of vulnerable people. The\
    \ system uses data matching and income averaging to determine if people have been\
    \ overpaid benefits. Onus was placed on those receiving letters to prove an error\
    \ had been made.\nNumerous accounts of errors were published in the press and\
    \ calls for investigation were taken up by opposition politicians. One case involved\
    \ a man who was repeatedly sent letters saying he owed the government repayment\
    \ of $4,000. This turned out to be an error. The man, who suffers from depression\
    \ and became suicidal, said he successfully convinced the government this was\
    \ an error only to receive a similar letter a few months later. He again successfully\
    \ proved this was an error. One of the ombudsman\u2019s conclusions was that better\
    \ project planning and risk management should have been done from the outset.\n\
    Cassandra Goldie, Chief Executive of the Australian Council of Social Service,\
    \ was quoted in the Guardian as saying:\n[R]obo-debt has issued thousands of debt\
    \ notices in error to parents, people with disabilities, carers and those seeking\
    \ paid work, resulting in people slapped with Centrelink debts they do not owe\
    \ or debts higher than what they owe \u2026 It has been a devastating abuse of\
    \ government power that has caused extensive harm, particularly among people who\
    \ are the most vulnerable in our community.\n\_\n\n\_\nIn October 2019, Virginia\
    \ Eubanks reported a similar practice happening in the United States. In this\
    \ case it is being reported that Government working with tech companies are\_\
    sending out debt notices to thousands of vulnerable people across the country\
    \ that allege\_people have been overpaid benefits. When people have received these\
    \ letters they have few\_options, particularly as challenging the details in the\
    \ letter may require finding pay stubs or\_other documents that are decades old.\
    \ These debts are being called zombie debts because\_of the devastating impact\
    \ they are having on the families forced to repay them, who have\_little ability\
    \ to challenge them. This is despite the fact, that much like the Australia robo-debt\_\
    scandal, people are finding error and \u2018miscalculation\u2019 in these notices.\n\
    \nSocial Exclusion \nSocial exclusion can be perpetuated by many factors including\
    \ identification systems. In a number of countries ethnic groups are being routinely\
    \ excluded and labelled as different through the use of national IDs. Privacy\
    \ International research of national identification systems raises concerns about\
    \ how ID systems can be used in ways that can lead to intentional and unintentional\
    \ exclusion. Such exclusion can lead to great harm by affecting people\u2019s\
    \ survival as ID cards are linked to the ability to access food, fuel, work and\
    \ education.\nIn India, data errors linked to the world\u2019s biggest biometric\
    \ identification system-Aadhaar are being linked to deaths due to starvation as\
    \ people, through data system errors, are being left without access to food and\
    \ other life essentials. In some cases this can be because of data system errors\
    \ such as ID\u2019s not being matched to the right person or people\u2019s finger\
    \ prints not registering. Aadhaar, India\u2019s identification database, contains\
    \ the names, addresses, phone numbers and biometrical specifics (fingerprints,\
    \ palm veins and print, face and iris recognition, DNA, hand geometry, retina)\
    \ of 80% of India\u2019s population.[56] The Aadhaar ID system started as a completely\
    \ voluntary ID card system run by the government on the private servers of HCL,\
    \ but quickly became a vital aspect of identification in India and more and more\
    \ government services have made the use of Aadhaar mandatory. As of 2019 access\
    \ to fuel, food, financial subsidies, health services, job positions and school\
    \ scholarships is open almost exclusively to Aadhaar number holders.\nOther examples\
    \ of data failure include attempts to automate welfare services in the U.S. Virginia\
    \ Eubanks details the system failures that devastated the lives of many in Indiana,\
    \ Florida and Texas at great cost to taxpayers. The automated system errors led\
    \ to people losing access to their Medicaid, food stamps and benefits. The changes\
    \ made to the system led to crisis, hospitalization and as Eubanks reports, death.\
    \ These states cancelled their contracts and were then sued.\n\_\n\n\_\nBig data\
    \ applications used by governments rely on combining multiple data sets. As noted\
    \ by Logan and Ferguson, \u2018small data (i.e. individual level discrete data\
    \ points) \u2026 provides the building blocks for all data-driven systems\u2019\
    . The accuracy of big data applications will be affected by the accuracy of small\
    \ data. We already know there are issues with government data, just two examples:\
    \ 1) in the United States, in 2011 the Los Angeles Times reported that nearly\
    \ 1500 people were unlawfully arrested in the previous five years due to invalid\
    \ warrants and 2) in New York, a Legal Action Center study of rap sheet records\
    \ \u2018found that sixty-two percent contained at least one significant error\
    \ and that thirty-two percent contained multiple errors\u2019. [57]\n\_\nHarms\
    \ due to algorithm / machine bias\nResearch into predictive policing and predictive\
    \ sentencing shows the potential to over-monitor and criminalize marginalized\
    \ communities and the poor.[58]\nJournalists working with ProPublica are investigating\
    \ algorithmic injustice. Their article titled \u2018Machine Bias\u2019 in particular,\
    \ has received a great deal of attention. Julia Angwin, Jeff Larson, Surya Mattu\
    \ and Lauren Kirchner\u2019s investigation was a response to concerns being raised\
    \ by various communities about judicial processes of risk assessment. These processes\
    \ of risk assessment involved computer programs that produce scores predicting\
    \ the likelihood that people charged with crimes would commit future crimes. These\
    \ scores are being integrated throughout the US criminal justice system and influencing\
    \ decisions about bond amounts and sentencing. The ProPublica journalists looked\
    \ at the risk scores assigned to 7,000 people and checked to see how many were\
    \ charged with new crimes. They found that the scores were \u2018remarkably unreliable\
    \ in forecasting violent crime\u2019. They found that only 61%, just over half,\
    \ of those predicted to commit future crimes did. But the big issue is bias. They\
    \ found that the system was much more likely to flag Black defendants as future\
    \ criminals, wrongly labelling them as future criminals at twice the rate as white\
    \ defendants. White people were also wrongly labelled as low risk more often than\
    \ Black defendants. The challenge is that these risk scores and the algorithm\
    \ that determines them is produced by a for profit company, so researchers were\
    \ not able to interrogate the algorithm only the outcomes. ProPublica reports\
    \ that the software is one of the most widely used tools in the country.\nKristian\
    \ Lum and William Isaac, of the Human Rights Data Analysis Group, published an\
    \ article detailing bias in predictive policing. They note that because predictive\
    \ policing tools rely on historical data, predictive policing should be understood\
    \ as predicting where police are likely to make arrests and not necessarily where\
    \ crime is happening. As noted by Lum and Isaac, as well as by O\u2019Neil, if\
    \ nuisance crimes like vagrancy are added to these models this further complicates\
    \ matters and there is an over policing of poor communities, more arrests, and\
    \ you have a feedback loop of injustice. Lum and Isaac used a range of data sources\
    \ to produce an estimate of illicit drug use from non-criminal justice, population\
    \ based data sources which they then compared to police records. They found that\
    \ while drug arrests tend to happen in areas with more BIPOC and low income communities,\
    \ drug use is fairly evenly distributed across all communities. Using one of the\
    \ most popular predictive policing tools, they find that the tool targets Black\
    \ people twice as much as whites even though their data on drug use shows that\
    \ drug use is roughly equivalent across racial classifications. Similarly they\
    \ find that low income households are targeted by police at much higher rates\
    \ than higher income households.\nO\u2019Neil describes how crime prediction software,\
    \ as used by the police in Pennsylvania leads to a biased feedback loop. In this\
    \ case the police include nuisance crimes, such as vagrancy, in their prediction\
    \ model. The inclusion of nuisance crimes, or so-called antisocial behaviour,\
    \ in a model that predicts where future crimes will occur distorts the analysis\
    \ and \u2018creates a pernicious feedback loop\u2019 by drawing more police into\
    \ the areas where there is likely to be vagrancy. This leads to more punishment\
    \ and recorded crimes in these areas, poor areas where there is likely to be vagrancy.\
    \ O\u2019Neil draws attention to specific examples of problems: Pennsylvania police\
    \ use of PredPol, the NYCPD use of CompStat and the Philadelphia police use of\
    \ Hunchlab.[59]\nAmnesty International also carried out an investigation of predictive\
    \ policing algorithms. They published a detailed report about the Gang Matrix\
    \ \u2013 the London Metropolitan Police database- and the implications it has\
    \ on marginalized communities. The Gang Matrix contains information about individuals\
    \ who are suspected gang members in the city of London. Created as a risk-management\
    \ tool after the riots in London in 2011, the database has proven inefficient\
    \ and been criticized as discriminating against young Black men often based on\
    \ nothing more substantial than their cultural preferences. The database has over\
    \ 3800 suspects and gathers intelligence about them from various sources online,\
    \ using data such as the websites the individuals visit, the songs they stream,\
    \ the content they watch on YouTube and more sensitive data such as ethnicity\
    \ and nationality. In 2018 the mayor of London Sadiq Khan commissioned a Review\
    \ of the Metropolitan Police Service Gang Matrix and, according to the paper,\
    \ there is a disproportionate number of Black men included in the list .[60] 78%\
    \ of the individuals on the list are young Black men aged under 25 and altogether\
    \ 80% of all the suspects on the list are Black. In reality, however, only 27%\
    \ of the people actually responsible for gang crime are Black.\n\_\n\n\_\nThe\
    \ highly controversial database perpetuates racial profiling and unjust prosecution\
    \ of people who have not committed any serious offences and can have serious repercussions\
    \ for the individuals, who are being routinely marginalized.[61] The information\
    \ on the database is being shared with jobcentre and housing workers, head teachers\
    \ and school principals and representatives from local hospitals.\nGang labelling\
    \ can not only affect the individuals listed, but their families as well. It also\
    \ can prevent young people from moving on with their lives. In 2012, the Metropolitan\
    \ Police threatened to evict the family of a young Black man that was suspected\
    \ for gang activity. The mother of the young man pursuing education at Cambridge\
    \ University, received a threatening letter from the MPS that the family was going\
    \ to lose their home, because their son was involved in gang activities. Although\
    \ this young man was not associated with the area where he used to live, the \u201C\
    gangster\u201D label continued to follow him even after he tried to move on.[62]\
    \ In 2013 another young Black man was expelled from college, after the college\
    \ authorities found that he had been listed in the Matrix.[63] In another case,\
    \ Paul, a 21 year old graduate was denied the position because his name was still\
    \ on the matrix for an offence committed when he was 12 years old.[64]\n\_\nHow\
    \ can harms be prevented?\nUltimately the goal of this Data Harm Record is to\
    \ stimulate more debate and critical interrogation of how automated and predictive\
    \ data systems are being used across sectors and areas of life.\nThe goal is to\
    \ maintain the Data Harm Record as a running record. Please let us know of any\
    \ cases you think we should add by sending a message here.\nIt is hoped that this\
    \ work contributes to the work of others in this area, many referenced in this\
    \ page, who are trying help us gain a better appreciation of: a) how uses of automated\
    \ and predictive systems are affecting people, b) the kind of datafied world we\
    \ are creating and experiencing, c) the fact that datafication practices affect\
    \ people differently, d) how datafication is political and may lead to practices\
    \ that intentionally or unintentionally discriminate, be unfair, and increase\
    \ inequality and e) how to challenge and redress data harms.\nThere are a range\
    \ of individuals and groups coming together to develop ideas about how data harms\
    \ can be prevented.[65] Researchers, civil society organizations, government bodies\
    \ and activists have all, in different ways, identified the need for greater transparency,\
    \ accountability, systems of oversight and due process, and the means for citizens\
    \ to interrogate and intervene in the datafied processes that affect them. It\
    \ is hoped that this record demonstrates the urgent need for more public debate\
    \ and attention to developing systems of transparency, accountability, oversight\
    \ and citizen intervention.\nFor example, O\u2019Neil argues that auditing should\
    \ be done across the stages of data projects and include auditing: the integrity\
    \ of the data; the terms being used; definitions of success; the accuracy of models;\
    \ who the models fail; the long-term effects of the algorithms being used; and\
    \ the feedback loops created through new big data applications. The Our Data Bodies\
    \ team is based in marginalized communities and interrogating data practices from\
    \ a human rights perspective. We at the Data Justice Lab are working on another\
    \ project, Towards Democratic Auditing, to investigate how to increase citizen\
    \ participation and intervention where these systems are being implemented. AI\
    \ Now, note the need for greater involvement with civil society groups, particularly\
    \ groups advocating for social justice who have long-standing experience identifying\
    \ and challenging the biases embedded in social systems. Researchers at AI Now\
    \ have argued that government uses of automated and artificial intelligence systems\
    \ in the delivery of core services in criminal justice, healthcare, welfare and\
    \ education should stop until the risks and harms can be fully assessed and we\
    \ can decide on where, given the risks involved, there should be no go areas for\
    \ uses of automated systems because the risks are too great.\n\_\n\_\nNotes\n\
    [1] For example see: a) www.datakind.org, b) Gangadharan, SP (2013) \u2018How\
    \ can big data be used for social good\u2019, Guardian, 30 May, available: https://www.theguardian.com/sustainable-business/how-can-big-data-social-good,\
    \ c) Raghupathi, W and Raghupathi, V (2014) \u2018Big data analytics in healthcare:\
    \ promise and potential\u2019 Health Information Science and Systems 2(3), available:\
    \ https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC4341817/ d) Mayer-Sch\xF6nberger,\
    \ Viktor and Cukier, Kenneth. 2013. Big Data: A Revolution That Will Transform\
    \ How We Live, Work, and Think. New York: Houghton Mifflin Harcourt, e) Manyika,\
    \ James, Chui, Michael, Brown, Brad, Bughin, Jacques, Dobbs, Richard, Roxburgh,\
    \ Charles and Hung Byers, Angela. 2011. \u201CBig Data: The Next Frontier for\
    \ Innovation, Competition, and Productivity.\u201D McKinsey Global Institute,\
    \ f) Armah, Nii Ayi. 2013. \u201CBig Data Analysis: The Next Frontier.\u201D Bank\
    \ of Canada Review. Summer.\n[2] Cambridge Dictionary \u2018harm\u2019, available:\
    \ https://dictionary.cambridge.org/dictionary/english/harm, Oxford Living Dictionaries\
    \ \u2018harm\u2019, available: https://en.oxforddictionaries.com/definition/harm\n\
    [3] See Citron, D K and Pasquale, F (2014) The scored society: due process for\
    \ automated Predictions. Washington Law Review, 89: 1-33.\n[4] Medium (2018) Data\
    \ Violence and How Bad Engineering Can Damage Society. [Online]. Available on:\
    \ https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4\n\
    [5] See Lyon, D (2015) Surveillance as Social Sorting: Privacy, Risk and Automated\
    \ Discrimination, New York: Routledge.\n[6] Federal Trade Commission (2015) FTC\
    \ charges data brokers with helping scammer take more than $7 million from Consumers\u2019\
    \ Accounts, 12 August, available: https://www.ftc.gov/news-events/press-releases/2015/08/ftc-charges-data-brokers-helping-scammer-take-more-7-million\n\
    [7] Andrews, Lori. 2013. I Know Who You Are and I Saw What You Did: Social Networks\
    \ and the Death of \nPrivacy, New York: Free Press.\n[8] As cited in Hurley, M\
    \ and Adebayo, J (2016) Credit scoring in the era of big data, Yale Journal of\
    \ Law and \nTechnology, 18(1), p.151.\n[9] Ibid, p. 151\n[10] Office of Oversight\
    \ and Investigations Majority Staff (2013) A Review of the Data Broker Industry:\
    \ Collection, Use, and Sale of Consumer Data for Marketing Purposes, Staff Report\
    \ for Chairman Rockefeller, Dec. 18, available: https://www.commerce.senate.gov/public/_cache/files/0d2b3642-6221-4888-a631-08f2f255b577/AE5D72CBE7F44F5BFC846BECE22C875B.12.18.13-senate-commerce-committee-report-on-data-broker-industry.pdf\n\
    [11] Madden, M, Gilman, M, Levy, K and Marwick, A (2017) \u2018Privacy, Poverty,\
    \ and Big Data: A Matrix of Vulnerabilities for Poor Americans\u2019, Washington\
    \ University Law Review, 95(1)\n[12] Whitener, M (2015) \u2018Cookies are so yesterday;\
    \ Cross-Device Tracking is In \u2013 Some Tips\u2019, Privacy Advisor, 27 Jan.\
    \ available: https://iapp.org/news/a/cookies-are-so-yesterday-cross-device-tracking-is-insome-tips/\n\
    [13] Newman, N (2014) \u2018How big data enables economic harm to consumers, especially\
    \ to low-income and other vulnerable sectors of the population\u2019, Public Comments\
    \ to FTC, available: https://www.ftc.gov/\nsystem/files/documents/public_comments/2014/08/00015-92370.pdf\n\
    [14] As cited in Citron, D K and Pasquale, F (2014) The scored society: due process\
    \ for automated\nPredictions. Washington Law Review, 89, p. 15.\n[15] Ibid\n[16]\
    \ Valentino-DeVries, J, Singer-Vine, J., and Soltani, A (2012) \u2018Watched:\
    \ Websites vary prices, deals based on users\u2019 information\u2019, The Wall\
    \ Street Journal, 24 Dec., A1\n[17] The Guardian (2015) Facebook Still Suspending\
    \ Native Americans Over \u2018Real Name\u2019 Policy. [Online]. Available on:https://www.theguardian.com/technology/2015/feb/16/facebook-real-name-policy-suspends-native-americans?source=post_elevate_sequence_page\u2014\
    \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n[18] The New York Times (2019)\
    \ Amazon Faces Investor Pressure Over Facial Recognition. [Online]. Available\
    \ on: https://www.nytimes.com/2019/05/20/technology/amazon-facial-recognition.html\n\
    [19] The Guardian (2018) Amazon Face Recognition Falsely Matches 28 Lawmakers\
    \ with Mugshots, ACLU says. [Online]. Available on: https://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\n\
    [20] ACLU (2018) Amazon Teams up With Law Enforcement to Deploy Dangerous New\
    \ Face Recognition Technology. [Online]. Available on: https://www.aclunc.org/blog/amazon-teams-law-enforcement-deploy-dangerous-new-face-recognition-technology\n\
    [21] Datta, A, Tschantz, MC and Datta, A (2015) \u2018Automated Experiments on\
    \ Ad Privacy Settings\u2019, Proceedings on Privacy Enhancing Technologies, available:\
    \ https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml\n\
    [22] ProPublica (2016) Facebook Lets Advertisers Exclude Users by Race. [Online].\
    \ Available on: https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race\n\
    [23] BBC (2018) Facebook Accused of Job Ad Gender Discrimination. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/technology-45569227\n[24] Financial Times (2018)\
    \ Facebook \u201CDark Ads\u201D and Discrimination. [Online]. Available on: https://search.proquest.com/docview/2129787570?accountid=9883&rfr_id=info%3Axri%2Fsid%3Aprimo\n\
    [25] The Guardian (2019) Facebook Charged with Housing Discrimination in targeted\
    \ Ads. [Online]. Available on: https://www.theguardian.com/technology/2019/mar/28/facebook-ads-housing-discrimination-charges-us-government-hud\n\
    [26] Ohm, P. (2010). \u201CBroken Promises of Privacy: responding to the surprising\
    \ failure of anonymization\u201D, UCLA Law Review, vol 57 (2010) pp 1701\u2013\
    1777\n[27] Arvind Narayanan & Vitaly Shmatikov (2008), How to Break the Anonymity\
    \ of the Netflix Prize Dataset, available: https://arxiv.org/abs/cs/0610105\n\
    [28] Singel, R (2009) Netflix spilled your Brokeback Mountain secret, lawsuit\
    \ claims, Wired, 17 December, available: https://www.wired.com/2009/12/netflix-privacy-lawsuit/\n\
    [29] The Washington Post (2019) With Fitness Trackers in the Workplace, Bosses\
    \ Can Monitor Your Every Step- And Possibly More. [Online]. Available on: https://www.washingtonpost.com/business/economy/with-fitness-trackers-in-the-workplace-bosses-can-monitor-your-every-step\u2013\
    and-possibly-more/2019/02/15/75ee0848-2a45-11e9-b011-d8500644dc98_story.html?utm_term=.b48be1cf9096\n\
    [30] The Guardian (2019) There\u2019s a Dark Side to Women\u2019s Health Apps:\
    \ \u201CMenstrual Surveillance\u201D. [Online]. Available::https://www.theguardian.com/world/2019/apr/13/theres-a-dark-side-to-womens-health-apps-menstrual-surveillance\n\
    [31] BBC (2018) MyFitnessPal Breach Affects Millions of Under Armour Users. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-43592470\n[32] The Guardian\
    \ (2018) Personal Data of a Billion Indians Sold Online for \xA36, Report Claims.\
    \ [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [33] Reuters (2019) Your Health App Could be Sharing Your Medical Data. [Online].\
    \ Available on: https://www.reuters.com/article/us-health-apps-privacy/your-health-app-could-be-sharing-your-medical-data-idUSKCN1R326W\n\
    [34] The Guardian (2018) Fitness Tracking App Strava Gives Away Locations of Secret\
    \ US Army Bases. [Online]. Available on:https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases\n\
    [35] The Guardian (2018) Strava Suggest Military Users Opt Out of Heatmap as Row\
    \ Deepens. [Online]. Available on:https://www.theguardian.com/technology/2018/jan/29/strava-secret-army-base-locations-heatmap-public-users-military-ban\n\
    [36] The Guardian (2018) Employers Are Monitoring Computers, Toilet Breaks- Even\
    \ Emotions. Is Your Boss Watching You? [Online]. Available on: https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you\n\
    [37] Ibid.\n[38] The Guardian (2018) Alarm Over Talks to Implant UK Employees\
    \ with Microchips. [Online]. Available on: https://www.theguardian.com/technology/2018/nov/11/alarm-over-talks-to-implant-uk-employees-with-microchips\n\
    [39]Bloomberg (2019) Amazon Workers Are Listening to What You Tell Alexa. [Online].\
    \ Available on: https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio\n\
    [40] BBC (2015) Not in Front of the Telly: Warning Over \u2018Listening\u2019\
    \ TV. [Online]. Available on: https://www.bbc.co.uk/news/technology-31296188\n\
    [41] The Guardian (2017) Wikileaks Publishes \u2018Biggest leak Ever of Secret\
    \ CIA Documents\u2019. [Online]. Available on: https://www.theguardian.com/media/2017/mar/07/wikileaks-publishes-biggest-ever-leak-of-secret-cia-documents-hacking-surveillance\n\
    [42] Dranoff, S (2014) \u2018Identity Theft: A Low-Income Issue\u2019, Dialogue,\
    \ Winter, available: https://www.\namericanbar.org/groups/legal_services/publications/dialogue/volume/17/winter-2014/identity-theft\u2013\
    a-lowincome-issue.html\n[43] BBC (2015) Ashley Madison Infidelity Site Customer\
    \ Data Leaked. [Online]. Available on: https://www.bbc.co.uk/news/business-33984017\n\
    [44] The Guardian (2015) Infidelity Site Ashley Madison Hacked as Attackers Demand\
    \ Total Shutdown. [Online]. Available on: https://www.theguardian.com/technology/2015/jul/20/ashley-madison-hacked-cheating-site-total-shutdown\n\
    [45] Reuters (2017) Ashley Madison Parent in $11.2 Million Settlement Over Data\
    \ Breach. [Online]. Available on: https://www.reuters.com/article/us-ashleymadison-settlement-idUSKBN19Z2F0\n\
    [46] BBC (2015) Ashley Madison: \u2018Suicides\u2019 Over Website Hacks. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-34044506\n[47] The Guardian\
    \ (2015) Dating Site Hackers Expose the Details of Millions of Users. [Online].\
    \ Available on: https://www.theguardian.com/lifeandstyle/2015/may/21/adult-friendfinder-dating-site-hackers-expose-users-millions\n\
    [48] BBC (2016) Up To 400 Million Accounts in Adult Friend Finder Breach. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-37974266\n[49] BBC (2017)\
    \ Sweden Data Leak a \u2018Disaster\u2019, Says PM. [Online]. Available on: https://www.bbc.co.uk/news/technology-40705473\n\
    [50] The New York Times (2017) Swedish Government Scrambles to Contain Damage\
    \ From Data Breach. [Online]. Available on:https://www.nytimes.com/2017/07/25/world/europe/ibm-sweden-data-outsourcing.html\n\
    [51] BBC (2018) Aadhaar: \u201CLeak\u201D in World\u2019s Biggest Database Worries\
    \ Indians. [Online]. Available at: https://www.bbc.co.uk/news/world-asia-india-42575443\n\
    [52] The Guardian (2018) Personal Data of a Billion Indians Sold Online for \xA3\
    6, Report Claims. [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [53] Kaplan, E (2015) \u2018The Spy Who Fired me\u2019, Harper\u2019s, March,\
    \ available: https://harpers.org/archive/2015/03/\nthe-spy-who-fired-me/3/\n[54]\
    \ The Guardian (2018) \u201CI Made Steve Bannon\u2019s Psychological Warfare Tool\u201D\
    : Meet the Data War Whistleblower. [Online]. Available on: https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump\n\
    [55] Hu, M. (2015) \u2018Big Data Blacklisting\u2019, Florida Law Review, 67:\
    \ 1735-1809.\n[56] Dixon, P. (2017) A Failure to \u201CDo No Harm\u201D \u2013\
    \ India\u2019s Aadhaar Biometric ID Program and its Inability to Protect Privacy\
    \ in Relation to Measures in Europe and the U.S. Health Technol,7(4): 539-567.\
    \ [Online]. Available on: https://link.springer.com/content/pdf/10.1007%2Fs12553-017-0202-6.pdf\n\
    [57] Logan, WA and Ferguson, AG (2016) \u2018Policing Criminal Justice Data\u2019\
    , Minnesota Law Review 541, available: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2761069\n\
    [58] See: Sullivan, E and Greene, R (2015) States predict inmates\u2019 future\
    \ crimes with secretive\nSurveys. AP, Feb. 24, available at: http://bigstory.ap.org/article/;\
    \ Barocas, S and Selbst, A D (2016) Big data\u2019s disparate impact. California\
    \ Law Review 104: 671-732; Starr, S (2016) The odds of justice: actuarial risk\
    \ prediction and the criminal justice system. Chance 29(1): 49-51.\n[59] O\u2019\
    Neil, C (2016) Weapons of Math Destruction, London: Allen Lane, p. 84-87.\n[60]BBC\
    \ (2018) Met Police \u201CGang Matrix\u201D Requires Overhaul. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/uk-england-london-46646260\n\_\n[61] Evening\
    \ Standard (2018) Sadiq Khan Calls for Overhaul of Scotland Yard\u2019s Gang Matrix\
    \ as 4 in 5 Names on it are Shown to be Black. [Online]. Available on: https://www.standard.co.uk/news/crime/sadiq-khan-calls-for-overhaul-of-scotland-yards-gang-matrix-as-4-in-5-names-on-it-are-shown-to-be-a4024006.html\n\
    [62] Amnesty (2018) Trapped in the Matrix: Secrecy, stigma, and bias in the Met\u2019\
    s Gangs Database. London: Amnesty International United Kingdom Section. Available\
    \ on: https://www.amnesty.org.uk/files/2018-05/Trapped%20in%20the%20Matrix%20Amnesty%20report.pdf?HSxuOpdpZW_8neOqHt_Kxu1DKk_gHtSL\n\
    [63] StopWatch (2018) Being Matrixed: The (Over)policing of Gang Suspects in London.\
    \ [Online]Available on: http://www.stop-watch.org/uploads/documents/Being_Matrixed.pdf\n\
    [64] The Guardian (2018) Met Gang Matrix May be Discriminatory, Review Finds.\
    \ [Online]. Available on: https://www.theguardian.com/uk-news/2018/dec/21/metropolitan-police-gangs-matrix-review-london-mayor-discriminatory\n\
    [65] Throughout the record the hyperlinks provided link to individuals and groups\
    \ whose work raises concerns and also provides recommendations about how to reduce\
    \ data harms. In addition to those links, some examples of others doing work in\
    \ this area include those working as part of the FAT / ML Fairness, Accountability\
    \ and Transparency in Machine Learning group and the Algorithmic Justice League.\n\
    Share this:\nClick to share on X (Opens in new window)\nX\n\nClick to share on\
    \ Facebook (Opens in new window)\nFacebook\nLike this:Like Loading..."
  date: '2017-12-06T13:11:28+00:00'
  id: 8863b0cf041c9cedc81d316399c2c433
  publication: Data Justice Lab
  tags: *id001
  title: Data Justice Lab
  url: https://datajusticelab.org/data-harm-record/#_ftn2
- author: 'Daniel J. Solove (Contact Author)



    George Washington University Law School  ( email )


    2000 H Street, N.W.Washington, DC  20052United States202-994-9514 (Phone)



    HOME PAGE: http://danielsolove.com'
  content: Content not found
  date: Unknown Date
  id: 2e6034e1d5cd3166985c1069d5c0ec1a
  publication: Data Justice Lab
  tags: *id001
  title: 'Risk and Anxiety: A Theory of Data Breach Harms'
  url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2885638
- author: Unknown Author
  content: "Data Harm Record \nData Harm Record (Updated)\nUpdated August 2020\nJoanna\
    \ Redden, Jessica Brand and Vanesa Terzieva\_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\nThe aim of this\
    \ document is to provide a running record of \u2018data harms\u2019, harms that\
    \ have been caused by uses of algorithmic systems. The goal is to document so\
    \ that we can learn from where things have gone wrong and ideally together work\
    \ toward redressing harms and preventing further harm. The document compiles examples\
    \ of harms that have been detailed in previous research and publications. Each\
    \ listed example contains a link to the original source and often also related\
    \ information.\nThe Data Harm Record pulls together concrete examples of harm\
    \ that have been referenced in previous work so that we might gain a better \u2018\
    big picture\u2019 appreciation of how people have already been negatively affected\
    \ by uses of algorithmic systems. A survey of harms also suggests where things\
    \ may go wrong in the future and ideally stimulates more debate and interventions\
    \ into where we may want to change course. The idea is that we can learn a lot\
    \ by paying attention to where things have gone wrong and by considering data\
    \ harms in relation to each other.\nThe Data Harm Record was first published in\
    \ 2017. Over the last year we have attempted to update it with recent examples.\
    \ We have tried to capture a wide range of examples, but there are gaps in what\
    \ we have been able to identify and list here due to time, resource and language\
    \ limitations.\n\_\nBackground\nPeople working in business, government, politics\
    \ and for non-profit organizations are all developing new ways to make use of\
    \ algorithmic systems. These bodies have always collected and analysed data, but\
    \ what\u2019s changed is the size, scope and methods to analyse data. The digitization\
    \ of near everything along with major computing advances mean that it is now possible\
    \ to combine sizes and types of data previously unimaginable, and to then analyse\
    \ these staggering datasets in new ways to find patterns and make predictions.\n\
    There is an abundance of enthusiasm and optimism about how automated, predictive\
    \ and AI data systems can be used for good. Optimism persists for good reason,\
    \ there is a lot of good that can be done through new uses of data systems [1]\
    \ There is also growing consensus that with these new algorithmic systems comes\
    \ risks to individuals and society. Previous work has detailed how data analytics\
    \ can be used in ways that threaten privacy, security, as well as increase inequality\
    \ and discrimination. The danger with automated and predictive decision support\
    \ systems is that harms can be caused unintentionally and intentionally.\nAs argued\
    \ by Cathy O\u2019Neil , this is important to keep in mind as in many cases the\
    \ algorithmic systems that are leading to harm were developed with very good intentions.\
    \ The problem is that new algorithmic tools present new ways to sort, profile,\
    \ exclude, exploit, and discriminate. The complexity, opacity, and proprietary\
    \ nature of many datafied systems mean that often we don\u2019t know things have\
    \ gone wrong until after large numbers of people have been affected. Another problem\
    \ is that few people have the skills needed to interrogate and challenge these\
    \ new automated and predictive systems. What recourse do citizens have if they\
    \ have been wrongfully targeted, profiled, excluded or exploited? Government agencies,\
    \ civil society organizations and researchers across disciplines are drawing attention\
    \ to these risks.\n\_\nDefining data harms\nDictionary definitions of harm link\
    \ it to physical and material injuries, but also to potential injuries, damages\
    \ and adverse effects.[2] Solove and Citron argue that harm can be understood\
    \ as \u2018the impairment, or set back, of a person, entity, or society\u2019\
    s interests. People or entities suffer harm if they are in worse shape than they\
    \ would be had the activity not occurred\u2019.[3]\nBuilding on these definitions,\
    \ one way to understand data harms is as the adverse effects caused by uses of\
    \ data that may impair, injure, or set back a person, entity or society\u2019\
    s interests. While this definition is a start, clearly it is insufficient and\
    \ will need to be developed given the increasing ubiquity of datafied practices\
    \ all around us.\nOur legal and political systems are struggling to come to terms\
    \ with data harms. Across nations it is becoming easier for corporate and government\
    \ bodies to share data internally and externally. New data about us is being generated\
    \ by us and collected by others through new systems. Consider for example the\
    \ range of data that can be generated and collected through the Internet of Things\
    \ and also the range of harms that can be caused if the wrong people hack into\
    \ industrial systems. Increasingly, our digital selves and the digitization of\
    \ services affect the kind of lives we lead, the opportunities afforded to us,\
    \ the services we can access and the ways we are treated. All of these developments\
    \ present new types of risk and harm. For all of these reasons we need to develop\
    \ a more complex understanding and appreciation of data harms and a means to assess\
    \ current and future harms, from the perspective of people who are and may be\
    \ negatively affected by these harms.\n\_\nData violence \nThe harms can be so\
    \ significant that researchers like Anna Lauren Hoffman are arguing that we need\
    \ to go further and recognize that in many cases we are dealing with \u2018data\
    \ violence.\u2019 [4] One example of data violence is when people are wrongly\
    \ denied access to essential services and resources.\n\_We know that algorithms\
    \ and automated systems are increasingly being used in decision- making: for job\
    \ recruitment, risk assessment , credit and bail hearings in the US  among others.\
    \ Research is documenting the ways these systems can embed bias. The implementation\
    \ of algorithmic systems in areas that link people to essential services means\
    \ that the bias and errors introduced via these algorithms can cause significant\
    \ harm. Research demonstrates that the already marginalized are far more likely\
    \ to be negatively affected. To quote Virginia Eubanks: \u201CThese systems impact\
    \ all of us, but they don\u2019t impact us all equally\u201D.\n\_\nExamples\n\
    Commercial uses of data \u2013 Exploitation\nTargeting based on perceived vulnerability\n\
    Some have drawn attention to how new tools make it possible to discriminate and\
    \ socially sort with increasing precision. By combining multiple forms of data\
    \ sets a lot can be learned.[5] Newman calls this \u2018algorithmic profiling\u2019\
    \ and raises concern about how much of this profiling is invisible as citizens\
    \ are unaware of how data is collected about them across searches, transactions,\
    \ site visits, movements, etc. This data can be used to profile and sort people\
    \ into marketing categories, some highly problematic. For example, data brokers\
    \ combine data sets to identify specific groups. Much of this sorting goes under\
    \ the radar. Some of it raises serious concerns. In her testimony to Congress,\
    \ World Privacy Forum\u2019s Pam Dixon reported finding brokers selling lists\
    \ of rape victims, addresses of domestic violence shelters, sufferers of genetic\
    \ diseases, sufferers of addiction and more.\n\nAnother example, in 2015 the U.S.\
    \ Federal Trade Commission \u2018charged a data broker operation with illegally\
    \ selling payday loan applicants\u2019 financial information to a scam operation\
    \ that took millions from consumers by debiting their bank accounts and charging\
    \ their credit cards without their consent\u2019.[6]\n\_\nWhen your personal information\
    \ gets used against you \nConcerns have been raised about how credit card companies\
    \ are using personal details like where someone shops or whether or not they have\
    \ paid for marriage counselling to set rates and limits.[7] This has been called\
    \ \u2018personalization\u2019, or \u2018behavioural analysis\u2019 or \u2018behavioural\
    \ scoring\u2019 and refers to companies tailoring things to people based on what\
    \ is known about them. Croll notes that American Express used purchase history\
    \ to adjust credit limits based on where customers shopped. Croll as well as Hurley\
    \ and Adebayo , describe the case of one man who found his credit rating reduced\
    \ from $10,800 to $3,800 in 2008 because American Express determined that \u2018\
    other customers who ha[d] used their card at establishments where [he] recently\
    \ shopped have a poor repayment history with American Express\u2019.[8] This event,\
    \ in 2008, was an early example of \u2018creditworthiness by association\u2019\
    \ and is linked to ongoing practices of determining value or trustworthiness by\
    \ drawing on \u2018big data\u2019 to make predictions about people.[9]\n\_\nDiscrimination\
    \ \u2013 skin colour, ethnicity, class or religion\nCredit Scoring\nAs companies\
    \ responsible for credit scoring, background checks, and hiring make more use\
    \ of automated data systems, an individual\u2019s appearance, background, personal\
    \ details, social network, or socio-economic status may influence their ability\
    \ to get housing, insurance, access education, or a job.\nThere are new start-up\
    \ companies that make use of a range of \u2018alternative\u2019 data points to\
    \ make predictions about consumers and provide people with credit scores. In addition,\
    \ traditional credit scoring agencies are making use of data and machine learning\
    \ to develop profiles. While the argument is that these tools could open up the\
    \ potential for some not served by traditional credit scoring systems to receive\
    \ credit, there are a range of concerns about how algorithmic scoring may discriminate.\
    \ For example, a consumer\u2019s purchase history could be used, intentionally\
    \ or unintentionally, as a proxy for ethnicity or religion. If an algorithmic\
    \ system ends up penalizing one group more than others it may be hard to figure\
    \ this out given the access issues, opacity and complexity of algorithmic processes.\
    \ While there are laws in place for people to review conventional credit scores,\
    \ there are not yet measures in place for people to interrogate new generated\
    \ scores.\nIn relation to all of these examples, researchers have raised concerns\
    \ about how new data driven processes reproduce illegal redlining practices. Historically,\
    \ redlining was used to discriminate against certain groups of people by denying\
    \ some groups access, or more expensive access, to housing or insurance. This\
    \ was often done by \u2018redlining\u2019 certain communities. The issue is that\
    \ where someone lives is often associated with ethnicity and class. In this way\
    \ location facilitates racism and inequality. Critics are concerned about how\
    \ new automated and predictive data tools can be used to \u2018redline\u2019 given\
    \ the amount of detail that can be determined about us through our data. Previous\
    \ research has demonstrated the potential to accurately determine our age, gender,\
    \ sexuality, ethnicity, religion and political views through the data that can\
    \ be collected and combined about us.\nRelatedly, groups are raising concerns\
    \ about how new data driven processes may facilitate \u2018reverse redlining\u2019\
    . This is when a particular group of people is targeted, as was done with sub-prime\
    \ mortgages. Newman argues that big data was central to the subprime financial\
    \ crash in 2007 as it played a key role in the manipulation of markets but also\
    \ in the subprime mortgage industry. Online advertising and data collected about\
    \ people online was used to direct and target them for sub-prime loans. In 2012\
    \ the American Department of Justice reached a settlement with the Wells Fargo\
    \ Bank concerning allegations that it had \u2018engaged in a pattern or practice\
    \ of discrimination against qualified African-American and Hispanic borrowers\
    \ in its mortgage lending from 2004 through 2009\u2019 by pushing these borrowers\
    \ into more costly sub-prime loans. In the settlement they agreed to provide $184\
    \ million in compensation.\nThe practice of targeting low-income groups continues\
    \ in the payday loan industry. A U.S. Senate Investigation reports that data brokers\
    \ have been found selling lists that focus on citizen financial vulnerability.\
    \ For example, data brokers have compiled the following lists to sell to those\
    \ interested in targeting such groups: \u2018Rural and Barely Making It\u2019\
    , \u2018Ethnic Second-City Strugglers\u2019, \u2018Retiring on Empty: Singles\u2019\
    , \u2018Tough Start: Young Single Parents\u2019. One company was found selling\
    \ a marketing tool to \u2018identify and more effectively market to under-banked\
    \ consumers\u2019.[10]\nAs argued by Madden et al., the fact that those with low-incomes\
    \ are less likely to take privacy protection measures when online and to also\
    \ rely more on their mobile phone for online access places them at greater risk\
    \ than others for online targeting and exploitation.[11] In fact, \u2018opting\
    \ out\u2019 of being tracked becomes increasingly difficult as technologies become\
    \ more sophisticated. New tools that make cross-device tracking possible or that\
    \ are embedded the Internet of Things, mean that the objects we use everyday make\
    \ more of our lives \u2018knowable\u2019 and trackable and make \u2018opting out\u2019\
    \ even harder.[12] Newman raises concerns about how in this age of datafication,\
    \ information inequality is transferred into economic inequality, as companies\
    \ have more information about citizens that can be used to target and exploit\
    \ them to their disadvantage.[13]\nCitron and Pasquale note that \u2018evidence\
    \ suggests that credit scoring does indeed have a negative disparate impact on\
    \ traditionally disadvantaged groups\u2019. They provide a number of examples\
    \ in their article, just one is the case of All-State which was challenged in\
    \ court and agreed to a multi-million dollar settlement over their scoring procedure\
    \ which plaintiffs argued \u2018resulted in discriminatory action against approximately\
    \ five million African-American and Hispanic customers\u2019.[14] They also raise\
    \ concerns about how scoring systems and predictive tools may actually create\
    \ the situations they claim to indicate and \u201Ctake a life\u201D of their own,\
    \ for example by labelling someone a poor candidate or unemployable.[15]\nIn 2015,\
    \ Christian Haigh, a Harvard undergraduate, discovered that the prices for The\
    \ Princeton Review\u2019s online SAT tutoring packages offered to high school\
    \ students varied depending on where customers live. Julia Angwin and Jeff Larson\
    \ of ProPublica investigated Haigh\u2019s findings and found that the highest\
    \ prices were being offered to ZIP codes with a large Asian population and high\
    \ median income. The Princeton Review said that the price difference was not intentional,\
    \ but as noted by ProPublica, the pricing algorithm clearly did discriminate.\
    \ Angwin and Larson note that it is significant that in the United States \u2018\
    unintentional racial discrimination is illegal in housing and employment under\
    \ the legal doctrine known as \u2018disparate impact\u2019 which prohibits inadvertent\
    \ actions that hurt people in a protected class\u2019. However this doctrine does\
    \ not extend to the online world, making it difficult in that country (and others)\
    \ to take legal action against \u2018adverse impact\u2019 caused by unintentional\
    \ algorithmic bias.\nIn 2012, a Wall Street Journal investigation found that Staples\
    \ Inc. website displayed \u2018different prices to people after estimating their\
    \ locations\u2019 and that in what appeared to be an \u2018unintended side effect\u2019\
    \ Staples tended to show discounted prices to areas with a higher average income\
    \ and higher prices to areas with lower average incomes.[16]\nA 2017 investigation\
    \ by ProPublica and Consumer Reports showed that minority neighborhoods pay more\
    \ for car insurance than white neighborhoods with the same risk levels. The study,\
    \ which compared premiums and payouts in California, Illinois, Texas and Missouri,\
    \ showed that minority neighborhoods paid \u2018as much as 30 percent more than\
    \ other areas with similar accident costs\u2019.\n\_\n\nhttp://a.msn.com/00/en-us/BBzv1Xg?ocid=scu2\n\
    \_\nIn 2015 Facebook suspended the accounts of Native Americans because its algorithm\
    \ did not recognize their names as real. [17] The \u201Creal name\u201D policy\
    \ left hundreds of native Americans with suspended accounts and they had to prove\
    \ their identity in order to use their accounts again. Dana Lone Hill was one\
    \ of the Native Americans who had to produce multiple ID documents to prove her\
    \ identity and have her profile reinstated. Her case generated a lot of media\
    \ attention and Facebook had to review its algorithm and eliminate the possibilities\
    \ for discrimination.\n\_\nRecognition technologies\nThere are numerous reports\
    \ of the biases embedded in facial recognition systems: they have problems identifying\
    \ people with darker skin and also with gender. Algorithms that are used to focus\
    \ smartphone cameras, for border security and advertisements sometimes cannot\
    \ identify, or misidentify, people of colour. It has been reported that the problem\
    \ is that the facial recognition algorithms used across various systems have been\
    \ trained using datasets that have mostly male white faces, that these algorithms\
    \ have not been exposed to enough diversity and that this problem is also connected\
    \ to the fact that many of these systems are being developed and tested largely\
    \ by white men. As argued by Joy Buolamwini, the issue of bias and inaccuracy\
    \ becomes increasingly important as facial recognition tools are adopted by police\
    \ and security systems.\n\_\n\n\n\n\_\nExamples of problems include the New Zealand\
    \ case where one man\u2019s passport photograph was rejected when a facial recognition\
    \ program mistakenly identified him as having closed eyes. People have posted\
    \ reviews online raising questions about the ability of Microsoft\u2019s Kinect\
    \ facial recognition feature to recognize people with darker skin and of HP\u2019\
    s tracking webcams \u2018to see Black people\u2019. Recent work by Buolamwini\
    \ and Gebru involved testing tools and found \u201Cdarker skinned females to be\
    \ the most misclassified group.\u201D\nOne of the biggest developers of facial\
    \ recognition software is Amazon and their tool \u2018Rekognition\u2019 has been\
    \ in the centre of debates about machine bias and racial discrimination in AI\
    \ technologies. Amazon\u2019s Rekognition has been designed to cross-reference\
    \ photos of unknown suspects and criminals against a database of mugshots from\
    \ jails in the country .[18] When the software was tested by the American Civil\
    \ Liberties Union of Northern California it was found that people of colour are\
    \ predominantly being misidentified from the mugshot database. The ACLU used photos\
    \ of Members of Congress and cross-referenced them with the mugshot database using\
    \ Amazon\u2019s facial recognition system: 28 members of Congress were misidentified\
    \ as people from the mugshot database and over 40% of them were from ethnic minorities.\
    \ People of colour make up only around 20% of Congress, but the misidentification\
    \ rate with them is two times higher than people of Caucasian origin.[19]\nThe\
    \ ACLU argues that due to concerns about the high risk of misidentification and\
    \ discrimination Amazon\u2019s software unfit and dangerous for use. The technology\
    \ has been sold to the American government and police forces.[20]\n\_\n\n\_\n\
    In August 2019 an American Federal court agreed with a group of Illinois Facebook\
    \ users that Facebook\u2019s use of face recognition technology on their photographs\
    \ without their knowledge or consent violated their privacy rights. The ACLU had\
    \ supported their case, arguing that \u2018performing a scan of an individual\u2019\
    s face without disclosing how that information will be stored, used, or destroyed,\
    \ and without properly obtaining written consent, creates an actionable privacy\
    \ harm. Notice and informed consent empower individuals to protect their privacy\
    \ and are central to privacy laws in the United States, generally, and to BIPA,\
    \ specifically.\u2019\nSasha Costanza-Chock\u2019s work details how \u201Cnorms,\
    \ values, assumptions \u2013 are encoded in and reproduced through the design\
    \ of sociotechnical data-driven systems.\u201D Their essay on the politics of\
    \ border security systems illustrates this as well as the harmful experience of\
    \ confronting the normative politics of these systems. The injustice and harm\
    \ caused by normative security systems has also been stressed by Shadi Petosky\
    \ as has the fact that there are alternatives that are being ignored.\n\_\n Eyeo\
    \ 2019 \u2013 Sasha Costanza-Chock from Eyeo Festival on Vimeo.\n\_\nDiscrimination\
    \ \u2013 gender and ethnicity\nA study of Google ads found that men and women\
    \ are being shown different job adverts, with men receiving ads for higher paying\
    \ jobs more often.[21] The study, which used a tool called AdFisher to set up\
    \ hundreds of simulated user profiles, was designed to investigate the operation\
    \ of Google\u2019s ad settings. Although researchers could determine that men\
    \ and women are being shown different ads, they could not determine why this is\
    \ happening. Doing so would require access to more information that would need\
    \ to be provided by advertisers about who they were targeting and by Google about\
    \ how their system works.\nFacebook allows advertisers to target people based\
    \ on race, ethnicity and gender. Another ProPublica investigation revealed that\
    \ third party companies can target ads to reach people by gender, ethnicity and\
    \ race and also to be hidden from people based on these kinds of classifications.\
    \ [22] A ProPublica investigation identified that Facebook\u2019s ads software\
    \ gives advertising companies the option to exclude men or women from their ad\
    \ demographic. Job positions for Uber and truck drivers, police officers and military\
    \ were all shown to predominantly male audiences, while job vacancies for nurses,\
    \ medical assistants and care-takers targeted women almost exclusively.\n\_\n\n\
    \_\nA complaint by the American Civil Liberties Union (ACLU) submitted to the\
    \ US Equal Opportunity Commission (EEOC) mentions three women from the states\
    \ of Ohio and Illinois who were not shown job advertisements for positions in\
    \ traditionally male-dominated fields which violates federal law of gender. [23]\
    \ The ACLU argued that by targeting only men in already predominantly male fields,\
    \ women are denied the opportunity to break into particular industries.\nSome\
    \ landlords have been found to selectively target different groups of users with\
    \ housing ads, excluding people from \u201Credlined\u201D neighbourhoods, and\
    \ inner-city areas with high rates of Black and Latino residents.[24] The housing\
    \ adverts run by some American landlords were devised to selectively target certain\
    \ communities and exclude people from Hispanic or African-American origin, or\
    \ citizens with bad credit scores . The ads that they publish are invisible for\
    \ the \u201Cexcluded\u201D people, as their ethnicity deems them an \u201Cundesirable\u201D\
    \ demographic. Discriminating based on sensitive factors such as ethnicity and\
    \ nationality in the United States is prohibited by The Fair Housing Act of 1968\
    \ and ads for housing that target based on those characteristics are in fact illegal,\
    \ but nonetheless existing .[25]\n\nDiscrimination \u2013 health\nCathy O\u2019\
    Neil has produced a great deal of work demonstrating how unfair and biased algorithmic\
    \ processes can be. In one example, she tells the story of Kyle Behm, a high achieving\
    \ university student who noticed that he was repeatedly not getting the minimum\
    \ wage jobs he was applying for. All of these job applications required him to\
    \ take personality tests which included questions about mental health. Although\
    \ healthy when looking for work, Behm did suffer from bipolar disorder and had\
    \ taken time out previously to get treatment. Behm\u2019s father is a lawyer and\
    \ he became suspicious of the fairness of these tests for hiring. He decided to\
    \ investigate and found that a lot of companies were using personality tests,\
    \ like the Kronos test. These tests are used as part of automated systems to sort\
    \ through applications and in this process decide which applicants proceed and\
    \ which are \u2018red-lighted\u2019 or discarded. As O\u2019Neil details, these\
    \ tests are often highly complex, with \u2018certain patterns of responses\u2019\
    \ disqualifying people. This example raises a number of ethical questions about\
    \ the use of health information in automated systems but also about the uses of\
    \ automated systems in hiring more generally, particularly as it is unlikely that\
    \ those who have been \u2018red-lighted\u2019 will ever know they were subject\
    \ to an automated system. O\u2019Neil argues that the increasing use of automated\
    \ systems to sort and whittle down job applications creates more unfairness as\
    \ those who know or can pay for help to ensure their applications get to the top\
    \ of the pile have an advantage.\n\_\nLoss of privacy\nThis can happen unintentionally\
    \ when attempts to release data anonymously do not work. Big data makes anonymity\
    \ difficult because it is possible to re-identify data that has been anonymized\
    \ by combining multiple data points.\n\_\nPlatforms\nAs detailed by Paul Ohm,\
    \ in 2006 America Online (AOL) launched \u2018AOL Research\u2019 to \u2018embrace\
    \ the vision of an open research community\u2019. The initiative involved publicly\
    \ releasing twenty million search queries from 650,000 users of AOL\u2019s search\
    \ engine. The data, which represented three months of activity, was posted to\
    \ a public website. Although the data was anonymized, once the data was posted\
    \ some users demonstrated that it was possible to identify people\u2019s identities\
    \ using the data which included name, age and address.\nTwo New York Times reporters\
    \ Michael Barbaro and Tom Zeller Jr. cross-linked data to identify Thelma Arnold,\
    \ a sixty-two year old widow from Lilburn Georgia. Her case demonstrates the problems\
    \ with \u2018anonymisation\u2019 in an age of big data, but also the danger in\
    \ reading too much into search queries. As Barbaro and Zeller note, Ms Arnold\u2019\
    s search queries \u2018hand tremors\u2019, \u2018nicotine effects on the body\u2019\
    , \u2018dry mouth\u2019 and \u2018bipolar\u2019, could lead someone to think she\
    \ suffered from a range of health issues. Such a conclusion could have negative\
    \ effects if the organization making that conclusion was her insurance provider.\
    \ In fact, when they interviewed Arnold, Barbaro and Zeller found that Arnold\
    \ often does searches for her friends because she wants to help them.\nIn 2006\
    \ Netflix publicly released one hundred million records detailing the film ratings\
    \ of 500,000 of its users between Dec. 1999 and Dec. 2005. As Ohm reports, the\
    \ objective was to launch a competition and for those competing to use this data\
    \ to improve Netflix\u2019s recommendation algorithm.[26] Netflix anonymized the\
    \ data by assigning users a unique identifier. Researchers from the University\
    \ of Texas demonstrated not long after this release how relatively easy it was\
    \ for people to be re-identified with the data.[27] This led to a court case in\
    \ which Jane Doe argued that the data could be used to out her sexuality.[28]\
    \ Jane Doe argued that her homosexuality was being revealed by the data as it\
    \ revealed her interest in gay and lesbian themed films. She argued the data outed\
    \ her, a lesbian mother, against her wishes and could damage herself and her family.\
    \ The court case was covered by Wired in 2009.\n\_\nFitness Trackers\nSome employers\
    \ are gaining highly personal information about their employees through their\
    \ use of fitness trackers. Employers now collect health and biometric data about\
    \ their employees through the use of wearable tech and health tracking apps.[29]\
    \ Through performance monitoring employers can now collect regular reports about\
    \ staff activity. Concerns are being raised about companies that are encouraging\
    \ their female employees to use family planning apps that give employers and other\
    \ corporate entities access to details about their employee\u2019s private lives,\
    \ health, hopes and fears.[30]\nFitness apps and trackers allow its users to monitor\
    \ their calorie intake, physical activity and vital signals such as heart rate\
    \ and blood pressure. In 2018 a data leak involving MyFitnessPal exposed the accounts\
    \ of 115 million users after a security breach in their systems.[31] The user\
    \ names, email addresses and scrambled passwords to user accounts were stolen\
    \ from the parent sportswear company Under Armour.[32] Access to the fitness accounts\
    \ means access to vital information that can be used to track individuals, view\
    \ their location in live time, predict behaviour and activities or share sensitive\
    \ health information with third party organisations such as private health clinics,\
    \ insurance companies and even employers [33]\nIn 2018, Strava revealed that fitness\
    \ app data can reveal highly sensitive location information. Heat map visualisations\
    \ released by Strava showed activity captured by the app, lighting up different\
    \ user routes. This mapping involved more than 3 trillion GPS data points.[34]\
    \ A problem is that this app is used by military personnel and by releasing these\
    \ \u2018anonymous\u2019 heat maps Strava was revealing the location of military\
    \ activity. While the information on the heatmaps is an aggregate of all the user\
    \ activities, the Strava website allows the user to track running routes in detail\
    \ and eventually connect them to usernames and the individuals behind them, which\
    \ could endanger military personnel on missions overseas.[35]\nAnother fitness\
    \ tracking app \u2013 Polar Flow, also exposed the geolocation of its users through\
    \ a tool called \u201CExplore map\u201D. An investigation by De Correspondent\
    \ and Bellingcat revealed that the app makes it possible to explore sensitive\
    \ locations and locate individual users and their exercise routines. It turned\
    \ out that tracking an individual behind a username has been made readily available\
    \ and fairly easy and the names and addresses of personnel from intelligence agencies\
    \ such as the NSA, the US Secret Services and the MI6 could be uncovered.\n\_\n\
    \n\_\nSmart microchips\nSome tech companies have found even more intrusive ways\
    \ to monitor their employees. They have started using smart microchips that can\
    \ be implanted under the skin- the same technology that the US justice department\
    \ uses on prisoners on probation instead of ankle braces. The chips work similar\
    \ to the access key cards a lot of firms have, except these key cards do not track\
    \ the workers\u2019 physical and physiological condition. A Chinese mining company\
    \ has even introduced helmets that could read the brainwaves of workers and distinguish\
    \ feelings such as fatigue, distraction and even anger [36]. Earlier last year,\
    \ the American company Three Square Market began putting microchips the size of\
    \ a rice grain in their workers\u2019 hands- from 6-7 chipped employees initially,\
    \ the process expanded to include more the 100 with plans to implant chips on\
    \ all its 10 000 employees.\nThe chips can be used instead of physical IDs to\
    \ open doors, log into computers, pay at the vending machines on site, and can\
    \ be paired with the GPS on employers\u2019 smartphones to show the exact location\
    \ of every employee at any point throughout the day.[37]\nBut unlike fitness tracking\
    \ devices and key cards that could be taken off at the end of the work day, chips\
    \ are worn constantly- giving the employer infinite possibilities to track its\
    \ employees. This invasion of personal space has expanded substantially. BioTeq,\
    \ a UK- based firm is one of the new businesses that offer implants to companies\
    \ and individuals and has implanted more than 150 chips in various firms across\
    \ the UK.[38] Experts and researchers warn that microchip implanting can hide\
    \ a lot of dangers, especially for employees, as it can completely erode their\
    \ right to privacy.\n\_\nSmart devices- smart spies \nAs of 2019, over 100 million\
    \ Amazon Echos have been sold. Amazon employs thousands of people in its headquarters\
    \ to listen to what Alexa has recorded about users.[39] The recordings are transcribed\
    \ and it is argued used to eliminate gaps in Alexa\u2019s communication and introduce\
    \ new accents and words. Together with the conversations that people have had\
    \ with their Alexa devices, the devices pick up other audio in the home. Access\
    \ to the device has been requested by judges in court proceedings. There is increasing\
    \ concern about how the use of such devices can compromise privacy.\n\nConcerns\
    \ are also being raised about Smart TVs.[40] In 2017 Wikileaks published documents\
    \ suggesting a CIA operation: Weeping Angel, which allegedly involved the use\
    \ of smart TV microphones for mass surveillance.[41] Consumer Reports also compiled\
    \ information and advice for the owners of smart TVs about to avoid tracking.\n\
    \nChildren\nCompanies have been facing increasing complaints for collecting data\
    \ about children and sharing or using this to target them with advertisements\
    \ or more content designed to keep them online for longer. In 2019, the American\
    \ Federal Trade Commission ordered Google, including its subsidiary YouTube, to\
    \ pay a record $170 million to settle allegations that YouTube had illegally collected\
    \ data about children without their parent\u2019s consent.\nConcerns are also\
    \ being raised about YouTube\u2019s site for children and the YouTube algorithm,\
    \ which privileges the kind of sensational content that keeps children online\
    \ for longer. Concerns have been raised about the algorthm through privileging\
    \ content like this leading to the promotion of violent and suggestive content.\n\
    The Children\u2019s Commissioner for England published a report highlighting the\
    \ range of data that is being collected about children. The report draws attention\
    \ to the fact that while there is increasing attention to the need to be alert\
    \ to privacy infringements related to online platforms, there is also a need to\
    \ consider the privacy issues and potentials for harms introduced by smart internet\
    \ connected devices, like monitors and toys. Consumer groups have called for some\
    \ smart toys to be re-called after learning that these toys could be hacked to\
    \ enable strangers to talk to children and parents reported that their baby monitors\
    \ were hacked.\n\_\nIdentity theft, blackmail, reputation damage, distress\n\_\
    \nData breaches\nAlthough data breaches are listed under corporate uses of data,\
    \ they could also be listed here under government uses of data as breaches have\
    \ happened in both sectors. Solove and Citron argue that \u2018harm\u2019 in relation\
    \ to data breaches relates to \u2018a risk of future injury, such as identity\
    \ theft, fraud, or damaged reputations\u2019 and also to a current injury as people\
    \ experience anxiety about this future risk. They note that the anxiety and emotional\
    \ distress created about future risk is a harm that people experience \u2018in\
    \ the here and now\u2019. Identity theft is a major problem, particularly for\
    \ those of low-income who lack the resources to pay for legal representation and\
    \ challenge mistakes due to identity fraud. Further, the sudden loss of income\
    \ or errors that result from identity fraud can be disastrous for those living\
    \ from pay cheque to pay cheque. Sarah Dranoff notes that in addition to financial\
    \ loss, identity theft can lead to \u2018wrongful arrests, loss of utility service,\
    \ erroneous information on health records, improper child support garnishments,\
    \ and harassment by collection agencies\u2019.[42] A number of data breach examples\
    \ are detailed by Solove and Citron: 1) The Office of Policy Management breach\
    \ leaked people\u2019s fingerprints, background check information, and analysis\
    \ of security risks, 2) The Ashley Madison breach released information about people\u2019\
    s extramarital affairs, 3) The Target breach resulted in leaking credit card information,\
    \ bank account numbers and other financial data and 4) the Sony breach involved\
    \ employee email.\nThis regularly updated visualization by the Information is\
    \ Beautiful team demonstrates how common major data breaches are:\n\_\nhttps://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/\n\
    \_\n\nCommercial Data Breaches\nDating sites\nThe Ashley Madison data hack was\
    \ one of the biggest data in online dating site history. In 2015 the personal\
    \ data of more than 37 million users of the site was stolen.[43] Personal details\
    \ of site users was posted by an online hacking group called the Impact Team.[44]\
    \ The sites parent company Avid Live Media (ALM) faced a class action in US court.\
    \ As a result the corporation had to pay nearly quarter of its revenue \u2013\
    \ $11.2 million in settlement.[45] The hack also led to reputational damage, high-profile\
    \ resignations from site users whose names were exposed, divorce filings and two\
    \ suicides of former employees of the company.[46]\nAnother online dating site,\
    \ Adult FriendFinder, was hacked in 2015 and the highly personal data of almost\
    \ 4 million users was leaked online.[47] Just hours after the data was posted\
    \ on a dark web forum, the victims of the hack received spam and threatening emails\
    \ to expose their private information. A year later, the site suffered a second\
    \ hack, this time exposing the information from 412 million accounts.[48]\n\_\n\
    Security companies \nIn August 2019 the Guardian reported that researchers had\
    \ discovered that Suprema\u2019s Biostar 2 database was \u2018unprotected and\
    \ mostly unencrypted\u2019. The researchers said they had access to millions of\
    \ personal records which included fingerprint and facial recognition data and\
    \ usernames and passwords. The system is used by government agencies, defence\
    \ contractors and banks.\n\_\nGovernment Database Breaches \nSwedish Government\
    \ Database \nIn 2016 the Swedish government suffered a massive data breach which\
    \ endangered the identities of undercover operatives. The data breach originated\
    \ from the Swedish transport Agency which exposed the personal information of\
    \ millions of Swedish citizens and the identities of some military personnel.\
    \ The agency previously had contracted a deal with an outsourcing company \u2013\
    \ IBM, and the mishandling of data between the government agency and the private\
    \ company led to a massive leak of sensitive information.[49] The information\
    \ also exposed sensitive information about bridges, roads, ports, subway systems\
    \ in the capital and other key infrastructures. The exposure of sensitive information\
    \ in this case was the result of an absence of proper safeguards and protective\
    \ measures between the government agency and IBM.[50]\nBritish Government Database\n\
    In 2019 the Guardian reported that the fingerprints of \u201Cover 1 million people,\
    \ as well as facial recognition information, unencrypted usernames and passwords,\
    \ and personal information of employees, was discovered on a publicly accessible\
    \ database for a company used by the likes of the UK Metropolitan police, defence\
    \ contractors and banks.\u201D The company involved was called Suprema and the\
    \ breach involved their web-based Biostar 2 biometrics lock system.\nIndia\u2019\
    s Aadhaar Data Breach \nIndia\u2019s ID System Aadhaar has also suffered a data\
    \ breach that exposed the identities of more than a billion people online. In\
    \ 2018 Excel files and documents containing the names, addresses and phone numbers\
    \ of Aadhaar holders were erroneously leaked by various government websites, compromising\
    \ data and giving unauthorised access to personal information of Aadhaar ID\u2019\
    s.[51] A Tribune investigation revealed that the personal and biometric information\
    \ of more than a billion Indian citizens was being sold online for as little as\
    \ 500 rupees or \xA36. Authorities denied the allegations and said that the leaked\
    \ demographic data cannot be misused without biometric information, which was\
    \ kept safe and protected .[52]\n\_\nPhysical injury\nEsther Kaplan\u2019s investigation\
    \ into the effects of workplace data monitoring revealed how the monitoring of\
    \ employees in order to increase their productivity is leading to physical injury\
    \ in some cases. She interviewed a UPS worker who noted that the physical demands\
    \ of his job have increased since the company introduced a telematics system.\
    \ The system monitors employees in real time through tracking devices that include\
    \ \u2018delivery information acquisition devices\u2019 and sensors on delivery\
    \ trucks. The pressure to do more work in less time is leading to injury as drivers\
    \ do not have the time to lift and carry packages properly.[53]\n\_\nPolitical\
    \ uses of Data\nPolitical Manipulation and social harm\nThe damage that can be\
    \ done by fake news, bots and filter bubbles have generated much discussion recently.\
    \ Uses of automated and algorithmic processes in these cases can lead to social\
    \ and political harm as the information that informs citizens is manipulated,\
    \ potentially leading to misinformation and undermining democratic and political\
    \ processes as well as social well-being. A recent study by researchers at the\
    \ Oxford Internet Institute details the diverse ways that people are trying to\
    \ use social media to manipulate public opinion across nine countries. They note\
    \ that this is a concern given the increasing role that social media plays as\
    \ a key information source for citizens, particularly young people. Further, that\
    \ social media are fundamental in many countries to the sharing of political information.\
    \ Civil society groups are \u2018trying, but struggling, to protect themselves\
    \ and respond to active misinformation campaigns\u2019.\nWoolley and Howard define\
    \ computational propaganda as involving \u2018learning from and mimicking real\
    \ people so as to manipulate public opinion across a diverse range of platforms\
    \ and device networks\u2019. Bots, automated programs, are used to spread computational\
    \ propaganda. While bots can be used for legitimate functions, the Internet Institute\
    \ study details how bots can be used to spam, harass, silence opponents, \u2018\
    give the illusion of large-scale consensus\u2019, sway votes, defame critics,\
    \ and spread disinformation campaigns. The authors argue that \u2018computational\
    \ propaganda is one of the most powerful new tools against democracy\u2019.\n\
    Facebook- Cambridge- Analytica Scandal \nIn 2018, through the reporting of Carole\
    \ Cadwalladr, we learned about how Facebook was implicated in political manipulation\
    \ on a grand scale through its involvement with Cambridge Analytica and others.\
    \ Whistleblower Christopher Wylie revealed how the company used the data of more\
    \ than 80 million people to build a profiling system used for political advertising.[54]\
    \ The company allegedly used the psychological profiles for what a CA intern has\
    \ called \u201CPsyops\u201D- psychological operations that, much like in the military,\
    \ are used to affect and change opinion. The use of \u2018dark ads\u2019 on Facebook\
    \ have been linked to Brexit and Trump\u2019s election campaign in the United\
    \ States.\n\_\n\n\n\n\_\nBut the Cambridge Analytica scandal was not only a data\
    \ leak crisis; in fact, it can be argued that it was not a data breach at all,\
    \ as Facebook is designed for this- to collect data, analyze and exploit it.\n\
    \_\nGovernment uses of Data\nExclusion and Error\nBig data blacklisting and watch-lists\
    \ in the U.S. have wrongfully identified individuals. As detailed by Margaret\
    \ Hu, being wrongfully identified in this case can negatively affect employment,\
    \ ability to travel, and in some cases lead to wrongful detention and deportation.[55]\n\
    Hu details the problems with the American E-Verify programme, which \u2018attempts\
    \ to \u201Cverify\u201D the identity or citizenship of a worker based upon complex\
    \ statistical algorithms and multiple databases\u2019. Employers across states\
    \ use the programme to determine if a person is legally able to work in the U.S.\
    \ Hu writes that it appears that employers have wrongfully denied employment for\
    \ thousands. Hu argues that e-verify is problematic due to the unreliability of\
    \ the data that informs the database screening protocol. The problems with the\
    \ e-verify programme have also been detailed by Upturn. A study by the American\
    \ Civil Liberties Union demonstrates that errors are far more likely to affect\
    \ foreign-born employees and citizens with foreign names. People with multiple\
    \ surnames and women who change their names after marriage are also more likely\
    \ to face errors. Harm is further exacerbated by the difficulty in challenging\
    \ or correcting e-verify errors. As discussed by Alex Rosenblat and others: \u2018\
    [L]ow-wage, hourly workers, whether they are flagged for a spelling error or for\
    \ other reasons, often lack the time, resources, or legal literacy required to\
    \ navigate complex bureaucracies to correct misinformation about them in a national\
    \ database\u2019.\nHu also raises concerns about The Prioritised Enforcement Programme\
    \ (PEP), formerly the Secure Communities Programme (S-COMM). This is a data-sharing\
    \ programme between the Federal Bureau of Investigation (FBI), DHS and local law\
    \ enforcement agencies that requires local agencies to run fingerprints taken\
    \ from suspects against federal fingerprint databases (ibid: 1770). The programme\
    \ has made errors. For example, inaccurate database screening results wrongfully\
    \ targeted 5,880 US citizens for potential detention and deportation, leading\
    \ critics to question the reliability of PEP/S-COMM\u2019s algorithms and data.\
    \ Furthermore, by using the biometric data of arrestees contained in the S-COMM\
    \ databases the Immigration and Customs Enforcement (ICE) reportedly may have\
    \ wrongly apprehended approximately 3,600 US citizens, due to faulty information\
    \ feeding database screening protocols. As Hu points out, \u2018error-prone\u2019\
    \ databases and screening protocols \u2018appear to facilitate the unlawful detention\
    \ and deportation of US citizens\u2019.\nHu argues that the big data systems underlying\
    \ both E-Verify and S-COMM/PEP are causing harm by mistakenly targeting and assigning\
    \ inferential guilt to individuals. Legally speaking, this kind of digitally generated\
    \ suspicion is at odds with constitutional rights and there is a growing consensus,\
    \ at least in the U.S, on the need for substantive and binding due process when\
    \ it comes to big data governance.\nIn Arkansas, U.S., the government introduced\
    \ an algorithm to determine how many hours of home care people were entitled to.\
    \ This was something that was previously done by home care nurses. The change\
    \ meant that home care nurses were now required to help people fill in a questionnaire\
    \ with 260 questions. The responses to the questionnaire were then processed by\
    \ an algorithmic system which then determined how many home care hours people\
    \ were entitled to. The result for many was a major reduction in home care hours,\
    \ which drastically limited people\u2019s quality of life and in some cases their\
    \ ability to stay in their own homes. As with other examples listed in this record,\
    \ finding out information about how the algorithm worked proved very difficult.\n\
    Seven of those affected took the government to court with the help of Legal Aid\
    \ of Arkansas. Six of those involved in this case had their home care hours reduced\
    \ by more than 30 percent. There have been ongoing challenges to the use of this\
    \ algorithm and its effects.\n\_\n\n\_\nA similar situation has occurred in Idaho\
    \ where the government started using a data system to determine home care costs\
    \ which led to beneficiaries seeing their funds drastically reduced. Only after\
    \ an ACLU lawsuit did it become clear how limited the data being used was and\
    \ the need for system change.\nA study published in Science magazine in 2019 has\
    \ found that an algorithmic system used to identify follow up health care needs\
    \ of patients across the United States is biased against Black patients \u2013\
    \ the system dramatically underestimates the amount of care Black patients need\
    \ as compared to white patients.\n\_\n\n\_\nConcerns are being raised in the United\
    \ States about how data matching systems are being used as part of a wider strategy\
    \ to disenfranchise African American and Latino voters. In one highly publicized\
    \ example, data matching requirements in Georgia, have been linked to voter suppression\
    \ by civil rights activists and the democratic nominee. According to the \u201C\
    exact match\u201D legislation, the system that processes the voter registration\
    \ applications would only count the votes of the people with the same name or\
    \ address spelling on all documents as legitimate. The changes were introduced\
    \ by Brian Kemp\u2019s office, who at that time was Georgia\u2019s Secretary of\
    \ State and the Republican candidate in the governor\u2019s race. The new regulations\
    \ resulted in 53 000 voter applications being put on hold, as a result of \u201C\
    misspelling of names.\u201D The data inconsistencies and application suspensions\
    \ mostly affected people with foreign names, people with more than one surname,\
    \ those from minority groups and people who have recently changed their surnames\
    \ (newly married women) or have a new address.\n\_\n\n\_\nIn Australia in 2019,\
    \ after years of activism and advocacy, the federal government conceded that the\
    \ automated debt recovery system it had introduced was flawed. Government communications\
    \ suggested that anywhere from 600,000 to 900,000 \u201Crobo-debts\u201D that\
    \ had been issued to people to repay would need to be reassessed. The program\
    \ had at this point already been investigated by the Ombudsman and Senate after\
    \ numerous complaints of errors and unfair targeting of vulnerable people. The\
    \ system uses data matching and income averaging to determine if people have been\
    \ overpaid benefits. Onus was placed on those receiving letters to prove an error\
    \ had been made.\nNumerous accounts of errors were published in the press and\
    \ calls for investigation were taken up by opposition politicians. One case involved\
    \ a man who was repeatedly sent letters saying he owed the government repayment\
    \ of $4,000. This turned out to be an error. The man, who suffers from depression\
    \ and became suicidal, said he successfully convinced the government this was\
    \ an error only to receive a similar letter a few months later. He again successfully\
    \ proved this was an error. One of the ombudsman\u2019s conclusions was that better\
    \ project planning and risk management should have been done from the outset.\n\
    Cassandra Goldie, Chief Executive of the Australian Council of Social Service,\
    \ was quoted in the Guardian as saying:\n[R]obo-debt has issued thousands of debt\
    \ notices in error to parents, people with disabilities, carers and those seeking\
    \ paid work, resulting in people slapped with Centrelink debts they do not owe\
    \ or debts higher than what they owe \u2026 It has been a devastating abuse of\
    \ government power that has caused extensive harm, particularly among people who\
    \ are the most vulnerable in our community.\n\_\n\n\_\nIn October 2019, Virginia\
    \ Eubanks reported a similar practice happening in the United States. In this\
    \ case it is being reported that Government working with tech companies are\_\
    sending out debt notices to thousands of vulnerable people across the country\
    \ that allege\_people have been overpaid benefits. When people have received these\
    \ letters they have few\_options, particularly as challenging the details in the\
    \ letter may require finding pay stubs or\_other documents that are decades old.\
    \ These debts are being called zombie debts because\_of the devastating impact\
    \ they are having on the families forced to repay them, who have\_little ability\
    \ to challenge them. This is despite the fact, that much like the Australia robo-debt\_\
    scandal, people are finding error and \u2018miscalculation\u2019 in these notices.\n\
    \nSocial Exclusion \nSocial exclusion can be perpetuated by many factors including\
    \ identification systems. In a number of countries ethnic groups are being routinely\
    \ excluded and labelled as different through the use of national IDs. Privacy\
    \ International research of national identification systems raises concerns about\
    \ how ID systems can be used in ways that can lead to intentional and unintentional\
    \ exclusion. Such exclusion can lead to great harm by affecting people\u2019s\
    \ survival as ID cards are linked to the ability to access food, fuel, work and\
    \ education.\nIn India, data errors linked to the world\u2019s biggest biometric\
    \ identification system-Aadhaar are being linked to deaths due to starvation as\
    \ people, through data system errors, are being left without access to food and\
    \ other life essentials. In some cases this can be because of data system errors\
    \ such as ID\u2019s not being matched to the right person or people\u2019s finger\
    \ prints not registering. Aadhaar, India\u2019s identification database, contains\
    \ the names, addresses, phone numbers and biometrical specifics (fingerprints,\
    \ palm veins and print, face and iris recognition, DNA, hand geometry, retina)\
    \ of 80% of India\u2019s population.[56] The Aadhaar ID system started as a completely\
    \ voluntary ID card system run by the government on the private servers of HCL,\
    \ but quickly became a vital aspect of identification in India and more and more\
    \ government services have made the use of Aadhaar mandatory. As of 2019 access\
    \ to fuel, food, financial subsidies, health services, job positions and school\
    \ scholarships is open almost exclusively to Aadhaar number holders.\nOther examples\
    \ of data failure include attempts to automate welfare services in the U.S. Virginia\
    \ Eubanks details the system failures that devastated the lives of many in Indiana,\
    \ Florida and Texas at great cost to taxpayers. The automated system errors led\
    \ to people losing access to their Medicaid, food stamps and benefits. The changes\
    \ made to the system led to crisis, hospitalization and as Eubanks reports, death.\
    \ These states cancelled their contracts and were then sued.\n\_\n\n\_\nBig data\
    \ applications used by governments rely on combining multiple data sets. As noted\
    \ by Logan and Ferguson, \u2018small data (i.e. individual level discrete data\
    \ points) \u2026 provides the building blocks for all data-driven systems\u2019\
    . The accuracy of big data applications will be affected by the accuracy of small\
    \ data. We already know there are issues with government data, just two examples:\
    \ 1) in the United States, in 2011 the Los Angeles Times reported that nearly\
    \ 1500 people were unlawfully arrested in the previous five years due to invalid\
    \ warrants and 2) in New York, a Legal Action Center study of rap sheet records\
    \ \u2018found that sixty-two percent contained at least one significant error\
    \ and that thirty-two percent contained multiple errors\u2019. [57]\n\_\nHarms\
    \ due to algorithm / machine bias\nResearch into predictive policing and predictive\
    \ sentencing shows the potential to over-monitor and criminalize marginalized\
    \ communities and the poor.[58]\nJournalists working with ProPublica are investigating\
    \ algorithmic injustice. Their article titled \u2018Machine Bias\u2019 in particular,\
    \ has received a great deal of attention. Julia Angwin, Jeff Larson, Surya Mattu\
    \ and Lauren Kirchner\u2019s investigation was a response to concerns being raised\
    \ by various communities about judicial processes of risk assessment. These processes\
    \ of risk assessment involved computer programs that produce scores predicting\
    \ the likelihood that people charged with crimes would commit future crimes. These\
    \ scores are being integrated throughout the US criminal justice system and influencing\
    \ decisions about bond amounts and sentencing. The ProPublica journalists looked\
    \ at the risk scores assigned to 7,000 people and checked to see how many were\
    \ charged with new crimes. They found that the scores were \u2018remarkably unreliable\
    \ in forecasting violent crime\u2019. They found that only 61%, just over half,\
    \ of those predicted to commit future crimes did. But the big issue is bias. They\
    \ found that the system was much more likely to flag Black defendants as future\
    \ criminals, wrongly labelling them as future criminals at twice the rate as white\
    \ defendants. White people were also wrongly labelled as low risk more often than\
    \ Black defendants. The challenge is that these risk scores and the algorithm\
    \ that determines them is produced by a for profit company, so researchers were\
    \ not able to interrogate the algorithm only the outcomes. ProPublica reports\
    \ that the software is one of the most widely used tools in the country.\nKristian\
    \ Lum and William Isaac, of the Human Rights Data Analysis Group, published an\
    \ article detailing bias in predictive policing. They note that because predictive\
    \ policing tools rely on historical data, predictive policing should be understood\
    \ as predicting where police are likely to make arrests and not necessarily where\
    \ crime is happening. As noted by Lum and Isaac, as well as by O\u2019Neil, if\
    \ nuisance crimes like vagrancy are added to these models this further complicates\
    \ matters and there is an over policing of poor communities, more arrests, and\
    \ you have a feedback loop of injustice. Lum and Isaac used a range of data sources\
    \ to produce an estimate of illicit drug use from non-criminal justice, population\
    \ based data sources which they then compared to police records. They found that\
    \ while drug arrests tend to happen in areas with more BIPOC and low income communities,\
    \ drug use is fairly evenly distributed across all communities. Using one of the\
    \ most popular predictive policing tools, they find that the tool targets Black\
    \ people twice as much as whites even though their data on drug use shows that\
    \ drug use is roughly equivalent across racial classifications. Similarly they\
    \ find that low income households are targeted by police at much higher rates\
    \ than higher income households.\nO\u2019Neil describes how crime prediction software,\
    \ as used by the police in Pennsylvania leads to a biased feedback loop. In this\
    \ case the police include nuisance crimes, such as vagrancy, in their prediction\
    \ model. The inclusion of nuisance crimes, or so-called antisocial behaviour,\
    \ in a model that predicts where future crimes will occur distorts the analysis\
    \ and \u2018creates a pernicious feedback loop\u2019 by drawing more police into\
    \ the areas where there is likely to be vagrancy. This leads to more punishment\
    \ and recorded crimes in these areas, poor areas where there is likely to be vagrancy.\
    \ O\u2019Neil draws attention to specific examples of problems: Pennsylvania police\
    \ use of PredPol, the NYCPD use of CompStat and the Philadelphia police use of\
    \ Hunchlab.[59]\nAmnesty International also carried out an investigation of predictive\
    \ policing algorithms. They published a detailed report about the Gang Matrix\
    \ \u2013 the London Metropolitan Police database- and the implications it has\
    \ on marginalized communities. The Gang Matrix contains information about individuals\
    \ who are suspected gang members in the city of London. Created as a risk-management\
    \ tool after the riots in London in 2011, the database has proven inefficient\
    \ and been criticized as discriminating against young Black men often based on\
    \ nothing more substantial than their cultural preferences. The database has over\
    \ 3800 suspects and gathers intelligence about them from various sources online,\
    \ using data such as the websites the individuals visit, the songs they stream,\
    \ the content they watch on YouTube and more sensitive data such as ethnicity\
    \ and nationality. In 2018 the mayor of London Sadiq Khan commissioned a Review\
    \ of the Metropolitan Police Service Gang Matrix and, according to the paper,\
    \ there is a disproportionate number of Black men included in the list .[60] 78%\
    \ of the individuals on the list are young Black men aged under 25 and altogether\
    \ 80% of all the suspects on the list are Black. In reality, however, only 27%\
    \ of the people actually responsible for gang crime are Black.\n\_\n\n\_\nThe\
    \ highly controversial database perpetuates racial profiling and unjust prosecution\
    \ of people who have not committed any serious offences and can have serious repercussions\
    \ for the individuals, who are being routinely marginalized.[61] The information\
    \ on the database is being shared with jobcentre and housing workers, head teachers\
    \ and school principals and representatives from local hospitals.\nGang labelling\
    \ can not only affect the individuals listed, but their families as well. It also\
    \ can prevent young people from moving on with their lives. In 2012, the Metropolitan\
    \ Police threatened to evict the family of a young Black man that was suspected\
    \ for gang activity. The mother of the young man pursuing education at Cambridge\
    \ University, received a threatening letter from the MPS that the family was going\
    \ to lose their home, because their son was involved in gang activities. Although\
    \ this young man was not associated with the area where he used to live, the \u201C\
    gangster\u201D label continued to follow him even after he tried to move on.[62]\
    \ In 2013 another young Black man was expelled from college, after the college\
    \ authorities found that he had been listed in the Matrix.[63] In another case,\
    \ Paul, a 21 year old graduate was denied the position because his name was still\
    \ on the matrix for an offence committed when he was 12 years old.[64]\n\_\nHow\
    \ can harms be prevented?\nUltimately the goal of this Data Harm Record is to\
    \ stimulate more debate and critical interrogation of how automated and predictive\
    \ data systems are being used across sectors and areas of life.\nThe goal is to\
    \ maintain the Data Harm Record as a running record. Please let us know of any\
    \ cases you think we should add by sending a message here.\nIt is hoped that this\
    \ work contributes to the work of others in this area, many referenced in this\
    \ page, who are trying help us gain a better appreciation of: a) how uses of automated\
    \ and predictive systems are affecting people, b) the kind of datafied world we\
    \ are creating and experiencing, c) the fact that datafication practices affect\
    \ people differently, d) how datafication is political and may lead to practices\
    \ that intentionally or unintentionally discriminate, be unfair, and increase\
    \ inequality and e) how to challenge and redress data harms.\nThere are a range\
    \ of individuals and groups coming together to develop ideas about how data harms\
    \ can be prevented.[65] Researchers, civil society organizations, government bodies\
    \ and activists have all, in different ways, identified the need for greater transparency,\
    \ accountability, systems of oversight and due process, and the means for citizens\
    \ to interrogate and intervene in the datafied processes that affect them. It\
    \ is hoped that this record demonstrates the urgent need for more public debate\
    \ and attention to developing systems of transparency, accountability, oversight\
    \ and citizen intervention.\nFor example, O\u2019Neil argues that auditing should\
    \ be done across the stages of data projects and include auditing: the integrity\
    \ of the data; the terms being used; definitions of success; the accuracy of models;\
    \ who the models fail; the long-term effects of the algorithms being used; and\
    \ the feedback loops created through new big data applications. The Our Data Bodies\
    \ team is based in marginalized communities and interrogating data practices from\
    \ a human rights perspective. We at the Data Justice Lab are working on another\
    \ project, Towards Democratic Auditing, to investigate how to increase citizen\
    \ participation and intervention where these systems are being implemented. AI\
    \ Now, note the need for greater involvement with civil society groups, particularly\
    \ groups advocating for social justice who have long-standing experience identifying\
    \ and challenging the biases embedded in social systems. Researchers at AI Now\
    \ have argued that government uses of automated and artificial intelligence systems\
    \ in the delivery of core services in criminal justice, healthcare, welfare and\
    \ education should stop until the risks and harms can be fully assessed and we\
    \ can decide on where, given the risks involved, there should be no go areas for\
    \ uses of automated systems because the risks are too great.\n\_\n\_\nNotes\n\
    [1] For example see: a) www.datakind.org, b) Gangadharan, SP (2013) \u2018How\
    \ can big data be used for social good\u2019, Guardian, 30 May, available: https://www.theguardian.com/sustainable-business/how-can-big-data-social-good,\
    \ c) Raghupathi, W and Raghupathi, V (2014) \u2018Big data analytics in healthcare:\
    \ promise and potential\u2019 Health Information Science and Systems 2(3), available:\
    \ https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC4341817/ d) Mayer-Sch\xF6nberger,\
    \ Viktor and Cukier, Kenneth. 2013. Big Data: A Revolution That Will Transform\
    \ How We Live, Work, and Think. New York: Houghton Mifflin Harcourt, e) Manyika,\
    \ James, Chui, Michael, Brown, Brad, Bughin, Jacques, Dobbs, Richard, Roxburgh,\
    \ Charles and Hung Byers, Angela. 2011. \u201CBig Data: The Next Frontier for\
    \ Innovation, Competition, and Productivity.\u201D McKinsey Global Institute,\
    \ f) Armah, Nii Ayi. 2013. \u201CBig Data Analysis: The Next Frontier.\u201D Bank\
    \ of Canada Review. Summer.\n[2] Cambridge Dictionary \u2018harm\u2019, available:\
    \ https://dictionary.cambridge.org/dictionary/english/harm, Oxford Living Dictionaries\
    \ \u2018harm\u2019, available: https://en.oxforddictionaries.com/definition/harm\n\
    [3] See Citron, D K and Pasquale, F (2014) The scored society: due process for\
    \ automated Predictions. Washington Law Review, 89: 1-33.\n[4] Medium (2018) Data\
    \ Violence and How Bad Engineering Can Damage Society. [Online]. Available on:\
    \ https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4\n\
    [5] See Lyon, D (2015) Surveillance as Social Sorting: Privacy, Risk and Automated\
    \ Discrimination, New York: Routledge.\n[6] Federal Trade Commission (2015) FTC\
    \ charges data brokers with helping scammer take more than $7 million from Consumers\u2019\
    \ Accounts, 12 August, available: https://www.ftc.gov/news-events/press-releases/2015/08/ftc-charges-data-brokers-helping-scammer-take-more-7-million\n\
    [7] Andrews, Lori. 2013. I Know Who You Are and I Saw What You Did: Social Networks\
    \ and the Death of \nPrivacy, New York: Free Press.\n[8] As cited in Hurley, M\
    \ and Adebayo, J (2016) Credit scoring in the era of big data, Yale Journal of\
    \ Law and \nTechnology, 18(1), p.151.\n[9] Ibid, p. 151\n[10] Office of Oversight\
    \ and Investigations Majority Staff (2013) A Review of the Data Broker Industry:\
    \ Collection, Use, and Sale of Consumer Data for Marketing Purposes, Staff Report\
    \ for Chairman Rockefeller, Dec. 18, available: https://www.commerce.senate.gov/public/_cache/files/0d2b3642-6221-4888-a631-08f2f255b577/AE5D72CBE7F44F5BFC846BECE22C875B.12.18.13-senate-commerce-committee-report-on-data-broker-industry.pdf\n\
    [11] Madden, M, Gilman, M, Levy, K and Marwick, A (2017) \u2018Privacy, Poverty,\
    \ and Big Data: A Matrix of Vulnerabilities for Poor Americans\u2019, Washington\
    \ University Law Review, 95(1)\n[12] Whitener, M (2015) \u2018Cookies are so yesterday;\
    \ Cross-Device Tracking is In \u2013 Some Tips\u2019, Privacy Advisor, 27 Jan.\
    \ available: https://iapp.org/news/a/cookies-are-so-yesterday-cross-device-tracking-is-insome-tips/\n\
    [13] Newman, N (2014) \u2018How big data enables economic harm to consumers, especially\
    \ to low-income and other vulnerable sectors of the population\u2019, Public Comments\
    \ to FTC, available: https://www.ftc.gov/\nsystem/files/documents/public_comments/2014/08/00015-92370.pdf\n\
    [14] As cited in Citron, D K and Pasquale, F (2014) The scored society: due process\
    \ for automated\nPredictions. Washington Law Review, 89, p. 15.\n[15] Ibid\n[16]\
    \ Valentino-DeVries, J, Singer-Vine, J., and Soltani, A (2012) \u2018Watched:\
    \ Websites vary prices, deals based on users\u2019 information\u2019, The Wall\
    \ Street Journal, 24 Dec., A1\n[17] The Guardian (2015) Facebook Still Suspending\
    \ Native Americans Over \u2018Real Name\u2019 Policy. [Online]. Available on:https://www.theguardian.com/technology/2015/feb/16/facebook-real-name-policy-suspends-native-americans?source=post_elevate_sequence_page\u2014\
    \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n[18] The New York Times (2019)\
    \ Amazon Faces Investor Pressure Over Facial Recognition. [Online]. Available\
    \ on: https://www.nytimes.com/2019/05/20/technology/amazon-facial-recognition.html\n\
    [19] The Guardian (2018) Amazon Face Recognition Falsely Matches 28 Lawmakers\
    \ with Mugshots, ACLU says. [Online]. Available on: https://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\n\
    [20] ACLU (2018) Amazon Teams up With Law Enforcement to Deploy Dangerous New\
    \ Face Recognition Technology. [Online]. Available on: https://www.aclunc.org/blog/amazon-teams-law-enforcement-deploy-dangerous-new-face-recognition-technology\n\
    [21] Datta, A, Tschantz, MC and Datta, A (2015) \u2018Automated Experiments on\
    \ Ad Privacy Settings\u2019, Proceedings on Privacy Enhancing Technologies, available:\
    \ https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml\n\
    [22] ProPublica (2016) Facebook Lets Advertisers Exclude Users by Race. [Online].\
    \ Available on: https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race\n\
    [23] BBC (2018) Facebook Accused of Job Ad Gender Discrimination. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/technology-45569227\n[24] Financial Times (2018)\
    \ Facebook \u201CDark Ads\u201D and Discrimination. [Online]. Available on: https://search.proquest.com/docview/2129787570?accountid=9883&rfr_id=info%3Axri%2Fsid%3Aprimo\n\
    [25] The Guardian (2019) Facebook Charged with Housing Discrimination in targeted\
    \ Ads. [Online]. Available on: https://www.theguardian.com/technology/2019/mar/28/facebook-ads-housing-discrimination-charges-us-government-hud\n\
    [26] Ohm, P. (2010). \u201CBroken Promises of Privacy: responding to the surprising\
    \ failure of anonymization\u201D, UCLA Law Review, vol 57 (2010) pp 1701\u2013\
    1777\n[27] Arvind Narayanan & Vitaly Shmatikov (2008), How to Break the Anonymity\
    \ of the Netflix Prize Dataset, available: https://arxiv.org/abs/cs/0610105\n\
    [28] Singel, R (2009) Netflix spilled your Brokeback Mountain secret, lawsuit\
    \ claims, Wired, 17 December, available: https://www.wired.com/2009/12/netflix-privacy-lawsuit/\n\
    [29] The Washington Post (2019) With Fitness Trackers in the Workplace, Bosses\
    \ Can Monitor Your Every Step- And Possibly More. [Online]. Available on: https://www.washingtonpost.com/business/economy/with-fitness-trackers-in-the-workplace-bosses-can-monitor-your-every-step\u2013\
    and-possibly-more/2019/02/15/75ee0848-2a45-11e9-b011-d8500644dc98_story.html?utm_term=.b48be1cf9096\n\
    [30] The Guardian (2019) There\u2019s a Dark Side to Women\u2019s Health Apps:\
    \ \u201CMenstrual Surveillance\u201D. [Online]. Available::https://www.theguardian.com/world/2019/apr/13/theres-a-dark-side-to-womens-health-apps-menstrual-surveillance\n\
    [31] BBC (2018) MyFitnessPal Breach Affects Millions of Under Armour Users. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-43592470\n[32] The Guardian\
    \ (2018) Personal Data of a Billion Indians Sold Online for \xA36, Report Claims.\
    \ [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [33] Reuters (2019) Your Health App Could be Sharing Your Medical Data. [Online].\
    \ Available on: https://www.reuters.com/article/us-health-apps-privacy/your-health-app-could-be-sharing-your-medical-data-idUSKCN1R326W\n\
    [34] The Guardian (2018) Fitness Tracking App Strava Gives Away Locations of Secret\
    \ US Army Bases. [Online]. Available on:https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases\n\
    [35] The Guardian (2018) Strava Suggest Military Users Opt Out of Heatmap as Row\
    \ Deepens. [Online]. Available on:https://www.theguardian.com/technology/2018/jan/29/strava-secret-army-base-locations-heatmap-public-users-military-ban\n\
    [36] The Guardian (2018) Employers Are Monitoring Computers, Toilet Breaks- Even\
    \ Emotions. Is Your Boss Watching You? [Online]. Available on: https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you\n\
    [37] Ibid.\n[38] The Guardian (2018) Alarm Over Talks to Implant UK Employees\
    \ with Microchips. [Online]. Available on: https://www.theguardian.com/technology/2018/nov/11/alarm-over-talks-to-implant-uk-employees-with-microchips\n\
    [39]Bloomberg (2019) Amazon Workers Are Listening to What You Tell Alexa. [Online].\
    \ Available on: https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio\n\
    [40] BBC (2015) Not in Front of the Telly: Warning Over \u2018Listening\u2019\
    \ TV. [Online]. Available on: https://www.bbc.co.uk/news/technology-31296188\n\
    [41] The Guardian (2017) Wikileaks Publishes \u2018Biggest leak Ever of Secret\
    \ CIA Documents\u2019. [Online]. Available on: https://www.theguardian.com/media/2017/mar/07/wikileaks-publishes-biggest-ever-leak-of-secret-cia-documents-hacking-surveillance\n\
    [42] Dranoff, S (2014) \u2018Identity Theft: A Low-Income Issue\u2019, Dialogue,\
    \ Winter, available: https://www.\namericanbar.org/groups/legal_services/publications/dialogue/volume/17/winter-2014/identity-theft\u2013\
    a-lowincome-issue.html\n[43] BBC (2015) Ashley Madison Infidelity Site Customer\
    \ Data Leaked. [Online]. Available on: https://www.bbc.co.uk/news/business-33984017\n\
    [44] The Guardian (2015) Infidelity Site Ashley Madison Hacked as Attackers Demand\
    \ Total Shutdown. [Online]. Available on: https://www.theguardian.com/technology/2015/jul/20/ashley-madison-hacked-cheating-site-total-shutdown\n\
    [45] Reuters (2017) Ashley Madison Parent in $11.2 Million Settlement Over Data\
    \ Breach. [Online]. Available on: https://www.reuters.com/article/us-ashleymadison-settlement-idUSKBN19Z2F0\n\
    [46] BBC (2015) Ashley Madison: \u2018Suicides\u2019 Over Website Hacks. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-34044506\n[47] The Guardian\
    \ (2015) Dating Site Hackers Expose the Details of Millions of Users. [Online].\
    \ Available on: https://www.theguardian.com/lifeandstyle/2015/may/21/adult-friendfinder-dating-site-hackers-expose-users-millions\n\
    [48] BBC (2016) Up To 400 Million Accounts in Adult Friend Finder Breach. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-37974266\n[49] BBC (2017)\
    \ Sweden Data Leak a \u2018Disaster\u2019, Says PM. [Online]. Available on: https://www.bbc.co.uk/news/technology-40705473\n\
    [50] The New York Times (2017) Swedish Government Scrambles to Contain Damage\
    \ From Data Breach. [Online]. Available on:https://www.nytimes.com/2017/07/25/world/europe/ibm-sweden-data-outsourcing.html\n\
    [51] BBC (2018) Aadhaar: \u201CLeak\u201D in World\u2019s Biggest Database Worries\
    \ Indians. [Online]. Available at: https://www.bbc.co.uk/news/world-asia-india-42575443\n\
    [52] The Guardian (2018) Personal Data of a Billion Indians Sold Online for \xA3\
    6, Report Claims. [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [53] Kaplan, E (2015) \u2018The Spy Who Fired me\u2019, Harper\u2019s, March,\
    \ available: https://harpers.org/archive/2015/03/\nthe-spy-who-fired-me/3/\n[54]\
    \ The Guardian (2018) \u201CI Made Steve Bannon\u2019s Psychological Warfare Tool\u201D\
    : Meet the Data War Whistleblower. [Online]. Available on: https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump\n\
    [55] Hu, M. (2015) \u2018Big Data Blacklisting\u2019, Florida Law Review, 67:\
    \ 1735-1809.\n[56] Dixon, P. (2017) A Failure to \u201CDo No Harm\u201D \u2013\
    \ India\u2019s Aadhaar Biometric ID Program and its Inability to Protect Privacy\
    \ in Relation to Measures in Europe and the U.S. Health Technol,7(4): 539-567.\
    \ [Online]. Available on: https://link.springer.com/content/pdf/10.1007%2Fs12553-017-0202-6.pdf\n\
    [57] Logan, WA and Ferguson, AG (2016) \u2018Policing Criminal Justice Data\u2019\
    , Minnesota Law Review 541, available: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2761069\n\
    [58] See: Sullivan, E and Greene, R (2015) States predict inmates\u2019 future\
    \ crimes with secretive\nSurveys. AP, Feb. 24, available at: http://bigstory.ap.org/article/;\
    \ Barocas, S and Selbst, A D (2016) Big data\u2019s disparate impact. California\
    \ Law Review 104: 671-732; Starr, S (2016) The odds of justice: actuarial risk\
    \ prediction and the criminal justice system. Chance 29(1): 49-51.\n[59] O\u2019\
    Neil, C (2016) Weapons of Math Destruction, London: Allen Lane, p. 84-87.\n[60]BBC\
    \ (2018) Met Police \u201CGang Matrix\u201D Requires Overhaul. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/uk-england-london-46646260\n\_\n[61] Evening\
    \ Standard (2018) Sadiq Khan Calls for Overhaul of Scotland Yard\u2019s Gang Matrix\
    \ as 4 in 5 Names on it are Shown to be Black. [Online]. Available on: https://www.standard.co.uk/news/crime/sadiq-khan-calls-for-overhaul-of-scotland-yards-gang-matrix-as-4-in-5-names-on-it-are-shown-to-be-a4024006.html\n\
    [62] Amnesty (2018) Trapped in the Matrix: Secrecy, stigma, and bias in the Met\u2019\
    s Gangs Database. London: Amnesty International United Kingdom Section. Available\
    \ on: https://www.amnesty.org.uk/files/2018-05/Trapped%20in%20the%20Matrix%20Amnesty%20report.pdf?HSxuOpdpZW_8neOqHt_Kxu1DKk_gHtSL\n\
    [63] StopWatch (2018) Being Matrixed: The (Over)policing of Gang Suspects in London.\
    \ [Online]Available on: http://www.stop-watch.org/uploads/documents/Being_Matrixed.pdf\n\
    [64] The Guardian (2018) Met Gang Matrix May be Discriminatory, Review Finds.\
    \ [Online]. Available on: https://www.theguardian.com/uk-news/2018/dec/21/metropolitan-police-gangs-matrix-review-london-mayor-discriminatory\n\
    [65] Throughout the record the hyperlinks provided link to individuals and groups\
    \ whose work raises concerns and also provides recommendations about how to reduce\
    \ data harms. In addition to those links, some examples of others doing work in\
    \ this area include those working as part of the FAT / ML Fairness, Accountability\
    \ and Transparency in Machine Learning group and the Algorithmic Justice League.\n\
    Share this:\nClick to share on X (Opens in new window)\nX\n\nClick to share on\
    \ Facebook (Opens in new window)\nFacebook\nLike this:Like Loading..."
  date: '2017-12-06T13:11:28+00:00'
  id: 56f67a56820e2515f15ade3c7124313a
  publication: Data Justice Lab
  tags: *id001
  title: Data Justice Lab
  url: https://datajusticelab.org/data-harm-record/#_ftn3
- author: Unknown Author
  content: "Teaching & Learning\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe Internet of Things\
    \ relies on sensors embedded in a wide variety of devices and systems to make\
    \ your life incredibly convenient while exchanging a stunning amount of very personal\
    \ information about you via cloud computing.\nCredit: Harry Campbell\n\n\n\n\n\
    \n\n\n        The New Age of Surveillance\n      \n\n\n\n          May 10, 2016\n\
    \      \n\n            By\n      Elaine McArdle \n\n\n\n\n\n\n\n\nCellphones may\
    \ be the least of your privacy concerns\nConsider your home in five years: Before\
    \ you\u2019re out of bed in the morning, the drapes open themselves, the shower\
    \ turns the water to the perfect temperature, and the toaster toasts your bagel\
    \ just the way you like. Motion detectors know when you\u2019ve left for work\
    \ and switch on your home security system as a robot vacuum begins cleaning your\
    \ floors. At the office, you realize you forgot to start the clothes dryer; a\
    \ simple voice command to your smartphone means the laundry\u2019s ready when\
    \ you get home. As you head up the driveway at night, sensors in your smart car\
    \ alert the garage door to open and the lights in your home to turn on while the\
    \ TV tunes itself to your favorite program.\nWelcome to the Internet of Things.\
    \ It may be about to change our lives as radically as the Internet itself did\
    \ 20 years ago.\n\n\n\n\n\nThe Internet of Things relies on sensors embedded in\
    \ a wide variety of devices and systems to make your life incredibly convenient\
    \ while exchanging a stunning amount of very personal information about you via\
    \ cloud computing. Credit: harry Campbell\n\n\u201CSome analysts think this is\
    \ the future\u2014it\u2019s huge, as big as the Internet and World Wide Web,\u201D\
    \ says David O\u2019Brien, a senior researcher at the Berkman Center for Internet\
    \ & Society at HLS. \u201CIt\u2019s hard to separate the hype from reality, but\
    \ signs suggest we\u2019re at the early stages of a tectonic shift.\u201D\nThe\
    \ Internet of Things, or\_IoT, relies on sensors embedded in a wide variety of\
    \ devices and systems to make your life incredibly convenient while exchanging\
    \ a stunning amount of very personal information about you via cloud computing.\n\
    This technology is already available in everything from home appliances to Fitbits\
    \ and children\u2019s toys, and over the next 10 years, it is expected to become\
    \ a multitrillion-dollar industry, according to a report released in February\
    \ by the Berkman Center, \u201CDon\u2019t Panic: Making Progress on the \u2018\
    Going Dark\u2019 Debate.\u201D\nAll that personal data\u2014just waiting to be\
    \ mined. The implications for privacy, national security, human rights, cyberespionage\
    \ and the economy are staggering.\nFor corporations, the IoT is the golden goose\
    \ of the very near future, with everyone from Amazon to Nike creating products\
    \ with cloud-connected sensors\u2014including cameras, microphones, fingerprint\
    \ readers, gyroscopes, motion detectors, and infrareds\u2014collecting streams\
    \ of data about your movements, preferences, and habits.\n\nThe Internet of Things\
    \ \u201Chas the potential to drastically change surveillance, providing more access\
    \ than ever in history,\u201D according to a new report by the Berkman Center\
    \ for Internet & Society at HLS.\n\nFor law enforcement, it\u2019s one reason\
    \ we are entering a \u201CGolden Age of Surveillance,\u201D to use a term coined\
    \ by Peter Swire and Kenesa Ahmad at the Center for Democracy & Technology in\
    \ Washington, D.C. The IoT \u201Chas the potential to drastically change surveillance,\
    \ providing more access than ever in history,\u201D says the Berkman Center\u2019\
    s report, the result of a highly unusual gathering of government intelligence\
    \ officials, think tank experts and HLS faculty who met for a series of off-the-record\
    \ conversations about cybersecurity over the previous year. Several leaders in\
    \ the U.S. Senate and House, and key staffers from the White House, have reached\
    \ out to Berkman about the report, which has also garnered praise from members\
    \ of the international intelligence community, O\u2019Brien notes.\nHuman Rights\
    \ and Encryption\n\nRelated\n\n\n\n\n\n\n\n\n\n\n\n\nHuman Rights and Encryption\n\
    \n\nLast fall, the Cyberlaw Clinic at the Berkman Center for Internet & Society\
    \ at Harvard Law School, produced a report for Amnesty International on the legal\
    \ issues surrounding encryption. While the encryption debate is most often painted\
    \ as a two-sided battle between law enforcement and technology companies,  there\
    \ are many other stakeholders around the world that are deeply concerned about\
    \ the widespread implications of regulating encryption in iPhones and other telecommunications\
    \ devices.\n\n\n\nRecently, the FBI and Apple waged a very public battle over\
    \ the government\u2019s access to encrypted data in an iPhone connected to the\
    \ terrorist attack in San Bernardino, California, last December, which ended in\
    \ late March when the FBI, with help from an unnamed third party, cracked the\
    \ encryption without Apple\u2019s help. But that has not ended the debate raging\
    \ over the legal status of encryption in telecommunications and other digital\
    \ devices. The government worries that its ability to protect the country from\
    \ terrorism and other crimes is \u201Cgoing dark\u201D as a result of widespread\
    \ \u201Cend-to-end encryption\u201D in smartphone operating systems and Internet\
    \ services\u2014where even the device manufacturers and service providers can\u2019\
    t see customers\u2019 data\u2014while civil libertarians and others say unlocking\
    \ iPhones won\u2019t solve the problem and in fact will raise serious new dangers\
    \ such as terrorists hacking into cellphones.\nBut at the very time the tussle\
    \ between Apple and the FBI was grabbing international headlines during the winter,\
    \ the Internet of Things was quietly stepping up to offer an overwhelming treasure\
    \ trove of information about all of us. In other words, even as the technology\
    \ gods close an encrypted window or two, they\u2019ve been opening huge, Internet-connected\
    \ doors.\n\n\n\n\n\nDavid O\u2019Brien, Jonathan Zittrain and Matt Olsen (from\
    \ left) are among the authors of a much-cited Berkman Center report on the encryption\
    \ debate. Samantha Bates (second from left) and Tiffany Lin (right) contributed\
    \ research and editing. Credit: Jessica Scranton\n\n\u201CThe good news and the\
    \ bad news is that we aren\u2019t \u2018going dark,\u2019\u201D says HLS Professor\
    \ Jonathan Zittrain \u201995, faculty director of the Berkman Center and a co-convener\
    \ of the group, along with Matt Olsen \u201988, former general counsel for the\
    \ NSA, and Bruce Schneier, a cybersecurity expert and fellow at Berkman. \u201C\
    It\u2019s good news because law enforcement isn\u2019t as hamstrung as they may\
    \ feel. If you look at the digital trails people are laying down, the clear trajectory\
    \ is toward much more available to someone with a subpoena than ever before,\u201D\
    \ Zittrain says.\nThe \u201Cgoing dark\u201D metaphor is the wrong one for several\
    \ reasons, the group agrees. For one, aside from Apple, most tech companies, including\
    \ Microsoft and Google, rely on access to unencrypted user data as their primary\
    \ revenue streams, selling your information to advertisers. In addition, so-called\
    \ \u201Cmetadata\u201D about your communications, such as location data from cellphones\
    \ and header information in emails, is unencrypted and is a useful investigatory\
    \ tool. Perhaps most importantly, the burgeoning Internet of Things offers a vast\
    \ array of new opportunities for surveillance through cameras, microphones, GPS\
    \ trackers and other sensors in your home, in your car, even on your wrist: Police\
    \ could seek a warrant to track your whereabouts through your Fitbit, say, or\
    \ watch and listen to you in your home via your baby monitor or Internet-connected\
    \ TV.\n\n\n\n\nPolice could seek a warrant to track a suspect\u2019s whereabouts\
    \ through a Fitbit.\n\n\n\n\n\u201CThere\u2019s a lot of opportunity to learn\
    \ about a suspect in a way that didn\u2019t exist 20 or even just 10 years ago,\u201D\
    \ says Olsen, former director of the National Counterterrorism Center, who has\
    \ been teaching a national security course at HLS this spring.\nAt the same time,\
    \ the fact that the cyberworld is not \u201Cgoing dark\u201D is also bad news,\
    \ Zittrain believes, because the overwhelming amount of personal information floating\
    \ in cyberspace raises \u201Ctroubling questions about how exposed to eavesdropping\
    \ the general public is poised to become,\u201D and how vulnerable to a host of\
    \ bad actors, including malicious hackers, cyberthieves, and terrorists.\n\n\n\
    \n\n\nAs technology develops, government will have more tools to \u201Cfind the\
    \ bad guys,\u201D says HLS Professor Jack Goldsmith, but he also believes that\
    \ \u201Cthe bad guys will have many more tools to evade the government.\u201D\
    \ Credit: Martha Stewart\n\n\u201CThe thrust of the report is that as technology\
    \ develops, government will have many more tools available to find the bad guys,\u201D\
    \ says HLS Professor Jack Goldsmith, a national security and terrorism expert\
    \ who was part of the group. \u201CBut it\u2019s also true that the bad guys will\
    \ have many more tools to evade the government.\u201D\nFor now, he says, it\u2019\
    s unclear who\u2019s going to come out ahead.\nApple v. the FBI\nOf course, this\
    \ rapidly expanding compendium of potential information via the IoT is \u201C\
    small solace to a prosecutor holding both a warrant and an iPhone with a password\
    \ that can\u2019t be readily cracked,\u201D as Zittrain puts it, which was precisely\
    \ the case in the Apple-FBI showdown.\nApple itself couldn\u2019t access the data\
    \ in end-to-end encrypted iPhones (most modern iPhones are encrypted, by default)\
    \ and insisted it should not be forced to write a software program to assist in\
    \ bypassing the passcode on the iPhone used by Syed Rizwan Farook when he and\
    \ his wife killed 14 people in San Bernardino. Apple argued\u2014with many Silicon\
    \ Valley companies in strong support\u2014that a court order to do so would set\
    \ a disturbing precedent of \u201Cbackdoor access\u201D that would leave it vulnerable\
    \ to a rash of similar requests, including from foreign governments, placing the\
    \ privacy\u2014and in the case of political dissidents, the personal safety\u2014\
    of all iPhone users in jeopardy.\n\u201CThis is big stuff; this is dramatic stuff.\
    \ I have never seen a company\u2014a Fortune 500 company, let alone one of the\
    \ five biggest companies on earth\u2014take on the government this way,\u201D\
    \ including with a lengthy letter defending Apple\u2019s position by CEO Tim Cook\
    \ posted on the company\u2019s website, says Vivek Krishnamurthy, a clinical instructor\
    \ at Berkman\u2019s Cyberlaw Clinic. Stakeholders around the world have been watching\
    \ with deep interest (see sidebar).\nEven before the FBI cracked the code, counterterrorism\
    \ expert Olsen and many others in law enforcement agreed it was essential for\
    \ the FBI to access the data on that particular phone. \u201CThere\u2019s every\
    \ reason to think that the cellphone Farook used could contain critical information\
    \ and evidence,\u201D he said, in an interview before the code was cracked. \u201C\
    There\u2019s still a lot we don\u2019t know about the attack. Was it directed\
    \ by ISIS or some other terrorist group? Were the shooters part of a cell? Are\
    \ there others planning additional attacks? There are lots of important questions\
    \ the FBI is responsible for answering.\u201D\nThe government had a strong position\
    \ because the circumstances of the case were so compelling, and because it \u201C\
    did everything right\u201D by obtaining a warrant and also trying to access the\
    \ data without Apple\u2019s assistance before turning to the court to compel Apple\u2019\
    s help, Olsen argued. Moreover, the request was narrowly tailored to one phone,\
    \ he added, the property of Farook\u2019s employer, which had consented to disabling\
    \ the security feature.\nBut Schneier argues that our national security is better\
    \ protected by strong encryption despite the difficulties it presents to law enforcement.\
    \ \u201CIf a back door exists, then anyone can exploit it,\u201D Schneier wrote\
    \ in a New York Times blog. \u201CThat means that if the FBI can eavesdrop on\
    \ your conversations or get into your computers without your consent, so can cybercriminals.\
    \ So can the Chinese. So can terrorists.\u201D\n\n\n\n\n\nJennifer Daskal \u2019\
    01 agrees that while encryption makes it harder for government to do its job,\
    \ \u201Cbackdoor\u201D access to encrypted devices would not be worth the costs.\n\
    \n\u201CEncryption makes it harder for the government to do its job\u2014that\u2019\
    s indisputable,\u201D says Jennifer Daskal \u201901, former senior counterterrorism\
    \ counsel at Human Rights Watch who now teaches at American University Washington\
    \ College of Law. \u201CBut is the increased ease of government access worth the\
    \ security costs that would result from a government-mandated back door? I don\u2019\
    t think it is.\u201D The new Berkman report, she adds, \u201Cpoints to a whole\
    \ host of other potential ways for the government to access sought-after information.\
    \ Some may be more costly or time-consuming for the government, but they are much\
    \ preferable to the kind of insecurities\u2014as well as costs to American businesses\u2014\
    that would result from mandatory back doors.\u201D\nMatt Perault \u201908 is head\
    \ of global policy development at Facebook. While Facebook recognizes that law\
    \ enforcement has an important role in fighting \u201Clegitimate threats to public\
    \ safety,\u201D the company \u201Cwill fight aggressively against requirements\
    \ for companies to weaken the security of their systems,\u201D he says. \u201C\
    We can\u2019t make it easier for law enforcement to access encrypted communications\
    \ without making it easier for cyber\xADcriminals and foreign governments to do\
    \ the same.\u201D\n\n\n\n\n\nMatt Perault \u201908, head of global policy development\
    \ at Facebook, says the company \u201Cwill fight aggressively against requirements\
    \ for companies to weaken the security of their systems.\u201D Credit: The Washington\
    \ Post\n\nZittrain points out that a larger battle looms between companies like\
    \ Apple and law enforcement organizations such as the FBI. \u201CApple is in a\
    \ position to make a new generation of phones that even Apple can\u2019t crack,\u201D\
    \ he says. \u201CAt that point, the tension between law enforcement and industry\
    \ will shift from a one-off demand for assistance through judicial action to the\
    \ U.S. Congress, which might be asked to mandate how companies build their products\
    \ and services.\u201D\nUltimately, it is up to Congress to resolve the issue,\
    \ Goldsmith says: \u201CThe balance must be struck by Congress because there will\
    \ not be agreement on the costs and benefits, and in a democracy, that\u2019s\
    \ for Congress to sort out.\u201D While he predicts that Congress ultimately will\
    \ force tech companies to give government the tools it needs for criminal investigations,\
    \ he doesn\u2019t see it happening any time soon, given the political paralysis\
    \ in Washington, D.C.\n\n\n\n\nWithout regulation, there\u2019s a risk the Internet\
    \ of Things \u201Cwill become the Wild West of the Internet.\u201D\n\n\n\n\nMeanwhile,\
    \ the Internet of Things is raising \u201Cnew and difficult questions about privacy\
    \ over the long term,\u201D according to Zittrain, yet is mushrooming with almost\
    \ no legislative or regulatory oversight. We are, he says, \u201Churtling toward\
    \ a world in which a truly staggering amount of data will be only a warrant or\
    \ a subpoena away, and in many jurisdictions, even that gap need not be traversed.\u201D\
    \ The law\u2014notoriously slow in responding to technological changes\u2014may\
    \ be facing one of its biggest challenges yet.\n\u201CWhile a number of federal\
    \ agencies have flagged the IoT as being problematic for both privacy and security\
    \ reasons,\u201D says O\u2019Brien, \u201Clittle has happened,\u201D which demonstrates\
    \ not only a lack of coordination among various arms of the government but also\
    \ uncertainty about competing policy interests between innovation and consumer\
    \ protectionism. A strict regulatory regime could hamper this growing part of\
    \ the economy, but on the other hand, without regulation, \u201Cthere\u2019s a\
    \ risk the IoT will become the Wild West of the Internet,\u201D he says, adding,\
    \ \u201CSome might argue we\u2019re already headed in that direction.\u201D\n\u201C\
    It\u2019s a wake-up call that we really should be thinking about building in certain\
    \ protections now for the Internet of Things,\u201D says Zittrain, who wants academia,\
    \ governments and industry to focus on developing an Internet of Things \u201C\
    Bill of Rights.\u201D\n\u201CThat\u2019s why this report and the deliberations\
    \ behind it are genuinely only a beginning, and there\u2019s much more work to\
    \ do before the future is upon us.\u201D\n\n\n\n\nModal Gallery\n\n\n\n\nClose\
    \ modal gallery\n\n\n\n\n\n\n\n of \n\n\n\n\n\n\nPrevious\nNext\n\n\n\n\n\n\n\
    Gallery Block Modal Gallery\n\n\n\n\nClose modal gallery\n\n\n\n\n\n\n\n\n\n\n\
    \n\n\n\n of \n\n\n\n\n\n\nPrevious\nNext"
  date: Unknown Date
  id: d12115891f8145db4cfdd1b7c4c4357c
  publication: Data Justice Lab
  tags: *id001
  title: Harvard Law Bulletin
  url: https://today.law.harvard.edu/feature/new-age-surveillance/
- author: Alex Wright
  content: "News\n\n\nArchitecture and Hardware \n\n\t\t\t\tNews\t\t\t\n\nMapping\
    \ the Internet of Things\nResearchers are discovering surprising new risks across\
    \ the fast-growing IoT.\n\n\n\t\t\t\tBy Alex Wright \n\nPosted Jan 1 2017 \n\n\
    \n\n\n\n\n\n\nArticle\nAuthor\nFigures \n\n\n\n\n Shodan founder John Matherly\
    \ used the search engine to map all Internet-connected device in the world.\n\n\
    \n\nAs more and more physical objects get connected to the Internet\u2014from\
    \ consumer products like webcams and pacemakers to industrial equipment like wind\
    \ turbines and power plants\u2014the contours of the Internet are shifting beyond\
    \ the realm of screen-based devices to encompass a much broader swath of the world\
    \ around us.\nWherever the Internet goes, security risks seem to follow. As the\
    \ Internet of Things (IoT) continues to expand, those risks are taking on new\
    \ dimensions well beyond the familiar threats of stolen passwords and credit cards.\n\
    \   \n\u201CWhen you say \u2018Internet of Things,\u2019 the first thing most\
    \ people think of are Apple Watches or Fitbits,\u201D says David O\u2019Brien,\
    \ a senior researcher at Harvard University\u2019s Berkman-Klein Center for the\
    \ Internet and Society. \u201CThey\u2019re not thinking about programmable logic\
    \ controllers or other infrastructure devices.\u201D\nIndustrial computing devices\
    \ are a vast, largely invisible realm of the IoT, one that remains out of sight\
    \ to most of us, yet plays a critical role in sustaining our everyday quality\
    \ of life: power plants, water pumps, and oil rigs all rely on industrial computers\
    \ connected to the Internet, and these devices appear to be far less secure than\
    \ we might assume.\nThe lack of security across the industrial IoT has come to\
    \ light largely thanks to an experimental search engine called Shodan. First launched\
    \ in 2009, the service now crawls nearly four billion devices over the IPv4 network,\
    \ as well as a number of IPv6-connected devices. At any given time, it monitors\
    \ about 700 million devices (depending on network connectivity, and whether the\
    \ devices are turned off or on).\nShodan\u2019s creator, John Matherly, first\
    \ started work on the service as a teenager in the mid-2000s. \u201CThe idea for\
    \ Shodan came to me during the age of peer-to-peer software such as Napster and\
    \ E-mule. The original concept was to provide a tool that would let security researchers\
    \ scan networks and share the data (via P2P) with others.\u201D\nUnlike Web browsers\
    \ that traverse the Internet via the Hypertext Transport Protocol (HTTP), Shodan\
    \ surveys other TCP/IP-connected ports including FTP, SSH, SNMP, SIP and RTSP\
    \ ports in search of responsive servers. When it receives a welcome message (or\
    \ any response), it retrieves what metadata it can find, and catalogs the information.\n\
    At first, Matherly envisioned collecting data on the kinds of Internet-connected\
    \ products in use, and create a repository of information about patches, site\
    \ licenses, and other useful meta-data. Like many a project that started out as\
    \ an interesting hack, however, Shodan has since taken on some interesting, unexpected\
    \ applications.\nOver the past few years, Shodan users have uncovered a series\
    \ of alarming network vulnerabilities in Internet-connected devices, including\
    \ a nuclear reactor; the cyclotron at the Lawrence Livermore National Laboratory\
    \ outside Berkeley, CA; a water treatment plant outside Houston; electric power\
    \ generators; oil rigs; and even a crematorium.\nEireann Leverett, a researcher\
    \ in the Centre for Risk Studies at the University of Cambridge, U.K., used Shodan\
    \ to identify more than 100,000 vulnerable IoT devices in 2011, concluding these\
    \ flaws left them vulnerable to attack by \u201Cmalicious actors.\u201D In a similar\
    \ vein, Billy Rios at Google and Michael McCorkle of Boeing also have identified\
    \ a series of serious security exposures across a wide range of connected industrial\
    \ devices.\nTo Matherly\u2019s surprise, many of these devices turned out to be\
    \ special-purpose industrial computers: control systems that perform highly specific\
    \ tasks, like regulating the flow of water and other utilities, transportation\
    \ systems, and even entire power grids\u2014all controlled over the network by\
    \ remote supervisory staff.\nUnlike the consumer-facing Websites that most of\
    \ us can find readily using commercial search engines like Google, industrial\
    \ control systems (ICS) have largely remained hidden in plain view, invisible\
    \ to web crawlers. Since Shodan\u2019s launch, however, it has shone an unforgiving\
    \ light on some of these devices\u2019 glaring security flaws.\n\u201CIndustrial\
    \ control systems have relied on security by obscurity,\u201D says Mather, who\
    \ now spends much of his time consulting with organizations on strengthening the\
    \ network security of these devices.\nMost of these devices rely on proprietary\
    \ hardware and software protocols that tend to mask their vulnerabilities\u2014\
    but also make it difficult for security researchers to develop generalized and\
    \ replicable approaches to security. \u201CThe more accessible the technology,\
    \ the easier it is for people to find and fix vulnerabilities,\u201D says Mather.\n\
    More troublingly, many vendors failed to treat these risks seriously, assuming\
    \ these systems could only be addressed directly, rather than over an external\
    \ network. As a result, many hardware makers have tended to treat potential vulnerabilities\
    \ lightly.\n\n\n\u201CIt\u2019s a technical problem, but it\u2019s also closely\
    \ tied to business interests \u2026 these days, the way companies tend to look\
    \ at security is as a loss leader.\u201D\n\n\nO\u2019Brien feels these exposures\
    \ stem not just from technical failures, but from a fundamental lack of industry\
    \ focus on security. \u201CIt\u2019s a technical problem, but it\u2019s also closely\
    \ tied to business interests,\u201D he says. \u201CThese days, the way companies\
    \ tend to look at security is as a loss leader.\u201D\nMoreover, customers for\
    \ these systems\u2014like, say, power plant operators\u2014tend to resist adding\
    \ layers of security, to ensure their ability to respond quickly in case of emergency.\
    \ End-users within these organizations often see additional security controls\u2014\
    like layers of password prompts\u2014as more of a burden than a benefit.\nGiven\
    \ the lack of customer demand, product managers at hardware companies often find\
    \ it difficult to justify investing resources in preventive security measures\
    \ that do not add new functionality. Complicating matters further is the difficulty\
    \ of sending updates and patches to these devices without user-initiated firmware\
    \ updates\u2014a common practice for Web-based software applications. As a result,\
    \ these industrial devices can often remain vulnerable for extended periods of\
    \ time.\n\u201CTo be fair, many of these systems were designed before the age\
    \ of ubiquitous connectivity,\u201D says Mather, \u201Cso the engineers didn\u2019\
    t worry about hardening their device against software attacks.\u201D\nMather also\
    \ points to economic factors at play: \u201CThere wasn\u2019t a push by the ICS\
    \ operators to demand better computer security from the manufacturers.\u201D Instead,\
    \ they tended to focus more on issues of availability and reliability, and treated\
    \ security as a secondary consideration.\nThat is now starting to change, thanks\
    \ in part to the visibility that Shodan has brought to these vulnerabilities.\
    \ In a similar vein, an open source project called Onionscan has made considerable\
    \ headway in exposing the possible vulnerabilities of physical devices over the\
    \ Internet.\nLooking ahead, Shodan is focused on developing more sophisticated\
    \ tools and visualizations to make the data more accessible to non-technical users.\n\
    Elsewhere, Nathan Freitas of the Guardian Project is spearheading an effort to\
    \ use Tor\u2014a free software package often used by hackers and journalists to\
    \ protect their privacy via a worldwide network of volunteer-run servers\u2014\
    to safeguard IoT devices by means of Home Assistant, a Python-based system that\
    \ allows for Tor to be used for physical devices. The system relies on a Raspberry\
    \ Pi computer running Tor\u2019s software to mask the location of smart home devices\
    \ by means of an authenticated hidden service that prevents anyone from locating\
    \ and connecting to the devices without access to a passcode that the developers\
    \ describe as a \u201Ccookie.\u201D\nWhile this technology remains in the experimental\
    \ stage, Freitas hopes it will pave the way for more fully developed commercial\
    \ IoT security applications in the figure.\nAmid the rise of connected devices\
    \ and growing public concern about Stuxnet-style attacks on major infrastructure\
    \ projects, the conditions seem ripe for IoT security applications to find more\
    \ traction in the marketplace. Yet Mather feels the industry at large remains\
    \ too blithe about these dangers.\n\u201CWe keep deploying new devices that are\
    \ insecure-by-default,\u201D he explains. \u201CThe vulnerable IoT devices of\
    \ today that get installed are going to stick around a long time and they have\
    \ access to the internal networks of many homes and businesses.\u201D\nThat might\
    \ seem like a borderline-paranoid fantasy, but the rapidly accelerating development\
    \ of \u201Csmart hardware\u201D devices may bring these risks closer to home\u2014\
    and soon. For example, some firms are developing light bulbs that serve as Internet\
    \ hubs, relaying Wi-Fi signals, connecting thermostats, or even interfacing with\
    \ a home security system. As these everyday devices become increasingly interconnected,\
    \ the security risks multiply exponentially.\nO\u2019Brien believes the long-term\
    \ solution to IoT security will require a balanced approach to technological innovation\
    \ and public policy-making to create a more reliable physical computing infrastructure.\n\
    \n\nSecuring critical infrastructure remains an area of unclear jurisdictional\
    \ ownership within the U.S. federal government.\n\n\n\u201CWe\u2019re at a point\
    \ where we\u2019re rethinking what the role of government ought to be,\u201D he\
    \ says. Despite a number of high-profile cyberattacks in recent years, securing\
    \ critical infrastructure remains an area of unclear jurisdictional ownership\
    \ within the federal government, partially involving the Federal Bureau of Investigation\
    \ (FBI), Department of Homeland Security, and other agencies. The White House\
    \ recently released a statement clarifying the role of first responders in cybersecurity\
    \ attacks, but there is plenty of work left to do on this front.\nMeanwhile, Mather\
    \ worries people fail to recognize the growing sophistication of seemingly everyday\
    \ devices like light bulbs or coffee machines that are fast becoming part of a\
    \ deeply interconnected\u2014and potentially insecure\u2014worldwide network of\
    \ smart objects. \u201CThose devices are full-fledged computers nowadays,\u201D\
    \ he explains, \u201Cand with the increasing number of IoT devices that are being\
    \ deployed, those vulnerabilities become a real concern.\u201D\n\n\n\n\n\t\t\_\
    Further Reading\nLeverett, E. Quantitatively Assessing and Visualising Industrial\
    \ System Attack Surfaces. University of Cambridge Computer Laboratory, Darwin\
    \ College, June 2011. http://www.cl.cam.ac.uk/~fms27/papers/2011-Leverett-industrial.pdf\
    \  National Science and Technology Council, Federal Cybersecurity Research and\
    \ Development Strategic Plan: Ensuring Prosperity and National Security, 2016.\
    \ https://www.whitehouse.gov/sites/whitehouse.gov/files/documents/2016_Federal_Cybersecurity_Research_and_Development_Stratgeic_Plan.pdf\n\
    Olsen, M., Schneier, B., Zittrain, J. Don\u2019t Panic: Making Progress on the\
    \ \u2018Going Dark\u2019 Debate. Berkman Center for Internet and Society, Harvard\
    \ University, February 1, 2016. https://cyber.law.harvard.edu/pubrelease/dont-panic/Dont_Panic_Making_Progress_on_Going_Dark_Debate.pdf\n\
    Seitz, J. Dark Web OSINT With Python and OnionScan. Automating OSINT, July 28,\
    \ 2016. http://www.automatingosint.com/blog/2016/07/dark-web-osint-with-python-and-onionscan-part-one/\n\
    \nBack to Top\nBack to Top\n\n\n\nFigures\n\n\n\n\nFigure. Shodan founder John\
    \ Matherly used the search engine to map all Internet-connected device in the\
    \ world.\nBack to top\n\n\n\n\n\n\n\n\t\t\tAbout the Authors\t\t\n\n \n\n\n\n\n\
    \nAuthor\nAlex Wright is a writer and researcher based in Brooklyn, NY.\n \n\n\
    \n\n\n\n\n\n\n\n\t\t\t\tSubmit an Article to CACM\t\t\t\n\n\t\t\t\tCACM welcomes\
    \ unsolicited submissions on topics of relevance and value to the computing community.\t\
    \t\t\n\n\n\n\n\n\n\t\t\tYou Just Read\t\t\n\n\t\t\tMapping the Internet of Things\t\
    \t\n\n\t\t\t\tView in the ACM Digital Library\t\t\t\t \n\n\n\n\n\xA92017 ACM\_\
    \_0001-0782/17/01\nPermission to make digital or hard copies of part or all of\
    \ this work for personal or classroom use is granted without fee provided that\
    \ copies are not made or distributed for profit or commercial advantage and that\
    \ copies bear this notice and full citation on the first page. Copyright for components\
    \ of this work owned by others than ACM must be honored. Abstracting with credit\
    \ is permitted. To copy otherwise, to republish, to post on servers, or to redistribute\
    \ to lists, requires prior specific permission and/or fee. Request permission\
    \ to publish from permissions@acm.org or fax (212) 869-0481.\n The Digital Library\
    \ is published by the Association for Computing Machinery. Copyright\_\xA9\_2017\
    \ ACM, Inc. \n\n\n\n\nDOI\n\t10.1145/3014392\n\n\nJanuary 2017 Issue\nPublished:\
    \ January 1, 2017\nVol. 60 No. 1\nPages: 16-18\n\n\n\t\t\tTable of Contents\t\t\
    \t\n\n\n\n\nRelated Reading\n\n\n\n\nBLOG@CACM \n\n\n\t\t\t\t\t\t\tIntermittent\
    \ Net and Mobile/Cloud Development\t\t\t\t\t\t\n\n\nArchitecture and Hardware\
    \ \n\n\n\nOpinion \n\n\n\t\t\t\t\t\t\tQ&A: Big Challenge\t\t\t\t\t\t\n\n\nArtificial\
    \ Intelligence and Machine Learning \n\n\n\nResearch Highlights \n\n\n\t\t\t\t\
    \t\t\tTechnical Perspective: Can We Uncover Private Backbone Infrastructures?\t\
    \t\t\t\t\t\n\n\nArtificial Intelligence and Machine Learning \n\n\n\nOpinion \n\
    \n\n\t\t\t\t\t\t\tLog on Education: Handheld Devices Are Ready-at-Hand\t\t\t\t\
    \t\t\n\n\nComputing Applications \n\n\n\n\n\n\nAdvertisement\n\n\n\n\n\n\nAdvertisement\n\
    \n\n\n\n\n\n\n\n\t\t\tJoin the Discussion (0)\t\t\n\n\nBecome a Member or Sign\
    \ In to Post a Comment\n\nSign In\nSign Up\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\
    \n\n\n\n\n\t\t\t\t\t\tThe Latest from CACM\t\t\t\t\t\n\n\n\nExplore More\n\n\n\
    \n\n\n\n\n\n\n\nNews  May 1 2025\n\n\nMachine Learning Framework Integrates Geometry\
    \ into Fast PDE Solving\n\n\n\nAllyn Jackson \nArtificial Intelligence and Machine\
    \ Learning\n\n\n\n\n \n\n\n\n\n\n\n\n\n\nNews  May 1 2025\n\n\nRethinking Social\
    \ Media\u2019s Future\n\n\n\nBennie Mols \nSociety\n\n\n\n\n \n\n\n\n\n\n\n\n\n\
    \nBLOG@CACM  Apr 29 2025\n\n\nWhat Ever Happened to\u2026and Other Tech History\n\
    \n\n\nDoug Meil \nComputer History\n\n\n\n\n \n\n\n\n\n \n\n\n\n\n\n\t\t\t\t\t\
    Shape the Future of Computing\t\t\t\t\n\n\t\t\t\t\t\tACM encourages its members\
    \ to take a direct hand in shaping the future of the association. There are more\
    \ ways than ever to get involved.\t\t\t\t\t\n\n\t\t\t\t\t\tGet Involved\t\t\t\t\
    \t\t\t\t\t\t\t\n\n\n\n\n\n\t\t\t\t\tCommunications of the ACM (CACM) is now a\
    \ fully Open Access publication.\t\t\t\t\n\n\t\t\t\t\t\tBy opening CACM to the\
    \ world, we hope to increase engagement among the broader computer science community\
    \ and encourage non-members to discover the rich resources ACM has to offer.\t\
    \t\t\t\t\n\n\t\t\t\t\t\tLearn More"
  date: Jan 1 2017
  id: 99c541d7c8c8d2cb27891664977e5a3f
  publication: Data Justice Lab
  tags: *id001
  title: Mapping the Internet of Things
  url: https://cacm.acm.org/magazines/2017/1/211101-mapping-the-internet-of-things/fulltext
- author: Unknown Author
  content: "Data Harm Record \nData Harm Record (Updated)\nUpdated August 2020\nJoanna\
    \ Redden, Jessica Brand and Vanesa Terzieva\_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_ \_\
    \ \_ \_ \_ \_ \_ \_ \_ \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\nThe aim of this\
    \ document is to provide a running record of \u2018data harms\u2019, harms that\
    \ have been caused by uses of algorithmic systems. The goal is to document so\
    \ that we can learn from where things have gone wrong and ideally together work\
    \ toward redressing harms and preventing further harm. The document compiles examples\
    \ of harms that have been detailed in previous research and publications. Each\
    \ listed example contains a link to the original source and often also related\
    \ information.\nThe Data Harm Record pulls together concrete examples of harm\
    \ that have been referenced in previous work so that we might gain a better \u2018\
    big picture\u2019 appreciation of how people have already been negatively affected\
    \ by uses of algorithmic systems. A survey of harms also suggests where things\
    \ may go wrong in the future and ideally stimulates more debate and interventions\
    \ into where we may want to change course. The idea is that we can learn a lot\
    \ by paying attention to where things have gone wrong and by considering data\
    \ harms in relation to each other.\nThe Data Harm Record was first published in\
    \ 2017. Over the last year we have attempted to update it with recent examples.\
    \ We have tried to capture a wide range of examples, but there are gaps in what\
    \ we have been able to identify and list here due to time, resource and language\
    \ limitations.\n\_\nBackground\nPeople working in business, government, politics\
    \ and for non-profit organizations are all developing new ways to make use of\
    \ algorithmic systems. These bodies have always collected and analysed data, but\
    \ what\u2019s changed is the size, scope and methods to analyse data. The digitization\
    \ of near everything along with major computing advances mean that it is now possible\
    \ to combine sizes and types of data previously unimaginable, and to then analyse\
    \ these staggering datasets in new ways to find patterns and make predictions.\n\
    There is an abundance of enthusiasm and optimism about how automated, predictive\
    \ and AI data systems can be used for good. Optimism persists for good reason,\
    \ there is a lot of good that can be done through new uses of data systems [1]\
    \ There is also growing consensus that with these new algorithmic systems comes\
    \ risks to individuals and society. Previous work has detailed how data analytics\
    \ can be used in ways that threaten privacy, security, as well as increase inequality\
    \ and discrimination. The danger with automated and predictive decision support\
    \ systems is that harms can be caused unintentionally and intentionally.\nAs argued\
    \ by Cathy O\u2019Neil , this is important to keep in mind as in many cases the\
    \ algorithmic systems that are leading to harm were developed with very good intentions.\
    \ The problem is that new algorithmic tools present new ways to sort, profile,\
    \ exclude, exploit, and discriminate. The complexity, opacity, and proprietary\
    \ nature of many datafied systems mean that often we don\u2019t know things have\
    \ gone wrong until after large numbers of people have been affected. Another problem\
    \ is that few people have the skills needed to interrogate and challenge these\
    \ new automated and predictive systems. What recourse do citizens have if they\
    \ have been wrongfully targeted, profiled, excluded or exploited? Government agencies,\
    \ civil society organizations and researchers across disciplines are drawing attention\
    \ to these risks.\n\_\nDefining data harms\nDictionary definitions of harm link\
    \ it to physical and material injuries, but also to potential injuries, damages\
    \ and adverse effects.[2] Solove and Citron argue that harm can be understood\
    \ as \u2018the impairment, or set back, of a person, entity, or society\u2019\
    s interests. People or entities suffer harm if they are in worse shape than they\
    \ would be had the activity not occurred\u2019.[3]\nBuilding on these definitions,\
    \ one way to understand data harms is as the adverse effects caused by uses of\
    \ data that may impair, injure, or set back a person, entity or society\u2019\
    s interests. While this definition is a start, clearly it is insufficient and\
    \ will need to be developed given the increasing ubiquity of datafied practices\
    \ all around us.\nOur legal and political systems are struggling to come to terms\
    \ with data harms. Across nations it is becoming easier for corporate and government\
    \ bodies to share data internally and externally. New data about us is being generated\
    \ by us and collected by others through new systems. Consider for example the\
    \ range of data that can be generated and collected through the Internet of Things\
    \ and also the range of harms that can be caused if the wrong people hack into\
    \ industrial systems. Increasingly, our digital selves and the digitization of\
    \ services affect the kind of lives we lead, the opportunities afforded to us,\
    \ the services we can access and the ways we are treated. All of these developments\
    \ present new types of risk and harm. For all of these reasons we need to develop\
    \ a more complex understanding and appreciation of data harms and a means to assess\
    \ current and future harms, from the perspective of people who are and may be\
    \ negatively affected by these harms.\n\_\nData violence \nThe harms can be so\
    \ significant that researchers like Anna Lauren Hoffman are arguing that we need\
    \ to go further and recognize that in many cases we are dealing with \u2018data\
    \ violence.\u2019 [4] One example of data violence is when people are wrongly\
    \ denied access to essential services and resources.\n\_We know that algorithms\
    \ and automated systems are increasingly being used in decision- making: for job\
    \ recruitment, risk assessment , credit and bail hearings in the US  among others.\
    \ Research is documenting the ways these systems can embed bias. The implementation\
    \ of algorithmic systems in areas that link people to essential services means\
    \ that the bias and errors introduced via these algorithms can cause significant\
    \ harm. Research demonstrates that the already marginalized are far more likely\
    \ to be negatively affected. To quote Virginia Eubanks: \u201CThese systems impact\
    \ all of us, but they don\u2019t impact us all equally\u201D.\n\_\nExamples\n\
    Commercial uses of data \u2013 Exploitation\nTargeting based on perceived vulnerability\n\
    Some have drawn attention to how new tools make it possible to discriminate and\
    \ socially sort with increasing precision. By combining multiple forms of data\
    \ sets a lot can be learned.[5] Newman calls this \u2018algorithmic profiling\u2019\
    \ and raises concern about how much of this profiling is invisible as citizens\
    \ are unaware of how data is collected about them across searches, transactions,\
    \ site visits, movements, etc. This data can be used to profile and sort people\
    \ into marketing categories, some highly problematic. For example, data brokers\
    \ combine data sets to identify specific groups. Much of this sorting goes under\
    \ the radar. Some of it raises serious concerns. In her testimony to Congress,\
    \ World Privacy Forum\u2019s Pam Dixon reported finding brokers selling lists\
    \ of rape victims, addresses of domestic violence shelters, sufferers of genetic\
    \ diseases, sufferers of addiction and more.\n\nAnother example, in 2015 the U.S.\
    \ Federal Trade Commission \u2018charged a data broker operation with illegally\
    \ selling payday loan applicants\u2019 financial information to a scam operation\
    \ that took millions from consumers by debiting their bank accounts and charging\
    \ their credit cards without their consent\u2019.[6]\n\_\nWhen your personal information\
    \ gets used against you \nConcerns have been raised about how credit card companies\
    \ are using personal details like where someone shops or whether or not they have\
    \ paid for marriage counselling to set rates and limits.[7] This has been called\
    \ \u2018personalization\u2019, or \u2018behavioural analysis\u2019 or \u2018behavioural\
    \ scoring\u2019 and refers to companies tailoring things to people based on what\
    \ is known about them. Croll notes that American Express used purchase history\
    \ to adjust credit limits based on where customers shopped. Croll as well as Hurley\
    \ and Adebayo , describe the case of one man who found his credit rating reduced\
    \ from $10,800 to $3,800 in 2008 because American Express determined that \u2018\
    other customers who ha[d] used their card at establishments where [he] recently\
    \ shopped have a poor repayment history with American Express\u2019.[8] This event,\
    \ in 2008, was an early example of \u2018creditworthiness by association\u2019\
    \ and is linked to ongoing practices of determining value or trustworthiness by\
    \ drawing on \u2018big data\u2019 to make predictions about people.[9]\n\_\nDiscrimination\
    \ \u2013 skin colour, ethnicity, class or religion\nCredit Scoring\nAs companies\
    \ responsible for credit scoring, background checks, and hiring make more use\
    \ of automated data systems, an individual\u2019s appearance, background, personal\
    \ details, social network, or socio-economic status may influence their ability\
    \ to get housing, insurance, access education, or a job.\nThere are new start-up\
    \ companies that make use of a range of \u2018alternative\u2019 data points to\
    \ make predictions about consumers and provide people with credit scores. In addition,\
    \ traditional credit scoring agencies are making use of data and machine learning\
    \ to develop profiles. While the argument is that these tools could open up the\
    \ potential for some not served by traditional credit scoring systems to receive\
    \ credit, there are a range of concerns about how algorithmic scoring may discriminate.\
    \ For example, a consumer\u2019s purchase history could be used, intentionally\
    \ or unintentionally, as a proxy for ethnicity or religion. If an algorithmic\
    \ system ends up penalizing one group more than others it may be hard to figure\
    \ this out given the access issues, opacity and complexity of algorithmic processes.\
    \ While there are laws in place for people to review conventional credit scores,\
    \ there are not yet measures in place for people to interrogate new generated\
    \ scores.\nIn relation to all of these examples, researchers have raised concerns\
    \ about how new data driven processes reproduce illegal redlining practices. Historically,\
    \ redlining was used to discriminate against certain groups of people by denying\
    \ some groups access, or more expensive access, to housing or insurance. This\
    \ was often done by \u2018redlining\u2019 certain communities. The issue is that\
    \ where someone lives is often associated with ethnicity and class. In this way\
    \ location facilitates racism and inequality. Critics are concerned about how\
    \ new automated and predictive data tools can be used to \u2018redline\u2019 given\
    \ the amount of detail that can be determined about us through our data. Previous\
    \ research has demonstrated the potential to accurately determine our age, gender,\
    \ sexuality, ethnicity, religion and political views through the data that can\
    \ be collected and combined about us.\nRelatedly, groups are raising concerns\
    \ about how new data driven processes may facilitate \u2018reverse redlining\u2019\
    . This is when a particular group of people is targeted, as was done with sub-prime\
    \ mortgages. Newman argues that big data was central to the subprime financial\
    \ crash in 2007 as it played a key role in the manipulation of markets but also\
    \ in the subprime mortgage industry. Online advertising and data collected about\
    \ people online was used to direct and target them for sub-prime loans. In 2012\
    \ the American Department of Justice reached a settlement with the Wells Fargo\
    \ Bank concerning allegations that it had \u2018engaged in a pattern or practice\
    \ of discrimination against qualified African-American and Hispanic borrowers\
    \ in its mortgage lending from 2004 through 2009\u2019 by pushing these borrowers\
    \ into more costly sub-prime loans. In the settlement they agreed to provide $184\
    \ million in compensation.\nThe practice of targeting low-income groups continues\
    \ in the payday loan industry. A U.S. Senate Investigation reports that data brokers\
    \ have been found selling lists that focus on citizen financial vulnerability.\
    \ For example, data brokers have compiled the following lists to sell to those\
    \ interested in targeting such groups: \u2018Rural and Barely Making It\u2019\
    , \u2018Ethnic Second-City Strugglers\u2019, \u2018Retiring on Empty: Singles\u2019\
    , \u2018Tough Start: Young Single Parents\u2019. One company was found selling\
    \ a marketing tool to \u2018identify and more effectively market to under-banked\
    \ consumers\u2019.[10]\nAs argued by Madden et al., the fact that those with low-incomes\
    \ are less likely to take privacy protection measures when online and to also\
    \ rely more on their mobile phone for online access places them at greater risk\
    \ than others for online targeting and exploitation.[11] In fact, \u2018opting\
    \ out\u2019 of being tracked becomes increasingly difficult as technologies become\
    \ more sophisticated. New tools that make cross-device tracking possible or that\
    \ are embedded the Internet of Things, mean that the objects we use everyday make\
    \ more of our lives \u2018knowable\u2019 and trackable and make \u2018opting out\u2019\
    \ even harder.[12] Newman raises concerns about how in this age of datafication,\
    \ information inequality is transferred into economic inequality, as companies\
    \ have more information about citizens that can be used to target and exploit\
    \ them to their disadvantage.[13]\nCitron and Pasquale note that \u2018evidence\
    \ suggests that credit scoring does indeed have a negative disparate impact on\
    \ traditionally disadvantaged groups\u2019. They provide a number of examples\
    \ in their article, just one is the case of All-State which was challenged in\
    \ court and agreed to a multi-million dollar settlement over their scoring procedure\
    \ which plaintiffs argued \u2018resulted in discriminatory action against approximately\
    \ five million African-American and Hispanic customers\u2019.[14] They also raise\
    \ concerns about how scoring systems and predictive tools may actually create\
    \ the situations they claim to indicate and \u201Ctake a life\u201D of their own,\
    \ for example by labelling someone a poor candidate or unemployable.[15]\nIn 2015,\
    \ Christian Haigh, a Harvard undergraduate, discovered that the prices for The\
    \ Princeton Review\u2019s online SAT tutoring packages offered to high school\
    \ students varied depending on where customers live. Julia Angwin and Jeff Larson\
    \ of ProPublica investigated Haigh\u2019s findings and found that the highest\
    \ prices were being offered to ZIP codes with a large Asian population and high\
    \ median income. The Princeton Review said that the price difference was not intentional,\
    \ but as noted by ProPublica, the pricing algorithm clearly did discriminate.\
    \ Angwin and Larson note that it is significant that in the United States \u2018\
    unintentional racial discrimination is illegal in housing and employment under\
    \ the legal doctrine known as \u2018disparate impact\u2019 which prohibits inadvertent\
    \ actions that hurt people in a protected class\u2019. However this doctrine does\
    \ not extend to the online world, making it difficult in that country (and others)\
    \ to take legal action against \u2018adverse impact\u2019 caused by unintentional\
    \ algorithmic bias.\nIn 2012, a Wall Street Journal investigation found that Staples\
    \ Inc. website displayed \u2018different prices to people after estimating their\
    \ locations\u2019 and that in what appeared to be an \u2018unintended side effect\u2019\
    \ Staples tended to show discounted prices to areas with a higher average income\
    \ and higher prices to areas with lower average incomes.[16]\nA 2017 investigation\
    \ by ProPublica and Consumer Reports showed that minority neighborhoods pay more\
    \ for car insurance than white neighborhoods with the same risk levels. The study,\
    \ which compared premiums and payouts in California, Illinois, Texas and Missouri,\
    \ showed that minority neighborhoods paid \u2018as much as 30 percent more than\
    \ other areas with similar accident costs\u2019.\n\_\n\nhttp://a.msn.com/00/en-us/BBzv1Xg?ocid=scu2\n\
    \_\nIn 2015 Facebook suspended the accounts of Native Americans because its algorithm\
    \ did not recognize their names as real. [17] The \u201Creal name\u201D policy\
    \ left hundreds of native Americans with suspended accounts and they had to prove\
    \ their identity in order to use their accounts again. Dana Lone Hill was one\
    \ of the Native Americans who had to produce multiple ID documents to prove her\
    \ identity and have her profile reinstated. Her case generated a lot of media\
    \ attention and Facebook had to review its algorithm and eliminate the possibilities\
    \ for discrimination.\n\_\nRecognition technologies\nThere are numerous reports\
    \ of the biases embedded in facial recognition systems: they have problems identifying\
    \ people with darker skin and also with gender. Algorithms that are used to focus\
    \ smartphone cameras, for border security and advertisements sometimes cannot\
    \ identify, or misidentify, people of colour. It has been reported that the problem\
    \ is that the facial recognition algorithms used across various systems have been\
    \ trained using datasets that have mostly male white faces, that these algorithms\
    \ have not been exposed to enough diversity and that this problem is also connected\
    \ to the fact that many of these systems are being developed and tested largely\
    \ by white men. As argued by Joy Buolamwini, the issue of bias and inaccuracy\
    \ becomes increasingly important as facial recognition tools are adopted by police\
    \ and security systems.\n\_\n\n\n\n\_\nExamples of problems include the New Zealand\
    \ case where one man\u2019s passport photograph was rejected when a facial recognition\
    \ program mistakenly identified him as having closed eyes. People have posted\
    \ reviews online raising questions about the ability of Microsoft\u2019s Kinect\
    \ facial recognition feature to recognize people with darker skin and of HP\u2019\
    s tracking webcams \u2018to see Black people\u2019. Recent work by Buolamwini\
    \ and Gebru involved testing tools and found \u201Cdarker skinned females to be\
    \ the most misclassified group.\u201D\nOne of the biggest developers of facial\
    \ recognition software is Amazon and their tool \u2018Rekognition\u2019 has been\
    \ in the centre of debates about machine bias and racial discrimination in AI\
    \ technologies. Amazon\u2019s Rekognition has been designed to cross-reference\
    \ photos of unknown suspects and criminals against a database of mugshots from\
    \ jails in the country .[18] When the software was tested by the American Civil\
    \ Liberties Union of Northern California it was found that people of colour are\
    \ predominantly being misidentified from the mugshot database. The ACLU used photos\
    \ of Members of Congress and cross-referenced them with the mugshot database using\
    \ Amazon\u2019s facial recognition system: 28 members of Congress were misidentified\
    \ as people from the mugshot database and over 40% of them were from ethnic minorities.\
    \ People of colour make up only around 20% of Congress, but the misidentification\
    \ rate with them is two times higher than people of Caucasian origin.[19]\nThe\
    \ ACLU argues that due to concerns about the high risk of misidentification and\
    \ discrimination Amazon\u2019s software unfit and dangerous for use. The technology\
    \ has been sold to the American government and police forces.[20]\n\_\n\n\_\n\
    In August 2019 an American Federal court agreed with a group of Illinois Facebook\
    \ users that Facebook\u2019s use of face recognition technology on their photographs\
    \ without their knowledge or consent violated their privacy rights. The ACLU had\
    \ supported their case, arguing that \u2018performing a scan of an individual\u2019\
    s face without disclosing how that information will be stored, used, or destroyed,\
    \ and without properly obtaining written consent, creates an actionable privacy\
    \ harm. Notice and informed consent empower individuals to protect their privacy\
    \ and are central to privacy laws in the United States, generally, and to BIPA,\
    \ specifically.\u2019\nSasha Costanza-Chock\u2019s work details how \u201Cnorms,\
    \ values, assumptions \u2013 are encoded in and reproduced through the design\
    \ of sociotechnical data-driven systems.\u201D Their essay on the politics of\
    \ border security systems illustrates this as well as the harmful experience of\
    \ confronting the normative politics of these systems. The injustice and harm\
    \ caused by normative security systems has also been stressed by Shadi Petosky\
    \ as has the fact that there are alternatives that are being ignored.\n\_\n Eyeo\
    \ 2019 \u2013 Sasha Costanza-Chock from Eyeo Festival on Vimeo.\n\_\nDiscrimination\
    \ \u2013 gender and ethnicity\nA study of Google ads found that men and women\
    \ are being shown different job adverts, with men receiving ads for higher paying\
    \ jobs more often.[21] The study, which used a tool called AdFisher to set up\
    \ hundreds of simulated user profiles, was designed to investigate the operation\
    \ of Google\u2019s ad settings. Although researchers could determine that men\
    \ and women are being shown different ads, they could not determine why this is\
    \ happening. Doing so would require access to more information that would need\
    \ to be provided by advertisers about who they were targeting and by Google about\
    \ how their system works.\nFacebook allows advertisers to target people based\
    \ on race, ethnicity and gender. Another ProPublica investigation revealed that\
    \ third party companies can target ads to reach people by gender, ethnicity and\
    \ race and also to be hidden from people based on these kinds of classifications.\
    \ [22] A ProPublica investigation identified that Facebook\u2019s ads software\
    \ gives advertising companies the option to exclude men or women from their ad\
    \ demographic. Job positions for Uber and truck drivers, police officers and military\
    \ were all shown to predominantly male audiences, while job vacancies for nurses,\
    \ medical assistants and care-takers targeted women almost exclusively.\n\_\n\n\
    \_\nA complaint by the American Civil Liberties Union (ACLU) submitted to the\
    \ US Equal Opportunity Commission (EEOC) mentions three women from the states\
    \ of Ohio and Illinois who were not shown job advertisements for positions in\
    \ traditionally male-dominated fields which violates federal law of gender. [23]\
    \ The ACLU argued that by targeting only men in already predominantly male fields,\
    \ women are denied the opportunity to break into particular industries.\nSome\
    \ landlords have been found to selectively target different groups of users with\
    \ housing ads, excluding people from \u201Credlined\u201D neighbourhoods, and\
    \ inner-city areas with high rates of Black and Latino residents.[24] The housing\
    \ adverts run by some American landlords were devised to selectively target certain\
    \ communities and exclude people from Hispanic or African-American origin, or\
    \ citizens with bad credit scores . The ads that they publish are invisible for\
    \ the \u201Cexcluded\u201D people, as their ethnicity deems them an \u201Cundesirable\u201D\
    \ demographic. Discriminating based on sensitive factors such as ethnicity and\
    \ nationality in the United States is prohibited by The Fair Housing Act of 1968\
    \ and ads for housing that target based on those characteristics are in fact illegal,\
    \ but nonetheless existing .[25]\n\nDiscrimination \u2013 health\nCathy O\u2019\
    Neil has produced a great deal of work demonstrating how unfair and biased algorithmic\
    \ processes can be. In one example, she tells the story of Kyle Behm, a high achieving\
    \ university student who noticed that he was repeatedly not getting the minimum\
    \ wage jobs he was applying for. All of these job applications required him to\
    \ take personality tests which included questions about mental health. Although\
    \ healthy when looking for work, Behm did suffer from bipolar disorder and had\
    \ taken time out previously to get treatment. Behm\u2019s father is a lawyer and\
    \ he became suspicious of the fairness of these tests for hiring. He decided to\
    \ investigate and found that a lot of companies were using personality tests,\
    \ like the Kronos test. These tests are used as part of automated systems to sort\
    \ through applications and in this process decide which applicants proceed and\
    \ which are \u2018red-lighted\u2019 or discarded. As O\u2019Neil details, these\
    \ tests are often highly complex, with \u2018certain patterns of responses\u2019\
    \ disqualifying people. This example raises a number of ethical questions about\
    \ the use of health information in automated systems but also about the uses of\
    \ automated systems in hiring more generally, particularly as it is unlikely that\
    \ those who have been \u2018red-lighted\u2019 will ever know they were subject\
    \ to an automated system. O\u2019Neil argues that the increasing use of automated\
    \ systems to sort and whittle down job applications creates more unfairness as\
    \ those who know or can pay for help to ensure their applications get to the top\
    \ of the pile have an advantage.\n\_\nLoss of privacy\nThis can happen unintentionally\
    \ when attempts to release data anonymously do not work. Big data makes anonymity\
    \ difficult because it is possible to re-identify data that has been anonymized\
    \ by combining multiple data points.\n\_\nPlatforms\nAs detailed by Paul Ohm,\
    \ in 2006 America Online (AOL) launched \u2018AOL Research\u2019 to \u2018embrace\
    \ the vision of an open research community\u2019. The initiative involved publicly\
    \ releasing twenty million search queries from 650,000 users of AOL\u2019s search\
    \ engine. The data, which represented three months of activity, was posted to\
    \ a public website. Although the data was anonymized, once the data was posted\
    \ some users demonstrated that it was possible to identify people\u2019s identities\
    \ using the data which included name, age and address.\nTwo New York Times reporters\
    \ Michael Barbaro and Tom Zeller Jr. cross-linked data to identify Thelma Arnold,\
    \ a sixty-two year old widow from Lilburn Georgia. Her case demonstrates the problems\
    \ with \u2018anonymisation\u2019 in an age of big data, but also the danger in\
    \ reading too much into search queries. As Barbaro and Zeller note, Ms Arnold\u2019\
    s search queries \u2018hand tremors\u2019, \u2018nicotine effects on the body\u2019\
    , \u2018dry mouth\u2019 and \u2018bipolar\u2019, could lead someone to think she\
    \ suffered from a range of health issues. Such a conclusion could have negative\
    \ effects if the organization making that conclusion was her insurance provider.\
    \ In fact, when they interviewed Arnold, Barbaro and Zeller found that Arnold\
    \ often does searches for her friends because she wants to help them.\nIn 2006\
    \ Netflix publicly released one hundred million records detailing the film ratings\
    \ of 500,000 of its users between Dec. 1999 and Dec. 2005. As Ohm reports, the\
    \ objective was to launch a competition and for those competing to use this data\
    \ to improve Netflix\u2019s recommendation algorithm.[26] Netflix anonymized the\
    \ data by assigning users a unique identifier. Researchers from the University\
    \ of Texas demonstrated not long after this release how relatively easy it was\
    \ for people to be re-identified with the data.[27] This led to a court case in\
    \ which Jane Doe argued that the data could be used to out her sexuality.[28]\
    \ Jane Doe argued that her homosexuality was being revealed by the data as it\
    \ revealed her interest in gay and lesbian themed films. She argued the data outed\
    \ her, a lesbian mother, against her wishes and could damage herself and her family.\
    \ The court case was covered by Wired in 2009.\n\_\nFitness Trackers\nSome employers\
    \ are gaining highly personal information about their employees through their\
    \ use of fitness trackers. Employers now collect health and biometric data about\
    \ their employees through the use of wearable tech and health tracking apps.[29]\
    \ Through performance monitoring employers can now collect regular reports about\
    \ staff activity. Concerns are being raised about companies that are encouraging\
    \ their female employees to use family planning apps that give employers and other\
    \ corporate entities access to details about their employee\u2019s private lives,\
    \ health, hopes and fears.[30]\nFitness apps and trackers allow its users to monitor\
    \ their calorie intake, physical activity and vital signals such as heart rate\
    \ and blood pressure. In 2018 a data leak involving MyFitnessPal exposed the accounts\
    \ of 115 million users after a security breach in their systems.[31] The user\
    \ names, email addresses and scrambled passwords to user accounts were stolen\
    \ from the parent sportswear company Under Armour.[32] Access to the fitness accounts\
    \ means access to vital information that can be used to track individuals, view\
    \ their location in live time, predict behaviour and activities or share sensitive\
    \ health information with third party organisations such as private health clinics,\
    \ insurance companies and even employers [33]\nIn 2018, Strava revealed that fitness\
    \ app data can reveal highly sensitive location information. Heat map visualisations\
    \ released by Strava showed activity captured by the app, lighting up different\
    \ user routes. This mapping involved more than 3 trillion GPS data points.[34]\
    \ A problem is that this app is used by military personnel and by releasing these\
    \ \u2018anonymous\u2019 heat maps Strava was revealing the location of military\
    \ activity. While the information on the heatmaps is an aggregate of all the user\
    \ activities, the Strava website allows the user to track running routes in detail\
    \ and eventually connect them to usernames and the individuals behind them, which\
    \ could endanger military personnel on missions overseas.[35]\nAnother fitness\
    \ tracking app \u2013 Polar Flow, also exposed the geolocation of its users through\
    \ a tool called \u201CExplore map\u201D. An investigation by De Correspondent\
    \ and Bellingcat revealed that the app makes it possible to explore sensitive\
    \ locations and locate individual users and their exercise routines. It turned\
    \ out that tracking an individual behind a username has been made readily available\
    \ and fairly easy and the names and addresses of personnel from intelligence agencies\
    \ such as the NSA, the US Secret Services and the MI6 could be uncovered.\n\_\n\
    \n\_\nSmart microchips\nSome tech companies have found even more intrusive ways\
    \ to monitor their employees. They have started using smart microchips that can\
    \ be implanted under the skin- the same technology that the US justice department\
    \ uses on prisoners on probation instead of ankle braces. The chips work similar\
    \ to the access key cards a lot of firms have, except these key cards do not track\
    \ the workers\u2019 physical and physiological condition. A Chinese mining company\
    \ has even introduced helmets that could read the brainwaves of workers and distinguish\
    \ feelings such as fatigue, distraction and even anger [36]. Earlier last year,\
    \ the American company Three Square Market began putting microchips the size of\
    \ a rice grain in their workers\u2019 hands- from 6-7 chipped employees initially,\
    \ the process expanded to include more the 100 with plans to implant chips on\
    \ all its 10 000 employees.\nThe chips can be used instead of physical IDs to\
    \ open doors, log into computers, pay at the vending machines on site, and can\
    \ be paired with the GPS on employers\u2019 smartphones to show the exact location\
    \ of every employee at any point throughout the day.[37]\nBut unlike fitness tracking\
    \ devices and key cards that could be taken off at the end of the work day, chips\
    \ are worn constantly- giving the employer infinite possibilities to track its\
    \ employees. This invasion of personal space has expanded substantially. BioTeq,\
    \ a UK- based firm is one of the new businesses that offer implants to companies\
    \ and individuals and has implanted more than 150 chips in various firms across\
    \ the UK.[38] Experts and researchers warn that microchip implanting can hide\
    \ a lot of dangers, especially for employees, as it can completely erode their\
    \ right to privacy.\n\_\nSmart devices- smart spies \nAs of 2019, over 100 million\
    \ Amazon Echos have been sold. Amazon employs thousands of people in its headquarters\
    \ to listen to what Alexa has recorded about users.[39] The recordings are transcribed\
    \ and it is argued used to eliminate gaps in Alexa\u2019s communication and introduce\
    \ new accents and words. Together with the conversations that people have had\
    \ with their Alexa devices, the devices pick up other audio in the home. Access\
    \ to the device has been requested by judges in court proceedings. There is increasing\
    \ concern about how the use of such devices can compromise privacy.\n\nConcerns\
    \ are also being raised about Smart TVs.[40] In 2017 Wikileaks published documents\
    \ suggesting a CIA operation: Weeping Angel, which allegedly involved the use\
    \ of smart TV microphones for mass surveillance.[41] Consumer Reports also compiled\
    \ information and advice for the owners of smart TVs about to avoid tracking.\n\
    \nChildren\nCompanies have been facing increasing complaints for collecting data\
    \ about children and sharing or using this to target them with advertisements\
    \ or more content designed to keep them online for longer. In 2019, the American\
    \ Federal Trade Commission ordered Google, including its subsidiary YouTube, to\
    \ pay a record $170 million to settle allegations that YouTube had illegally collected\
    \ data about children without their parent\u2019s consent.\nConcerns are also\
    \ being raised about YouTube\u2019s site for children and the YouTube algorithm,\
    \ which privileges the kind of sensational content that keeps children online\
    \ for longer. Concerns have been raised about the algorthm through privileging\
    \ content like this leading to the promotion of violent and suggestive content.\n\
    The Children\u2019s Commissioner for England published a report highlighting the\
    \ range of data that is being collected about children. The report draws attention\
    \ to the fact that while there is increasing attention to the need to be alert\
    \ to privacy infringements related to online platforms, there is also a need to\
    \ consider the privacy issues and potentials for harms introduced by smart internet\
    \ connected devices, like monitors and toys. Consumer groups have called for some\
    \ smart toys to be re-called after learning that these toys could be hacked to\
    \ enable strangers to talk to children and parents reported that their baby monitors\
    \ were hacked.\n\_\nIdentity theft, blackmail, reputation damage, distress\n\_\
    \nData breaches\nAlthough data breaches are listed under corporate uses of data,\
    \ they could also be listed here under government uses of data as breaches have\
    \ happened in both sectors. Solove and Citron argue that \u2018harm\u2019 in relation\
    \ to data breaches relates to \u2018a risk of future injury, such as identity\
    \ theft, fraud, or damaged reputations\u2019 and also to a current injury as people\
    \ experience anxiety about this future risk. They note that the anxiety and emotional\
    \ distress created about future risk is a harm that people experience \u2018in\
    \ the here and now\u2019. Identity theft is a major problem, particularly for\
    \ those of low-income who lack the resources to pay for legal representation and\
    \ challenge mistakes due to identity fraud. Further, the sudden loss of income\
    \ or errors that result from identity fraud can be disastrous for those living\
    \ from pay cheque to pay cheque. Sarah Dranoff notes that in addition to financial\
    \ loss, identity theft can lead to \u2018wrongful arrests, loss of utility service,\
    \ erroneous information on health records, improper child support garnishments,\
    \ and harassment by collection agencies\u2019.[42] A number of data breach examples\
    \ are detailed by Solove and Citron: 1) The Office of Policy Management breach\
    \ leaked people\u2019s fingerprints, background check information, and analysis\
    \ of security risks, 2) The Ashley Madison breach released information about people\u2019\
    s extramarital affairs, 3) The Target breach resulted in leaking credit card information,\
    \ bank account numbers and other financial data and 4) the Sony breach involved\
    \ employee email.\nThis regularly updated visualization by the Information is\
    \ Beautiful team demonstrates how common major data breaches are:\n\_\nhttps://www.informationisbeautiful.net/visualizations/worlds-biggest-data-breaches-hacks/\n\
    \_\n\nCommercial Data Breaches\nDating sites\nThe Ashley Madison data hack was\
    \ one of the biggest data in online dating site history. In 2015 the personal\
    \ data of more than 37 million users of the site was stolen.[43] Personal details\
    \ of site users was posted by an online hacking group called the Impact Team.[44]\
    \ The sites parent company Avid Live Media (ALM) faced a class action in US court.\
    \ As a result the corporation had to pay nearly quarter of its revenue \u2013\
    \ $11.2 million in settlement.[45] The hack also led to reputational damage, high-profile\
    \ resignations from site users whose names were exposed, divorce filings and two\
    \ suicides of former employees of the company.[46]\nAnother online dating site,\
    \ Adult FriendFinder, was hacked in 2015 and the highly personal data of almost\
    \ 4 million users was leaked online.[47] Just hours after the data was posted\
    \ on a dark web forum, the victims of the hack received spam and threatening emails\
    \ to expose their private information. A year later, the site suffered a second\
    \ hack, this time exposing the information from 412 million accounts.[48]\n\_\n\
    Security companies \nIn August 2019 the Guardian reported that researchers had\
    \ discovered that Suprema\u2019s Biostar 2 database was \u2018unprotected and\
    \ mostly unencrypted\u2019. The researchers said they had access to millions of\
    \ personal records which included fingerprint and facial recognition data and\
    \ usernames and passwords. The system is used by government agencies, defence\
    \ contractors and banks.\n\_\nGovernment Database Breaches \nSwedish Government\
    \ Database \nIn 2016 the Swedish government suffered a massive data breach which\
    \ endangered the identities of undercover operatives. The data breach originated\
    \ from the Swedish transport Agency which exposed the personal information of\
    \ millions of Swedish citizens and the identities of some military personnel.\
    \ The agency previously had contracted a deal with an outsourcing company \u2013\
    \ IBM, and the mishandling of data between the government agency and the private\
    \ company led to a massive leak of sensitive information.[49] The information\
    \ also exposed sensitive information about bridges, roads, ports, subway systems\
    \ in the capital and other key infrastructures. The exposure of sensitive information\
    \ in this case was the result of an absence of proper safeguards and protective\
    \ measures between the government agency and IBM.[50]\nBritish Government Database\n\
    In 2019 the Guardian reported that the fingerprints of \u201Cover 1 million people,\
    \ as well as facial recognition information, unencrypted usernames and passwords,\
    \ and personal information of employees, was discovered on a publicly accessible\
    \ database for a company used by the likes of the UK Metropolitan police, defence\
    \ contractors and banks.\u201D The company involved was called Suprema and the\
    \ breach involved their web-based Biostar 2 biometrics lock system.\nIndia\u2019\
    s Aadhaar Data Breach \nIndia\u2019s ID System Aadhaar has also suffered a data\
    \ breach that exposed the identities of more than a billion people online. In\
    \ 2018 Excel files and documents containing the names, addresses and phone numbers\
    \ of Aadhaar holders were erroneously leaked by various government websites, compromising\
    \ data and giving unauthorised access to personal information of Aadhaar ID\u2019\
    s.[51] A Tribune investigation revealed that the personal and biometric information\
    \ of more than a billion Indian citizens was being sold online for as little as\
    \ 500 rupees or \xA36. Authorities denied the allegations and said that the leaked\
    \ demographic data cannot be misused without biometric information, which was\
    \ kept safe and protected .[52]\n\_\nPhysical injury\nEsther Kaplan\u2019s investigation\
    \ into the effects of workplace data monitoring revealed how the monitoring of\
    \ employees in order to increase their productivity is leading to physical injury\
    \ in some cases. She interviewed a UPS worker who noted that the physical demands\
    \ of his job have increased since the company introduced a telematics system.\
    \ The system monitors employees in real time through tracking devices that include\
    \ \u2018delivery information acquisition devices\u2019 and sensors on delivery\
    \ trucks. The pressure to do more work in less time is leading to injury as drivers\
    \ do not have the time to lift and carry packages properly.[53]\n\_\nPolitical\
    \ uses of Data\nPolitical Manipulation and social harm\nThe damage that can be\
    \ done by fake news, bots and filter bubbles have generated much discussion recently.\
    \ Uses of automated and algorithmic processes in these cases can lead to social\
    \ and political harm as the information that informs citizens is manipulated,\
    \ potentially leading to misinformation and undermining democratic and political\
    \ processes as well as social well-being. A recent study by researchers at the\
    \ Oxford Internet Institute details the diverse ways that people are trying to\
    \ use social media to manipulate public opinion across nine countries. They note\
    \ that this is a concern given the increasing role that social media plays as\
    \ a key information source for citizens, particularly young people. Further, that\
    \ social media are fundamental in many countries to the sharing of political information.\
    \ Civil society groups are \u2018trying, but struggling, to protect themselves\
    \ and respond to active misinformation campaigns\u2019.\nWoolley and Howard define\
    \ computational propaganda as involving \u2018learning from and mimicking real\
    \ people so as to manipulate public opinion across a diverse range of platforms\
    \ and device networks\u2019. Bots, automated programs, are used to spread computational\
    \ propaganda. While bots can be used for legitimate functions, the Internet Institute\
    \ study details how bots can be used to spam, harass, silence opponents, \u2018\
    give the illusion of large-scale consensus\u2019, sway votes, defame critics,\
    \ and spread disinformation campaigns. The authors argue that \u2018computational\
    \ propaganda is one of the most powerful new tools against democracy\u2019.\n\
    Facebook- Cambridge- Analytica Scandal \nIn 2018, through the reporting of Carole\
    \ Cadwalladr, we learned about how Facebook was implicated in political manipulation\
    \ on a grand scale through its involvement with Cambridge Analytica and others.\
    \ Whistleblower Christopher Wylie revealed how the company used the data of more\
    \ than 80 million people to build a profiling system used for political advertising.[54]\
    \ The company allegedly used the psychological profiles for what a CA intern has\
    \ called \u201CPsyops\u201D- psychological operations that, much like in the military,\
    \ are used to affect and change opinion. The use of \u2018dark ads\u2019 on Facebook\
    \ have been linked to Brexit and Trump\u2019s election campaign in the United\
    \ States.\n\_\n\n\n\n\_\nBut the Cambridge Analytica scandal was not only a data\
    \ leak crisis; in fact, it can be argued that it was not a data breach at all,\
    \ as Facebook is designed for this- to collect data, analyze and exploit it.\n\
    \_\nGovernment uses of Data\nExclusion and Error\nBig data blacklisting and watch-lists\
    \ in the U.S. have wrongfully identified individuals. As detailed by Margaret\
    \ Hu, being wrongfully identified in this case can negatively affect employment,\
    \ ability to travel, and in some cases lead to wrongful detention and deportation.[55]\n\
    Hu details the problems with the American E-Verify programme, which \u2018attempts\
    \ to \u201Cverify\u201D the identity or citizenship of a worker based upon complex\
    \ statistical algorithms and multiple databases\u2019. Employers across states\
    \ use the programme to determine if a person is legally able to work in the U.S.\
    \ Hu writes that it appears that employers have wrongfully denied employment for\
    \ thousands. Hu argues that e-verify is problematic due to the unreliability of\
    \ the data that informs the database screening protocol. The problems with the\
    \ e-verify programme have also been detailed by Upturn. A study by the American\
    \ Civil Liberties Union demonstrates that errors are far more likely to affect\
    \ foreign-born employees and citizens with foreign names. People with multiple\
    \ surnames and women who change their names after marriage are also more likely\
    \ to face errors. Harm is further exacerbated by the difficulty in challenging\
    \ or correcting e-verify errors. As discussed by Alex Rosenblat and others: \u2018\
    [L]ow-wage, hourly workers, whether they are flagged for a spelling error or for\
    \ other reasons, often lack the time, resources, or legal literacy required to\
    \ navigate complex bureaucracies to correct misinformation about them in a national\
    \ database\u2019.\nHu also raises concerns about The Prioritised Enforcement Programme\
    \ (PEP), formerly the Secure Communities Programme (S-COMM). This is a data-sharing\
    \ programme between the Federal Bureau of Investigation (FBI), DHS and local law\
    \ enforcement agencies that requires local agencies to run fingerprints taken\
    \ from suspects against federal fingerprint databases (ibid: 1770). The programme\
    \ has made errors. For example, inaccurate database screening results wrongfully\
    \ targeted 5,880 US citizens for potential detention and deportation, leading\
    \ critics to question the reliability of PEP/S-COMM\u2019s algorithms and data.\
    \ Furthermore, by using the biometric data of arrestees contained in the S-COMM\
    \ databases the Immigration and Customs Enforcement (ICE) reportedly may have\
    \ wrongly apprehended approximately 3,600 US citizens, due to faulty information\
    \ feeding database screening protocols. As Hu points out, \u2018error-prone\u2019\
    \ databases and screening protocols \u2018appear to facilitate the unlawful detention\
    \ and deportation of US citizens\u2019.\nHu argues that the big data systems underlying\
    \ both E-Verify and S-COMM/PEP are causing harm by mistakenly targeting and assigning\
    \ inferential guilt to individuals. Legally speaking, this kind of digitally generated\
    \ suspicion is at odds with constitutional rights and there is a growing consensus,\
    \ at least in the U.S, on the need for substantive and binding due process when\
    \ it comes to big data governance.\nIn Arkansas, U.S., the government introduced\
    \ an algorithm to determine how many hours of home care people were entitled to.\
    \ This was something that was previously done by home care nurses. The change\
    \ meant that home care nurses were now required to help people fill in a questionnaire\
    \ with 260 questions. The responses to the questionnaire were then processed by\
    \ an algorithmic system which then determined how many home care hours people\
    \ were entitled to. The result for many was a major reduction in home care hours,\
    \ which drastically limited people\u2019s quality of life and in some cases their\
    \ ability to stay in their own homes. As with other examples listed in this record,\
    \ finding out information about how the algorithm worked proved very difficult.\n\
    Seven of those affected took the government to court with the help of Legal Aid\
    \ of Arkansas. Six of those involved in this case had their home care hours reduced\
    \ by more than 30 percent. There have been ongoing challenges to the use of this\
    \ algorithm and its effects.\n\_\n\n\_\nA similar situation has occurred in Idaho\
    \ where the government started using a data system to determine home care costs\
    \ which led to beneficiaries seeing their funds drastically reduced. Only after\
    \ an ACLU lawsuit did it become clear how limited the data being used was and\
    \ the need for system change.\nA study published in Science magazine in 2019 has\
    \ found that an algorithmic system used to identify follow up health care needs\
    \ of patients across the United States is biased against Black patients \u2013\
    \ the system dramatically underestimates the amount of care Black patients need\
    \ as compared to white patients.\n\_\n\n\_\nConcerns are being raised in the United\
    \ States about how data matching systems are being used as part of a wider strategy\
    \ to disenfranchise African American and Latino voters. In one highly publicized\
    \ example, data matching requirements in Georgia, have been linked to voter suppression\
    \ by civil rights activists and the democratic nominee. According to the \u201C\
    exact match\u201D legislation, the system that processes the voter registration\
    \ applications would only count the votes of the people with the same name or\
    \ address spelling on all documents as legitimate. The changes were introduced\
    \ by Brian Kemp\u2019s office, who at that time was Georgia\u2019s Secretary of\
    \ State and the Republican candidate in the governor\u2019s race. The new regulations\
    \ resulted in 53 000 voter applications being put on hold, as a result of \u201C\
    misspelling of names.\u201D The data inconsistencies and application suspensions\
    \ mostly affected people with foreign names, people with more than one surname,\
    \ those from minority groups and people who have recently changed their surnames\
    \ (newly married women) or have a new address.\n\_\n\n\_\nIn Australia in 2019,\
    \ after years of activism and advocacy, the federal government conceded that the\
    \ automated debt recovery system it had introduced was flawed. Government communications\
    \ suggested that anywhere from 600,000 to 900,000 \u201Crobo-debts\u201D that\
    \ had been issued to people to repay would need to be reassessed. The program\
    \ had at this point already been investigated by the Ombudsman and Senate after\
    \ numerous complaints of errors and unfair targeting of vulnerable people. The\
    \ system uses data matching and income averaging to determine if people have been\
    \ overpaid benefits. Onus was placed on those receiving letters to prove an error\
    \ had been made.\nNumerous accounts of errors were published in the press and\
    \ calls for investigation were taken up by opposition politicians. One case involved\
    \ a man who was repeatedly sent letters saying he owed the government repayment\
    \ of $4,000. This turned out to be an error. The man, who suffers from depression\
    \ and became suicidal, said he successfully convinced the government this was\
    \ an error only to receive a similar letter a few months later. He again successfully\
    \ proved this was an error. One of the ombudsman\u2019s conclusions was that better\
    \ project planning and risk management should have been done from the outset.\n\
    Cassandra Goldie, Chief Executive of the Australian Council of Social Service,\
    \ was quoted in the Guardian as saying:\n[R]obo-debt has issued thousands of debt\
    \ notices in error to parents, people with disabilities, carers and those seeking\
    \ paid work, resulting in people slapped with Centrelink debts they do not owe\
    \ or debts higher than what they owe \u2026 It has been a devastating abuse of\
    \ government power that has caused extensive harm, particularly among people who\
    \ are the most vulnerable in our community.\n\_\n\n\_\nIn October 2019, Virginia\
    \ Eubanks reported a similar practice happening in the United States. In this\
    \ case it is being reported that Government working with tech companies are\_\
    sending out debt notices to thousands of vulnerable people across the country\
    \ that allege\_people have been overpaid benefits. When people have received these\
    \ letters they have few\_options, particularly as challenging the details in the\
    \ letter may require finding pay stubs or\_other documents that are decades old.\
    \ These debts are being called zombie debts because\_of the devastating impact\
    \ they are having on the families forced to repay them, who have\_little ability\
    \ to challenge them. This is despite the fact, that much like the Australia robo-debt\_\
    scandal, people are finding error and \u2018miscalculation\u2019 in these notices.\n\
    \nSocial Exclusion \nSocial exclusion can be perpetuated by many factors including\
    \ identification systems. In a number of countries ethnic groups are being routinely\
    \ excluded and labelled as different through the use of national IDs. Privacy\
    \ International research of national identification systems raises concerns about\
    \ how ID systems can be used in ways that can lead to intentional and unintentional\
    \ exclusion. Such exclusion can lead to great harm by affecting people\u2019s\
    \ survival as ID cards are linked to the ability to access food, fuel, work and\
    \ education.\nIn India, data errors linked to the world\u2019s biggest biometric\
    \ identification system-Aadhaar are being linked to deaths due to starvation as\
    \ people, through data system errors, are being left without access to food and\
    \ other life essentials. In some cases this can be because of data system errors\
    \ such as ID\u2019s not being matched to the right person or people\u2019s finger\
    \ prints not registering. Aadhaar, India\u2019s identification database, contains\
    \ the names, addresses, phone numbers and biometrical specifics (fingerprints,\
    \ palm veins and print, face and iris recognition, DNA, hand geometry, retina)\
    \ of 80% of India\u2019s population.[56] The Aadhaar ID system started as a completely\
    \ voluntary ID card system run by the government on the private servers of HCL,\
    \ but quickly became a vital aspect of identification in India and more and more\
    \ government services have made the use of Aadhaar mandatory. As of 2019 access\
    \ to fuel, food, financial subsidies, health services, job positions and school\
    \ scholarships is open almost exclusively to Aadhaar number holders.\nOther examples\
    \ of data failure include attempts to automate welfare services in the U.S. Virginia\
    \ Eubanks details the system failures that devastated the lives of many in Indiana,\
    \ Florida and Texas at great cost to taxpayers. The automated system errors led\
    \ to people losing access to their Medicaid, food stamps and benefits. The changes\
    \ made to the system led to crisis, hospitalization and as Eubanks reports, death.\
    \ These states cancelled their contracts and were then sued.\n\_\n\n\_\nBig data\
    \ applications used by governments rely on combining multiple data sets. As noted\
    \ by Logan and Ferguson, \u2018small data (i.e. individual level discrete data\
    \ points) \u2026 provides the building blocks for all data-driven systems\u2019\
    . The accuracy of big data applications will be affected by the accuracy of small\
    \ data. We already know there are issues with government data, just two examples:\
    \ 1) in the United States, in 2011 the Los Angeles Times reported that nearly\
    \ 1500 people were unlawfully arrested in the previous five years due to invalid\
    \ warrants and 2) in New York, a Legal Action Center study of rap sheet records\
    \ \u2018found that sixty-two percent contained at least one significant error\
    \ and that thirty-two percent contained multiple errors\u2019. [57]\n\_\nHarms\
    \ due to algorithm / machine bias\nResearch into predictive policing and predictive\
    \ sentencing shows the potential to over-monitor and criminalize marginalized\
    \ communities and the poor.[58]\nJournalists working with ProPublica are investigating\
    \ algorithmic injustice. Their article titled \u2018Machine Bias\u2019 in particular,\
    \ has received a great deal of attention. Julia Angwin, Jeff Larson, Surya Mattu\
    \ and Lauren Kirchner\u2019s investigation was a response to concerns being raised\
    \ by various communities about judicial processes of risk assessment. These processes\
    \ of risk assessment involved computer programs that produce scores predicting\
    \ the likelihood that people charged with crimes would commit future crimes. These\
    \ scores are being integrated throughout the US criminal justice system and influencing\
    \ decisions about bond amounts and sentencing. The ProPublica journalists looked\
    \ at the risk scores assigned to 7,000 people and checked to see how many were\
    \ charged with new crimes. They found that the scores were \u2018remarkably unreliable\
    \ in forecasting violent crime\u2019. They found that only 61%, just over half,\
    \ of those predicted to commit future crimes did. But the big issue is bias. They\
    \ found that the system was much more likely to flag Black defendants as future\
    \ criminals, wrongly labelling them as future criminals at twice the rate as white\
    \ defendants. White people were also wrongly labelled as low risk more often than\
    \ Black defendants. The challenge is that these risk scores and the algorithm\
    \ that determines them is produced by a for profit company, so researchers were\
    \ not able to interrogate the algorithm only the outcomes. ProPublica reports\
    \ that the software is one of the most widely used tools in the country.\nKristian\
    \ Lum and William Isaac, of the Human Rights Data Analysis Group, published an\
    \ article detailing bias in predictive policing. They note that because predictive\
    \ policing tools rely on historical data, predictive policing should be understood\
    \ as predicting where police are likely to make arrests and not necessarily where\
    \ crime is happening. As noted by Lum and Isaac, as well as by O\u2019Neil, if\
    \ nuisance crimes like vagrancy are added to these models this further complicates\
    \ matters and there is an over policing of poor communities, more arrests, and\
    \ you have a feedback loop of injustice. Lum and Isaac used a range of data sources\
    \ to produce an estimate of illicit drug use from non-criminal justice, population\
    \ based data sources which they then compared to police records. They found that\
    \ while drug arrests tend to happen in areas with more BIPOC and low income communities,\
    \ drug use is fairly evenly distributed across all communities. Using one of the\
    \ most popular predictive policing tools, they find that the tool targets Black\
    \ people twice as much as whites even though their data on drug use shows that\
    \ drug use is roughly equivalent across racial classifications. Similarly they\
    \ find that low income households are targeted by police at much higher rates\
    \ than higher income households.\nO\u2019Neil describes how crime prediction software,\
    \ as used by the police in Pennsylvania leads to a biased feedback loop. In this\
    \ case the police include nuisance crimes, such as vagrancy, in their prediction\
    \ model. The inclusion of nuisance crimes, or so-called antisocial behaviour,\
    \ in a model that predicts where future crimes will occur distorts the analysis\
    \ and \u2018creates a pernicious feedback loop\u2019 by drawing more police into\
    \ the areas where there is likely to be vagrancy. This leads to more punishment\
    \ and recorded crimes in these areas, poor areas where there is likely to be vagrancy.\
    \ O\u2019Neil draws attention to specific examples of problems: Pennsylvania police\
    \ use of PredPol, the NYCPD use of CompStat and the Philadelphia police use of\
    \ Hunchlab.[59]\nAmnesty International also carried out an investigation of predictive\
    \ policing algorithms. They published a detailed report about the Gang Matrix\
    \ \u2013 the London Metropolitan Police database- and the implications it has\
    \ on marginalized communities. The Gang Matrix contains information about individuals\
    \ who are suspected gang members in the city of London. Created as a risk-management\
    \ tool after the riots in London in 2011, the database has proven inefficient\
    \ and been criticized as discriminating against young Black men often based on\
    \ nothing more substantial than their cultural preferences. The database has over\
    \ 3800 suspects and gathers intelligence about them from various sources online,\
    \ using data such as the websites the individuals visit, the songs they stream,\
    \ the content they watch on YouTube and more sensitive data such as ethnicity\
    \ and nationality. In 2018 the mayor of London Sadiq Khan commissioned a Review\
    \ of the Metropolitan Police Service Gang Matrix and, according to the paper,\
    \ there is a disproportionate number of Black men included in the list .[60] 78%\
    \ of the individuals on the list are young Black men aged under 25 and altogether\
    \ 80% of all the suspects on the list are Black. In reality, however, only 27%\
    \ of the people actually responsible for gang crime are Black.\n\_\n\n\_\nThe\
    \ highly controversial database perpetuates racial profiling and unjust prosecution\
    \ of people who have not committed any serious offences and can have serious repercussions\
    \ for the individuals, who are being routinely marginalized.[61] The information\
    \ on the database is being shared with jobcentre and housing workers, head teachers\
    \ and school principals and representatives from local hospitals.\nGang labelling\
    \ can not only affect the individuals listed, but their families as well. It also\
    \ can prevent young people from moving on with their lives. In 2012, the Metropolitan\
    \ Police threatened to evict the family of a young Black man that was suspected\
    \ for gang activity. The mother of the young man pursuing education at Cambridge\
    \ University, received a threatening letter from the MPS that the family was going\
    \ to lose their home, because their son was involved in gang activities. Although\
    \ this young man was not associated with the area where he used to live, the \u201C\
    gangster\u201D label continued to follow him even after he tried to move on.[62]\
    \ In 2013 another young Black man was expelled from college, after the college\
    \ authorities found that he had been listed in the Matrix.[63] In another case,\
    \ Paul, a 21 year old graduate was denied the position because his name was still\
    \ on the matrix for an offence committed when he was 12 years old.[64]\n\_\nHow\
    \ can harms be prevented?\nUltimately the goal of this Data Harm Record is to\
    \ stimulate more debate and critical interrogation of how automated and predictive\
    \ data systems are being used across sectors and areas of life.\nThe goal is to\
    \ maintain the Data Harm Record as a running record. Please let us know of any\
    \ cases you think we should add by sending a message here.\nIt is hoped that this\
    \ work contributes to the work of others in this area, many referenced in this\
    \ page, who are trying help us gain a better appreciation of: a) how uses of automated\
    \ and predictive systems are affecting people, b) the kind of datafied world we\
    \ are creating and experiencing, c) the fact that datafication practices affect\
    \ people differently, d) how datafication is political and may lead to practices\
    \ that intentionally or unintentionally discriminate, be unfair, and increase\
    \ inequality and e) how to challenge and redress data harms.\nThere are a range\
    \ of individuals and groups coming together to develop ideas about how data harms\
    \ can be prevented.[65] Researchers, civil society organizations, government bodies\
    \ and activists have all, in different ways, identified the need for greater transparency,\
    \ accountability, systems of oversight and due process, and the means for citizens\
    \ to interrogate and intervene in the datafied processes that affect them. It\
    \ is hoped that this record demonstrates the urgent need for more public debate\
    \ and attention to developing systems of transparency, accountability, oversight\
    \ and citizen intervention.\nFor example, O\u2019Neil argues that auditing should\
    \ be done across the stages of data projects and include auditing: the integrity\
    \ of the data; the terms being used; definitions of success; the accuracy of models;\
    \ who the models fail; the long-term effects of the algorithms being used; and\
    \ the feedback loops created through new big data applications. The Our Data Bodies\
    \ team is based in marginalized communities and interrogating data practices from\
    \ a human rights perspective. We at the Data Justice Lab are working on another\
    \ project, Towards Democratic Auditing, to investigate how to increase citizen\
    \ participation and intervention where these systems are being implemented. AI\
    \ Now, note the need for greater involvement with civil society groups, particularly\
    \ groups advocating for social justice who have long-standing experience identifying\
    \ and challenging the biases embedded in social systems. Researchers at AI Now\
    \ have argued that government uses of automated and artificial intelligence systems\
    \ in the delivery of core services in criminal justice, healthcare, welfare and\
    \ education should stop until the risks and harms can be fully assessed and we\
    \ can decide on where, given the risks involved, there should be no go areas for\
    \ uses of automated systems because the risks are too great.\n\_\n\_\nNotes\n\
    [1] For example see: a) www.datakind.org, b) Gangadharan, SP (2013) \u2018How\
    \ can big data be used for social good\u2019, Guardian, 30 May, available: https://www.theguardian.com/sustainable-business/how-can-big-data-social-good,\
    \ c) Raghupathi, W and Raghupathi, V (2014) \u2018Big data analytics in healthcare:\
    \ promise and potential\u2019 Health Information Science and Systems 2(3), available:\
    \ https://www.ncbi.nlm.nih.gov/pmc/articles/\nPMC4341817/ d) Mayer-Sch\xF6nberger,\
    \ Viktor and Cukier, Kenneth. 2013. Big Data: A Revolution That Will Transform\
    \ How We Live, Work, and Think. New York: Houghton Mifflin Harcourt, e) Manyika,\
    \ James, Chui, Michael, Brown, Brad, Bughin, Jacques, Dobbs, Richard, Roxburgh,\
    \ Charles and Hung Byers, Angela. 2011. \u201CBig Data: The Next Frontier for\
    \ Innovation, Competition, and Productivity.\u201D McKinsey Global Institute,\
    \ f) Armah, Nii Ayi. 2013. \u201CBig Data Analysis: The Next Frontier.\u201D Bank\
    \ of Canada Review. Summer.\n[2] Cambridge Dictionary \u2018harm\u2019, available:\
    \ https://dictionary.cambridge.org/dictionary/english/harm, Oxford Living Dictionaries\
    \ \u2018harm\u2019, available: https://en.oxforddictionaries.com/definition/harm\n\
    [3] See Citron, D K and Pasquale, F (2014) The scored society: due process for\
    \ automated Predictions. Washington Law Review, 89: 1-33.\n[4] Medium (2018) Data\
    \ Violence and How Bad Engineering Can Damage Society. [Online]. Available on:\
    \ https://medium.com/s/story/data-violence-and-how-bad-engineering-choices-can-damage-society-39e44150e1d4\n\
    [5] See Lyon, D (2015) Surveillance as Social Sorting: Privacy, Risk and Automated\
    \ Discrimination, New York: Routledge.\n[6] Federal Trade Commission (2015) FTC\
    \ charges data brokers with helping scammer take more than $7 million from Consumers\u2019\
    \ Accounts, 12 August, available: https://www.ftc.gov/news-events/press-releases/2015/08/ftc-charges-data-brokers-helping-scammer-take-more-7-million\n\
    [7] Andrews, Lori. 2013. I Know Who You Are and I Saw What You Did: Social Networks\
    \ and the Death of \nPrivacy, New York: Free Press.\n[8] As cited in Hurley, M\
    \ and Adebayo, J (2016) Credit scoring in the era of big data, Yale Journal of\
    \ Law and \nTechnology, 18(1), p.151.\n[9] Ibid, p. 151\n[10] Office of Oversight\
    \ and Investigations Majority Staff (2013) A Review of the Data Broker Industry:\
    \ Collection, Use, and Sale of Consumer Data for Marketing Purposes, Staff Report\
    \ for Chairman Rockefeller, Dec. 18, available: https://www.commerce.senate.gov/public/_cache/files/0d2b3642-6221-4888-a631-08f2f255b577/AE5D72CBE7F44F5BFC846BECE22C875B.12.18.13-senate-commerce-committee-report-on-data-broker-industry.pdf\n\
    [11] Madden, M, Gilman, M, Levy, K and Marwick, A (2017) \u2018Privacy, Poverty,\
    \ and Big Data: A Matrix of Vulnerabilities for Poor Americans\u2019, Washington\
    \ University Law Review, 95(1)\n[12] Whitener, M (2015) \u2018Cookies are so yesterday;\
    \ Cross-Device Tracking is In \u2013 Some Tips\u2019, Privacy Advisor, 27 Jan.\
    \ available: https://iapp.org/news/a/cookies-are-so-yesterday-cross-device-tracking-is-insome-tips/\n\
    [13] Newman, N (2014) \u2018How big data enables economic harm to consumers, especially\
    \ to low-income and other vulnerable sectors of the population\u2019, Public Comments\
    \ to FTC, available: https://www.ftc.gov/\nsystem/files/documents/public_comments/2014/08/00015-92370.pdf\n\
    [14] As cited in Citron, D K and Pasquale, F (2014) The scored society: due process\
    \ for automated\nPredictions. Washington Law Review, 89, p. 15.\n[15] Ibid\n[16]\
    \ Valentino-DeVries, J, Singer-Vine, J., and Soltani, A (2012) \u2018Watched:\
    \ Websites vary prices, deals based on users\u2019 information\u2019, The Wall\
    \ Street Journal, 24 Dec., A1\n[17] The Guardian (2015) Facebook Still Suspending\
    \ Native Americans Over \u2018Real Name\u2019 Policy. [Online]. Available on:https://www.theguardian.com/technology/2015/feb/16/facebook-real-name-policy-suspends-native-americans?source=post_elevate_sequence_page\u2014\
    \u2014\u2014\u2014\u2014\u2014\u2014\u2014\u2014\n[18] The New York Times (2019)\
    \ Amazon Faces Investor Pressure Over Facial Recognition. [Online]. Available\
    \ on: https://www.nytimes.com/2019/05/20/technology/amazon-facial-recognition.html\n\
    [19] The Guardian (2018) Amazon Face Recognition Falsely Matches 28 Lawmakers\
    \ with Mugshots, ACLU says. [Online]. Available on: https://www.theguardian.com/technology/2018/jul/26/amazon-facial-rekognition-congress-mugshots-aclu\n\
    [20] ACLU (2018) Amazon Teams up With Law Enforcement to Deploy Dangerous New\
    \ Face Recognition Technology. [Online]. Available on: https://www.aclunc.org/blog/amazon-teams-law-enforcement-deploy-dangerous-new-face-recognition-technology\n\
    [21] Datta, A, Tschantz, MC and Datta, A (2015) \u2018Automated Experiments on\
    \ Ad Privacy Settings\u2019, Proceedings on Privacy Enhancing Technologies, available:\
    \ https://www.degruyter.com/view/j/popets.2015.1.issue-1/popets-2015-0007/popets-2015-0007.xml\n\
    [22] ProPublica (2016) Facebook Lets Advertisers Exclude Users by Race. [Online].\
    \ Available on: https://www.propublica.org/article/facebook-lets-advertisers-exclude-users-by-race\n\
    [23] BBC (2018) Facebook Accused of Job Ad Gender Discrimination. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/technology-45569227\n[24] Financial Times (2018)\
    \ Facebook \u201CDark Ads\u201D and Discrimination. [Online]. Available on: https://search.proquest.com/docview/2129787570?accountid=9883&rfr_id=info%3Axri%2Fsid%3Aprimo\n\
    [25] The Guardian (2019) Facebook Charged with Housing Discrimination in targeted\
    \ Ads. [Online]. Available on: https://www.theguardian.com/technology/2019/mar/28/facebook-ads-housing-discrimination-charges-us-government-hud\n\
    [26] Ohm, P. (2010). \u201CBroken Promises of Privacy: responding to the surprising\
    \ failure of anonymization\u201D, UCLA Law Review, vol 57 (2010) pp 1701\u2013\
    1777\n[27] Arvind Narayanan & Vitaly Shmatikov (2008), How to Break the Anonymity\
    \ of the Netflix Prize Dataset, available: https://arxiv.org/abs/cs/0610105\n\
    [28] Singel, R (2009) Netflix spilled your Brokeback Mountain secret, lawsuit\
    \ claims, Wired, 17 December, available: https://www.wired.com/2009/12/netflix-privacy-lawsuit/\n\
    [29] The Washington Post (2019) With Fitness Trackers in the Workplace, Bosses\
    \ Can Monitor Your Every Step- And Possibly More. [Online]. Available on: https://www.washingtonpost.com/business/economy/with-fitness-trackers-in-the-workplace-bosses-can-monitor-your-every-step\u2013\
    and-possibly-more/2019/02/15/75ee0848-2a45-11e9-b011-d8500644dc98_story.html?utm_term=.b48be1cf9096\n\
    [30] The Guardian (2019) There\u2019s a Dark Side to Women\u2019s Health Apps:\
    \ \u201CMenstrual Surveillance\u201D. [Online]. Available::https://www.theguardian.com/world/2019/apr/13/theres-a-dark-side-to-womens-health-apps-menstrual-surveillance\n\
    [31] BBC (2018) MyFitnessPal Breach Affects Millions of Under Armour Users. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-43592470\n[32] The Guardian\
    \ (2018) Personal Data of a Billion Indians Sold Online for \xA36, Report Claims.\
    \ [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [33] Reuters (2019) Your Health App Could be Sharing Your Medical Data. [Online].\
    \ Available on: https://www.reuters.com/article/us-health-apps-privacy/your-health-app-could-be-sharing-your-medical-data-idUSKCN1R326W\n\
    [34] The Guardian (2018) Fitness Tracking App Strava Gives Away Locations of Secret\
    \ US Army Bases. [Online]. Available on:https://www.theguardian.com/world/2018/jan/28/fitness-tracking-app-gives-away-location-of-secret-us-army-bases\n\
    [35] The Guardian (2018) Strava Suggest Military Users Opt Out of Heatmap as Row\
    \ Deepens. [Online]. Available on:https://www.theguardian.com/technology/2018/jan/29/strava-secret-army-base-locations-heatmap-public-users-military-ban\n\
    [36] The Guardian (2018) Employers Are Monitoring Computers, Toilet Breaks- Even\
    \ Emotions. Is Your Boss Watching You? [Online]. Available on: https://www.theguardian.com/world/2018/may/14/is-your-boss-secretly-or-not-so-secretly-watching-you\n\
    [37] Ibid.\n[38] The Guardian (2018) Alarm Over Talks to Implant UK Employees\
    \ with Microchips. [Online]. Available on: https://www.theguardian.com/technology/2018/nov/11/alarm-over-talks-to-implant-uk-employees-with-microchips\n\
    [39]Bloomberg (2019) Amazon Workers Are Listening to What You Tell Alexa. [Online].\
    \ Available on: https://www.bloomberg.com/news/articles/2019-04-10/is-anyone-listening-to-you-on-alexa-a-global-team-reviews-audio\n\
    [40] BBC (2015) Not in Front of the Telly: Warning Over \u2018Listening\u2019\
    \ TV. [Online]. Available on: https://www.bbc.co.uk/news/technology-31296188\n\
    [41] The Guardian (2017) Wikileaks Publishes \u2018Biggest leak Ever of Secret\
    \ CIA Documents\u2019. [Online]. Available on: https://www.theguardian.com/media/2017/mar/07/wikileaks-publishes-biggest-ever-leak-of-secret-cia-documents-hacking-surveillance\n\
    [42] Dranoff, S (2014) \u2018Identity Theft: A Low-Income Issue\u2019, Dialogue,\
    \ Winter, available: https://www.\namericanbar.org/groups/legal_services/publications/dialogue/volume/17/winter-2014/identity-theft\u2013\
    a-lowincome-issue.html\n[43] BBC (2015) Ashley Madison Infidelity Site Customer\
    \ Data Leaked. [Online]. Available on: https://www.bbc.co.uk/news/business-33984017\n\
    [44] The Guardian (2015) Infidelity Site Ashley Madison Hacked as Attackers Demand\
    \ Total Shutdown. [Online]. Available on: https://www.theguardian.com/technology/2015/jul/20/ashley-madison-hacked-cheating-site-total-shutdown\n\
    [45] Reuters (2017) Ashley Madison Parent in $11.2 Million Settlement Over Data\
    \ Breach. [Online]. Available on: https://www.reuters.com/article/us-ashleymadison-settlement-idUSKBN19Z2F0\n\
    [46] BBC (2015) Ashley Madison: \u2018Suicides\u2019 Over Website Hacks. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-34044506\n[47] The Guardian\
    \ (2015) Dating Site Hackers Expose the Details of Millions of Users. [Online].\
    \ Available on: https://www.theguardian.com/lifeandstyle/2015/may/21/adult-friendfinder-dating-site-hackers-expose-users-millions\n\
    [48] BBC (2016) Up To 400 Million Accounts in Adult Friend Finder Breach. [Online].\
    \ Available on: https://www.bbc.co.uk/news/technology-37974266\n[49] BBC (2017)\
    \ Sweden Data Leak a \u2018Disaster\u2019, Says PM. [Online]. Available on: https://www.bbc.co.uk/news/technology-40705473\n\
    [50] The New York Times (2017) Swedish Government Scrambles to Contain Damage\
    \ From Data Breach. [Online]. Available on:https://www.nytimes.com/2017/07/25/world/europe/ibm-sweden-data-outsourcing.html\n\
    [51] BBC (2018) Aadhaar: \u201CLeak\u201D in World\u2019s Biggest Database Worries\
    \ Indians. [Online]. Available at: https://www.bbc.co.uk/news/world-asia-india-42575443\n\
    [52] The Guardian (2018) Personal Data of a Billion Indians Sold Online for \xA3\
    6, Report Claims. [Online]. Available on:https://www.theguardian.com/world/2018/jan/04/india-national-id-database-data-leak-bought-online-aadhaar\n\
    [53] Kaplan, E (2015) \u2018The Spy Who Fired me\u2019, Harper\u2019s, March,\
    \ available: https://harpers.org/archive/2015/03/\nthe-spy-who-fired-me/3/\n[54]\
    \ The Guardian (2018) \u201CI Made Steve Bannon\u2019s Psychological Warfare Tool\u201D\
    : Meet the Data War Whistleblower. [Online]. Available on: https://www.theguardian.com/news/2018/mar/17/data-war-whistleblower-christopher-wylie-faceook-nix-bannon-trump\n\
    [55] Hu, M. (2015) \u2018Big Data Blacklisting\u2019, Florida Law Review, 67:\
    \ 1735-1809.\n[56] Dixon, P. (2017) A Failure to \u201CDo No Harm\u201D \u2013\
    \ India\u2019s Aadhaar Biometric ID Program and its Inability to Protect Privacy\
    \ in Relation to Measures in Europe and the U.S. Health Technol,7(4): 539-567.\
    \ [Online]. Available on: https://link.springer.com/content/pdf/10.1007%2Fs12553-017-0202-6.pdf\n\
    [57] Logan, WA and Ferguson, AG (2016) \u2018Policing Criminal Justice Data\u2019\
    , Minnesota Law Review 541, available: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2761069\n\
    [58] See: Sullivan, E and Greene, R (2015) States predict inmates\u2019 future\
    \ crimes with secretive\nSurveys. AP, Feb. 24, available at: http://bigstory.ap.org/article/;\
    \ Barocas, S and Selbst, A D (2016) Big data\u2019s disparate impact. California\
    \ Law Review 104: 671-732; Starr, S (2016) The odds of justice: actuarial risk\
    \ prediction and the criminal justice system. Chance 29(1): 49-51.\n[59] O\u2019\
    Neil, C (2016) Weapons of Math Destruction, London: Allen Lane, p. 84-87.\n[60]BBC\
    \ (2018) Met Police \u201CGang Matrix\u201D Requires Overhaul. [Online]. Available\
    \ on: https://www.bbc.co.uk/news/uk-england-london-46646260\n\_\n[61] Evening\
    \ Standard (2018) Sadiq Khan Calls for Overhaul of Scotland Yard\u2019s Gang Matrix\
    \ as 4 in 5 Names on it are Shown to be Black. [Online]. Available on: https://www.standard.co.uk/news/crime/sadiq-khan-calls-for-overhaul-of-scotland-yards-gang-matrix-as-4-in-5-names-on-it-are-shown-to-be-a4024006.html\n\
    [62] Amnesty (2018) Trapped in the Matrix: Secrecy, stigma, and bias in the Met\u2019\
    s Gangs Database. London: Amnesty International United Kingdom Section. Available\
    \ on: https://www.amnesty.org.uk/files/2018-05/Trapped%20in%20the%20Matrix%20Amnesty%20report.pdf?HSxuOpdpZW_8neOqHt_Kxu1DKk_gHtSL\n\
    [63] StopWatch (2018) Being Matrixed: The (Over)policing of Gang Suspects in London.\
    \ [Online]Available on: http://www.stop-watch.org/uploads/documents/Being_Matrixed.pdf\n\
    [64] The Guardian (2018) Met Gang Matrix May be Discriminatory, Review Finds.\
    \ [Online]. Available on: https://www.theguardian.com/uk-news/2018/dec/21/metropolitan-police-gangs-matrix-review-london-mayor-discriminatory\n\
    [65] Throughout the record the hyperlinks provided link to individuals and groups\
    \ whose work raises concerns and also provides recommendations about how to reduce\
    \ data harms. In addition to those links, some examples of others doing work in\
    \ this area include those working as part of the FAT / ML Fairness, Accountability\
    \ and Transparency in Machine Learning group and the Algorithmic Justice League.\n\
    Share this:\nClick to share on X (Opens in new window)\nX\n\nClick to share on\
    \ Facebook (Opens in new window)\nFacebook\nLike this:Like Loading..."
  date: '2017-12-06T13:11:28+00:00'
  id: 5cf3cf6e8a39066967fbdbd2b5daa708
  publication: Data Justice Lab
  tags: *id001
  title: Data Justice Lab
  url: https://datajusticelab.org/data-harm-record/#_ftn4
- author: Alex Engler
  content: "Why Trump will blink first on China \n\n\n\n\n\n\n\n\n\n             \
    \           Why Trump will blink first on China"
  date: March 12, 2021
  id: 4efdd05313c00d69c7363aeb8df7948c
  publication: Data Justice Lab
  tags: *id001
  title: 'Algorithmic bias detection and mitigation: Best practices and policies to
    reduce consumer harms'
  url: https://www.brookings.edu/research/algorithmic-bias-detection-and-mitigation-best-practices-and-policies-to-reduce-consumer-harms/
